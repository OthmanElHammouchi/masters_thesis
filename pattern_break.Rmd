---
title: "Pattern break experiments"
author: "Othman El Hammouchi"
---
```{r}
here::i_am("pattern_break.Rmd")
library(tidyverse)
library(MASS)
library(ggplot2)
library(magrittr)
library(ChainLadder)
library(here)
library(knitr)
library(gridExtra)

library(iterators)
library(parallel)
library(doParallel)
library(foreach)

registerDoParallel()

source(here("pattern_break.r"))
```

We start by generating a claims triangle which satisfies the assumptions of Mack's method. For realism, we borrow the determining parameters from a real dataset, the UKMotor data from the `ChainLadder` package.

```{r}
nDev <- ncol(UKMotor)
benchmark <- MackChainLadder(UKMotor)
sigma <- benchmark$sigma
devFacs <- benchmark$f[-nDev]
initMean <- mean(UKMotor[, 1])
initStd <- sd(UKMotor[, 1])
```

The generated triangle is given below.

```{r}
initCol <- rnorm(nDev, initMean, initStd)

claimsTriangle <- matrix(ncol = nDev, nrow = nDev)
claimsTriangle[, 1] <- initCol

for (colIdx in 2:nDev) {
    for (rowIdx in 1:(nDev + 1 - colIdx)) {
        prevC <- claimsTriangle[rowIdx, colIdx - 1]
        claimsTriangle[rowIdx, colIdx] <-
            rnorm(1, devFacs[colIdx - 1] * prevC, sigma[colIdx - 1] * sqrt(prevC))
    }
}

kable(claimsTriangle)
```

As expected, the Mack estimator performs very well in this case:

```{r}
res <- MackChainLadder(claimsTriangle)

print(res$f)
```

\section{Perturbing a single observation}

We study the effect of perturbing a single observation and propagating this forward. The following function generates such data given a desired outlier column and row and a perturbation factor. We can repeat the simulation for every possible point and a range of perturbations between 0.5 and 1.5. 

```{r}
results <- tibble(
    row = numeric(),
    col = numeric(),
    pert = numeric(),
    triangle = list(),
    reserve = list()
)

for (colIdx in 2:nDev) {
    for (rowIdx in 1:(nDev + 1 - colIdx)) {
        for (pert in seq(0.5, 1.5, by = 0.25)) {
            triangle <- singleOutlier(colIdx, rowIdx, pert,
                nDev = nDev,
                initMean = initMean,
                initStd = initStd,
                devFacs = devFacs,
                sigma = sigma)
            reserve <- reserveBoot(triangle, 1e3)$reserve %>% list()
            triangle %<>% list()
            results %<>% add_row(row = rowIdx, col = colIdx, triangle = triangle, pert = pert, reserve = reserve)
        }
    }
}
```

```{r}
results <- foreach(colIdx = 2:nDev, .combine = "rbind", .errorhandling = "pass") %:%
    foreach(pert = seq(0.5, 1.5, by = 0.25), .combine = "rbind") %dopar% {

        rowResult <- tibble()

        for (rowIdx in 1:(nDev + 1 - colIdx)) {
            triangle <- singleOutlier(colIdx, rowIdx, pert,
                nDev = nDev,
                initMean = initMean,
                initStd = initStd,
                devFacs = devFacs,
                sigma = sigma)

            reserve <- reserveBoot(triangle, 1e3, exclude_residuals = list(c(rowIdx, colIdx)))$reserve

            rowResult <- bind_rows(rowResult, tibble(col = colIdx, row = rowIdx, pert = pert, reserve = list(tibble(reserve = reserve))))
        }

        rowResult
    }

stopImplicitCluster()
```

Now let's take a look at the bootstrap mean as a function of the perturbation for different rows and columns.

```{r}
results <- results %>%
    mutate(mean = map(reserve, function(df) mean(df$reserve, na.rm = TRUE))) %>%
    unnest_auto(mean)
```

```{r}
results <- results %>%
    mutate(mean = map(reserve, mean, na.rm = TRUE)) %>%
    unnest_auto(mean)
```

```{r}
ggplot(data = results) +
    geom_point(aes(pert, mean)) +
    facet_wrap(vars(col))
```

```{r}
ggplot(data = results) +
    geom_point(aes(pert, mean)) +
    facet_wrap(vars(row))
```

Surprisingly, the plots show a higher mean both for smaller and larger perturbations.

Now we want to go in the reverse direction: if we start from a perturbed triangle, does removal of the outlying observation create a significant difference in the reserve?

```{r}
rowIdx <- 2
colIdx <- 3

distortedTriangle <- singleOutlier(colIdx, rowIdx, 0.7,
    nDev = nDev,
    initMean = initMean,
    initStd = initStd,
    devFacs = devFacs,
    sigma = sigma)

reserveBoot(distortedTriangle, 1e3, exclude_residuals = list(c(rowIdx, colIdx)))
```

```{r}
rowIdx <- 2
colIdx <- 3

distortedTriangle <- singleOutlier(colIdx, rowIdx, 0.7,
    nDev = nDev,
    initMean = initMean,
    initStd = initStd,
    devFacs = devFacs,
    sigma = sigma)

reserveByExclusion <- tibble(
    row = integer(),
    col = integer(),
    reserve = list()
)
for (j in 1:(nDev - 1)) {
    for (i in 1:(nDev + 1 - j)) {
        exclude_residuals <- list(c(i, j))
        reserve <- list(reserveBoot(distortedTriangle, 1e3, exclude_residuals = exclude_residuals)$reserve)
        reserveByExclusion <- add_row(
            reserveByExclusion,
            row = i,
            col = j,
            reserve = reserve)
    }
}
```

```{r}
plots <- list()

for (j in 1:(nDev - 1)) {
    for (i in 1:(nDev + 1 - j)) {

        plot <- reserveByExclusion %>%
            filter(row == i & col == j) %>%
            dplyr::select(reserve) %>%
            unnest_auto(reserve) %>%
            ggplot(data = .) +
            geom_histogram(aes(x = reserve)) +
            ggtitle(paste0("Row: ", i, ", Col: ", j))

        plots <- c(plots, list(plot))
    }
}

do.call("grid.arrange", c(plots, ncol = 3))
```

```{r}
quantiles <- c()

for (j in 1:(nDev - 1)) {
    for (i in 1:(nDev + 1 - j)) {
        f <- reserveByExclusion %>%
            filter(row == i & col == j) %>%
            dplyr::select(reserve) %>%
            unlist() %>%
            ecdf()
        quantiles <- c(f(25000), quantiles)
    }
}

reserveByExclusion$quantiles <- 1 - quantiles

reserveByExclusion %>% dplyr::arrange(quantiles) %>%
    print(n = 27)
```

\subsection{Gamma distribution}

The problem with using the above normal model is that it allows for negative samples during the bootstrap simulation. To avoid this, we can use a different distribution which still respects Mack's assumptions. If we take for instance $C_{ij} \sim \Gamma(\alpha, \beta)$, then we must choose $\alpha, \beta$ to satisfy

$$
\begin{cases}
\frac{\alpha}{\beta} = f_{j-1} C_{i, j-1} \\
\frac{\alpha}{\beta^2} = \sigma^2_{j-1} C_{i, j-1} \,,
\end{cases}
$$

giving the values

$$
\alpha = \frac{f_{j-1}^2 C_{i, j-1}}{\sigma_{j-1}^2} \\
\beta = \frac{f_{j-1}}{\sigma_{j-1}^2} \,,
$$

for the distribution parameters.

```{r}
results <- tibble(
    row = numeric(),
    col = numeric(),
    pert = numeric(),
    triangle = list(),
    reserve = list()
)

for (colIdx in 2:nDev) {
    for (rowIdx in 1:(nDev + 1 - colIdx)) {
        for (pert in seq(0.5, 1.5, length.out = 10)) {
            triangle <- singleOutlierGamma(nDev, initMean, initStd, devFacs, sigma, colIdx, rowIdx, pert = pert)
            reserve <- bootReserveGamma(triangle, 1e3) %>% list()
            triangle %<>% list()
            results %<>% add_row(row = rowIdx, col = colIdx, triangle = triangle, pert = pert, reserve = reserve)
        }
    }
}
```

```{r}
results %<>%
    mutate(mean = map(reserve, mean, na.rm = TRUE)) %>%
    unnest_auto(mean)
```

```{r}
ggplot(data = results) +
    geom_point(aes(pert, mean)) +
    facet_wrap(vars(col))
```

```{r}
ggplot(data = results) +
    geom_point(aes(pert, mean)) +
    facet_wrap(vars(row))
```

