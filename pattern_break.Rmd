---
title: "Pattern break detection"
author: "Othman El Hammouchi"
output: "pdf_document"
header_includes:
    - \usepackage{amsmath}
---
```{r, echo = FALSE}
suppressPackageStartupMessages({
    library(tidyverse)
    library(MASS)
    library(ggplot2)
    library(magrittr)
    library(ChainLadder)
    library(here)
    library(knitr)
    library(gridExtra)
    library(patchwork)

    library(iterators)
    library(parallel)
    library(doParallel)
    library(foreach)

    registerDoParallel()

    source(here("pattern_break.r"))
})

suppressMessages({
    here::i_am("pattern_break.Rmd")
    knitr::opts_chunk$set(echo = FALSE)
})
```

\section{Introduction}

Stochastic claims reserving methods in non-life insurance are based on models whose underlying assumptions are difficult to verify using classical statistical inference owing to the limited amount of data that's typically available. A possible way to remedy this, which we will explore in this notebook, is through the use of bootstrap simulation. The idea is to start from a sythetic triangle which is constructed to perfectly satisfy some set of assumptions, and then to gauge the sensitivity of the simulated reserve to deviations from said assumptions. We will do this by applying various violations and studying the effect on the bootstrapped predictive distribution. If a significant effect is observed, this suggests a method for identifying observations in real claims data which do not conform to the given model.

\section{Perturbing a single observation}

Let's start by considering the underlying model of Mack's method, which is one the most popular reserving techniques. It makes the following assumptions:

\begin{gather}
\mathbb{E}[C_{ij} \ \Vert \ C_{i, j - 1}, \dots, C_{i1}] = \mathbb{E}[C_{ij} \ \Vert \ C_{i, j - 1}] = f_{j - 1} C_{i, j - 1} \\
\mathrm{Var}[C_{ij} \ \Vert \ C_{i, j - 1}, \dots, C_{i1}] = \mathrm{Var}[C_{ij} \ \Vert \ C_{i, j - 1}] = \sigma_{j - 1}^2 C_{i, j - 1}
\end{gather},
for $i \in \{ 0, \dots, I \}$ and
\begin{equation}
\{ C_{i, 0}, \dots, C_{i, I - i} \}, \{ C_{i', 0}, \dots, C_{i', I - i'} \} \ \text{independent for} \ i \neq i' \,.
\end{equation}

The first violation we consider is of the mean assumption. We can repeat the simulation for every possible point and a range of perturbations between 0.5 and 1.5. 

triangle conforming to Mack's assumptions. For added realism, we borrow the development factors and $\sigma_j$-parameters from a real dataset, the UKMotor data from the `ChainLadder` package.

```{r}
set.seed(5)

nDev <- ncol(UKMotor)
benchmark <- MackChainLadder(UKMotor)
sigma <- benchmark$sigma
devFacs <- benchmark$f[-nDev]
initMean <- mean(UKMotor[, 1])
initStd <- sd(UKMotor[, 1])

initCol <- rnorm(nDev, initMean, initStd)

claimsTriangle <- matrix(ncol = nDev, nrow = nDev)
claimsTriangle[, 1] <- initCol

for (colIdx in 2:nDev) {
    for (rowIdx in 1:(nDev + 1 - colIdx)) {
        prevC <- claimsTriangle[rowIdx, colIdx - 1]
        claimsTriangle[rowIdx, colIdx] <-
            rnorm(1, devFacs[colIdx - 1] * prevC, sigma[colIdx - 1] * sqrt(prevC))
    }
}

kable(claimsTriangle)
```

As expected, the Mack estimator performs very well in this case:

```{r}
res <- MackChainLadder(claimsTriangle)

print(res$f)
```

-- Describe that there are multiple ways of doing the bootstrap, depending on how parameter estimation error is quantified, assumptions on the residuals and choice of distribution for incorporating process error --

-- Describe the first class of experiments: we perturb a single observation by drawing from a normal distribution whose mean is multiplied with a given factor. This corresponds to scaling the mean of the corresponding individual development factor from the link ratio adhered to by the rest of the column by that factor. --

```{r, eval = FALSE, echo = FALSE}

# sequential version of computations in the next chunk; remove 'eval = FALSE' if your setup does not support multithreading

results <- tibble(
    row = numeric(),
    col = numeric(),
    pert = numeric(),
    reserve = list()
)

for (colIdx in 2:nDev) {
    for (rowIdx in 1:(nDev + 1 - colIdx)) {
        for (pert in seq(0.5, 1.5, by = 0.25)) {

            triangle <- singleOutlier(colIdx, rowIdx, pert,
                nDev = nDev,
                initMean = initMean,
                initStd = initStd,
                devFacs = devFacs,
                sigma = sigma)

            reserve <- reserveBoot(triangle, 1e3) %>%
                tibble(reserve = .) %>%
                list()

            results %<>% add_row(row = rowIdx, col = colIdx, pert = pert, reserve = reserve)
        }
    }
}
```
```{r, cache = TRUE}

results <- foreach(colIdx = 2:(nDev - 1), .combine = "rbind", .errorhandling = "pass") %:%
    foreach(pert = seq(0.5, 1.5, by = 0.25), .combine = "rbind") %dopar% {

        rowResult <- tibble()

        for (rowIdx in 1:(nDev + 1 - colIdx)) {
            triangle <- singleOutlier(colIdx, rowIdx, pert,
                nDev = nDev,
                initMean = initMean,
                initStd = initStd,
                devFacs = devFacs,
                sigma = sigma)

            reserve <- reserveBoot(triangle, 1e3, distribution = "normal")

            rowResult <- bind_rows(rowResult, tibble(col = colIdx, row = rowIdx, pert = pert, reserve = list(tibble(reserve = reserve))))
        }

        rowResult
    }

stopImplicitCluster()
```

-- Describe what the plots indicate:

```{r}
results <- mutate(results,
    mean = map_dbl(reserve, function(df) mean(df$reserve, na.rm = TRUE)))
```

```{r}
ggplot(data = results) +
    geom_point(aes(pert, mean)) +
    facet_wrap(vars(col)) +
    labs(
        x = "Perturbation factor",
        y = "Bootstrap reserve mean"
    ) +
    ggtitle("Reserve estimates for different perturbed columns") +
    theme(plot.title = element_text(hjust = 0.5, size = 30, face = "bold"))
```

```{r}
ggplot(data = results) +
    geom_point(aes(pert, mean)) +
    facet_wrap(vars(row))
```

Now we want to go in the reverse direction: if we start from a perturbed triangle, does removal of the outlying observation create a significant difference in the reserve?

```{r}
rowIdx <- 2
colIdx <- 3
factor <- 0.7

distortedTriangle <- singleOutlier(colIdx, rowIdx, factor,
    nDev = nDev,
    initMean = initMean,
    initStd = initStd,
    devFacs = devFacs,
    sigma = sigma)

```

```{r}
results <- foreach(colIdx = 2:(nDev - 1), .combine = "rbind", .errorhandling = "stop") %dopar% {
    rowResult <- tibble()
    for (rowIdx in 1:(nDev + 1 - colIdx)) {

        exclude_residuals <- list(c(rowIdx, colIdx))
        reserve <- reserveBoot(distortedTriangle, 1e3, exclude_residuals = exclude_residuals)

        rowResult <- bind_rows(rowResult, tibble(col = colIdx, row = rowIdx, reserve = list(tibble(reserve = reserve))))
    }
    rowResult
}
```

\subsection{Gamma distribution}

The problem with using the above normal model is that it allows for negative samples during the bootstrap simulation. To avoid this, we can use a different distribution which still respects Mack's assumptions. If we take for instance $C_{ij} \sim \Gamma(\alpha, \beta)$, then we must choose $\alpha, \beta$ to satisfy
\begin{displaymath}
\begin{cases}
\frac{\alpha}{\beta} = f_{j-1} C_{i, j-1} \\
\frac{\alpha}{\beta^2} = \sigma^2_{j-1} C_{i, j-1} \,,
\end{cases}
\end{displaymath}
giving the values
\begin{gather*}
\alpha = \frac{f_{j-1}^2 C_{i, j-1}}{\sigma_{j-1}^2} \\
\beta = \frac{f_{j-1}}{\sigma_{j-1}^2} \,,
\end{gather*}
for the distribution parameters.

```{r, eval = FALSE}

# sequential version of computations in the next chunk; remove 'eval = FALSE' if your setup does not support multithreading

results <- tibble(
    row = numeric(),
    col = numeric(),
    pert = numeric(),
    triangle = list(),
    reserve = list()
)

for (colIdx in 2:nDev) {
    for (rowIdx in 1:(nDev + 1 - colIdx)) {
        for (pert in seq(0.5, 1.5, length.out = 10)) {
            triangle <- singleOutlierGamma(nDev, initMean, initStd, devFacs, sigma, colIdx, rowIdx, pert = pert)
            reserve <- bootReserveGamma(triangle, 1e3) %>% list()
            triangle %<>% list()
            results %<>% add_row(row = rowIdx, col = colIdx, triangle = triangle, pert = pert, reserve = reserve)
        }
    }
}
```
```{r, cache = TRUE}

results <- foreach(colIdx = 2:(nDev - 1), .combine = "rbind", .errorhandling = "pass") %:%
    foreach(pert = seq(0.5, 1.5, by = 0.25), .combine = "rbind") %dopar% {

        rowResult <- tibble()

        for (rowIdx in 1:(nDev + 1 - colIdx)) {
            triangle <- singleOutlier(colIdx, rowIdx, pert,
                nDev = nDev,
                initMean = initMean,
                initStd = initStd,
                devFacs = devFacs,
                sigma = sigma)

            reserve <- reserveBoot(triangle, 1e3, distribution = "normal")

            rowResult <- bind_rows(rowResult, tibble(col = colIdx, row = rowIdx, pert = pert, reserve = list(tibble(reserve = reserve))))
        }

        rowResult
    }

stopImplicitCluster()
```

```{r, eval = FALSE}
results %<>%
    mutate(mean = map(reserve, mean, na.rm = TRUE)) %>%
    unnest_auto(mean)
```

```{r, eval = FALSE}
ggplot(data = results) +
    geom_point(aes(pert, mean)) +
    facet_wrap(vars(col))
```

```{r, eval = FALSE}
ggplot(data = results) +
    geom_point(aes(pert, mean)) +
    facet_wrap(vars(row))
```

