\sepfootnotecontent{fn:quasi-likelihood}{It would actually be more correct to call $Q$ a quasi-\emph{log}-likelihood, but the current nomenclature has been widely adopted and the literature seems to have resigned itself to it.}%

\sepfootnotecontent{fn:gauss-markov}{The third assumption is actually slightly stronger than necessary: Gauss-Markov only requires the errors to be uncorrelated.}%

\sepfootnotecontent{fn:bayes}{Strictly speaking, this is only one possible paradigm within Bayesian probability, known as subjective Bayes. Objective Bayes dispenses with the subjective aspect of the method through the use of so-called \emph{uninformative priors}. Since this distinction is not important for our exposition, however, we will not pursue it further.}%

\sepfootnotecontent{fn:quantile-resids}{In the original paper \cite{dunn:rand-quant-res}, these cumulative probabilities are further transformed using the inverse normal CDF in order to produce residual plots which more closely resemble the ones familiar from the classical normal linear model. Since this is not necessary for our purposes, we use this amended definition.}%

\sepfootnotecontent{fn:bayes-necessary}{Although we will endeavour in this text to remain agnostic on philosophical questions of probability interpretation, it will become clear shortly why it is necessary to adopt a Bayesian perspective here.}

\sepfootnotecontent{fn:glm}{This problem was recognised early on in the literature on GLM bootstrapping, see \cite{moulton}.}