\documentclass[a4paper]{book}

\input{preamble.tex}

\title{Sensitivity analysis of stochastic reserving models using bootstrap simulations}
\pretitle{\flushleft{Graduation thesis submitted in partial fulfilment of the requirements for the degree of Master of Science in Mathematics}}
\author{Othman El Hammouchi}
\date{June~2023}
\promotors{Promotors: dr.\ Robin Van Oirbeek \and prof.\ dr.\ Tim Verdonck}
\faculty{Sciences and Bioengineering Sciences}

\makenomenclature

\renewcommand{\nomname}{List of Symbols}
\renewcommand{\nompreamble}{The next list describes several symbols that will be later used within the body of the document}

\begin{document}

\frontmatter
\maketitle%
\input{footnotes.tex}%

\title{Gevoeligheidsanalyse van stochastische schadereserveringsmodellen aan de hand van bootstrap-simulaties}
\pretitle{\flushleft{Proefschrift ingediend met het oog op het behalen van de graad van Master of Science in de Wiskunde}}%
\date{Juni~2023}%
\promotors{Promotors: dr.\ Robin Van Oirbeek \and prof.\ dr.\ Tim Verdonck}
\faculty{Wetenschappen en Bio-ingenieurswetenschappen}%
\maketitle%

\chapter{Abstract}

Your abstract would go here.

\tableofcontents%

\input{nomenclature.tex}

\printnomenclature%

\mainmatter%

\chapter{Introduction} \label{chapter:intro}

The most defining characteristic of the insurance industry is the inverted nature of its production cycle. In manufacturing, commerce, transport, etc., payment is usually received only upon delivery of goods or services. By contrast, insurance products are purchased long before the adverse events which they protect against have occured, if they ever do. Insurers therefore face the challenge of forecasting the amount and variability of funds needed to settle outstanding contracts, a process known as \emph{claims reserving}. In this the reserving actuary relies historical data which is most often presented in the form of a \emph{loss} or \emph{run-off triangle} $\mathcal{D}_I$, which consists either of cumulative or incremental amounts of some actuarial variable (payments, number of claims, etc.), respectively denoted by $C_{ij}$ and $X_{ij}$. Here $1 \leq i \leq I$ denotes the \emph{cohort}, \emph{origin year} or \emph {accident year} and $1 \leq j \leq J$ the \emph{development year}, so that
\begin{equation}
    \mathcal{D}_I = \left  \{ C_{ij} \mid 1 \leq j \leq J, i + j \leq I + 1 \right \} 
    %
    \quad \text{or} \quad 
    %
    \mathcal{D}_I = \left  \{ X_{ij} \mid 1 \leq j \leq J, i + j \leq I + 1 \right \} \,.
\end{equation}
To simplify the formulas, we assume throughout this exposition that $I = J$. Embedding $\mathcal{D}_I$ into a matrix on and above the anti-diagonal, the actuary then seeks to predict the \emph{total outstanding loss liabilities}
\begin{equation}
    R = \sum_{i = 2}^I (C_{i, I} - C_{i, I + 1- i})
\end{equation}
by forecasting the values in the lower triangle $\mathcal{D}^{\mathsf{c}}_I$. A special difficulty arising in the actuarial context is the relatively small number of observations which is usually available.

\begin{table}
  \centering
  \begin{subtable}{0.45\textwidth}
    \centering
    \large
    \begin{tabular}{c c c c c}
      $C_{11}$ & $C_{12}$ & $C_{13}$ & $C_{14}$ & $C_{15}$ \\
      $C_{21}$ & $C_{22}$ & $C_{23}$ & $C_{24}$ & \\
      $C_{31}$ & $C_{32}$ & $C_{33}$ &  &  \\
      $C_{41}$ & $C_{42}$ &  &  & \\
      $C_{51}$ &  &  &  & 
    \end{tabular}
    \subcaption{Cumulative}
  \end{subtable}
  \begin{subtable}{0.45\textwidth}
    \centering
    \large
    \begin{tabular}{c c c c c}
      $X_{11}$ & $X_{12}$ & $X_{13}$ & $X_{14}$ & $X_{15}$ \\
      $X_{21}$ & $X_{22}$ & $X_{23}$ & $X_{24}$ & \\
      $X_{31}$ & $X_{32}$ & $X_{33}$ &  &  \\
      $X_{41}$ & $X_{42}$ &  &  & \\
      $X_{51}$ &  &  &  & 
    \end{tabular}
    \subcaption{Incremental}
  \end{subtable}
  \caption{General notation for a 5 by 5 claims triangle}
\end{table}

One of the most frequently used loss reserving techniques in practice is the so-called \emph{chain ladder} (CL), which predicts the cumulative claim in development year $j$ by multiplying the previous year's amount by a so-called \emph{age-to-age factor}, \emph{link ratio} or \emph{development factor}. It was originally conceived as a purely computational algorithm, but has since been framed as a stochastic model in a variety of ways. The central assumption it makes, is that the pattern observed in earlier cohorts is applicable to later ones. In one sense, this is of course perfectly reasonable: all models ultimately use the past as a guide to the future. The dearth of data typically available to the actuary makes it challenging to verify its validity, however, as it limits the efficacy of classical statistical techniques. In particular, it makes it difficult to detect structural breaks in the claims development pattern. 

To illustrate this point, consider \Cref{tab:uk-motor}, which contains the dataset of cumulative payments for a motor insurance account from the UK given in \cite{christofides} (this will serve as the running example throughout this text). It consists of a 7 by 7 claims triangle with a total of 28 observations. \Cref{fig:diag-plot-original} shows the diagnostic plot of standardised residuals against fitted value for the Mack chain ladder model, which will be discussed in \Cref{chapter:mack}. The details are not important at this point; all that matters at present is that it should be symmetric around the x-axis and exhibit no structural patterns if the model gives a good fit. 

\begin{table}[!htb]
  \centering
  \footnotesize
  \begin{tabular}{|c|c|c|c|c|c|c|c|}
    \hline 
    \diagbox{i}{j} &  1 &  2 & 3 & 4 & 5 & 6 & 7 \\ \hline
    2007 & 3511 & 6726 & 8992  & 10704 & 11763 & 12350 & 12690 \\ \hline
    2008 & 4001 & 7703 & 9981  & 11161 & 12117 & 12746 &       \\ \hline
    2009 & 4355 & 8287 & 10233 & 11755 & 12993 &       &       \\ \hline
    2010 & 4295 & 7750 &  9773 & 11093 &       &       &       \\ \hline
    2011 & 4150 & 7897 & 10217 &       &       &       &       \\ \hline
    2012 & 5102 & 9650 &       &       &       &       &       \\ \hline
    2013 & 6283 &      &       &       &       &       &       \\
    \hline
  \end{tabular}
  \caption{UK Motor claims triangle from \textcite{christofides}}
  \label{tab:uk-motor}
\end{table}

\begin{figure}[!htb]
  \centering
  \includegraphics[scale=0.9]{original_resids}
  \caption{Diagnostic plot for original triangle}
  \label{fig:diag-plot-original}
\end{figure}

The same diagnostic plots are shown in \Cref{fig:diag-plot-perturbed} for the case where the single points $(2, 5)$ and $(4, 4)$ of this triangle has been perturbed by a factor $1.5$, either by direct multiplication or through simulation from the underlying model. The residual corresponding to the pathological observation has been highlighted in red. As these examples demonstrate, it is not always feasible to identify deviations from the model assumptions by examining such plots, even for the trained eye.

\begin{figure}[!htb]
  \centering
  \begin{subfigure}{\textwidth}
    \includegraphics{perturbed_resids}
    \caption{Perturbed directly}
  \end{subfigure}
  \begin{subfigure}{\textwidth}
    \includegraphics{model_perturbed_resids}
    \caption{Perturbed according to model}
  \end{subfigure}
  \caption{Diagnostic plots for perturbed triangles}
  \label{fig:diag-plot-perturbed}
\end{figure}

Our aim in this chapter is to investigate whether it is possible to use bootstrap simulations to remedy this problem. The chapter is divided as follows. The next section introduces the bootstrap and explains how it can be used for inference on regression models, which is how we will frame the reserving methods under consideration. In the subsequent two sections, we will then apply this theory to two of the most widespread stochastic claims reserving models, the Mack chain ladder and the (overdispersed) Poisson GLM, in order to study whether it is possible to identify pattern breaks by using the bootstrap. Specifically, we simulate claims triangles which perfectly follow the model assumptions, perturb these, and generate a bootstrap reserve from the resulting dataset. This will allow us to investigate how the simulated reserve is impacted by deviations from the model assumptions.

\chapter{The bootstrap method} \label{chapter:boot}

When using a statistical model to describe a dataset in terms of a reduced number of parameters, we are not only interested in producing point estimates of these parameters, but also in quantifying their \emph{uncertainty}. In classical statistics, the usual approach to achieve this is to start from the model assumptions and derive from them analytically the sampling distribution of the estimators. In most cases (the Gaussian distribution being a notable exception) this leads to intractible calculations, so that one is either forced to rely on approximations and asymptotic results, or make unrealistic simplifying assumptions. Moreover, estimates obtained in this way often heavily depend on their underlying assumptions, which can potentially lead to gross errors if these are violated.

The bootstrap method aims to remedy this problem by using numerical simulations to compute estimates of model uncertainty. At its core, it is premised on the idea that the empirical distribution of the sample forms a good proxy for that the population distribution. Consequently, we can approximate sampling from the population by \emph{resampling our data}, which, to the uncareful observer, can give the impression that we're 'magically' producing new information, using our single sample to 'pull ourselves up by our own bootstraps', which is where the procedure derives its name from. Let's see how this can be done concretely for a simple estimation problem.

\section{Bootstrapping an estimator}

Let $X_1, \dots, X_n$ be an i.i.d. sample drawn from a distribution $F$, and consider an estimator $\widehat{h(F)} = g(X_1, \dots, X_n)$ of some quantity $h(F)$ whose uncertainty we wish to estimate, using e.g.\ the variance of the sampling distribution. Depending on the assumptions we are willing to make, we can choose between two broad approaches: \emph{parameteric} methods and \emph{nonparametric} ones.

In the nonparametric bootstrap, we use the data directly, drawing with replacement to simulate new samples $X^{(b)}_1, \dots, X^{(b)}_n$. In other words, we approximate $F$ using the \emph{empirical cumulative distribution function}
\begin{equation}
    \widehat{F}_n(x) \coloneqq \sum_{k=1}^n I_{\{ X_k \leq x \}} \,,
\end{equation}
which we use to generate new data. We then compute the statistic of interest on these pseudo-samples, yielding pseudo-observations $g^{(b)} = g(X^{(b)}_1, \dots, X^{(b)}_n)$ which approximate the sampling distribution of $\widehat{h(F)}$. Writing $B$ for the total number of bootstrap samples, we can esimate the variance of $\widehat{h(F)}$ by
\begin{equation}
    \frac{1}{B-1}\sum_{b=1}^B(g^{(b)} - \bar{g})^2 \,,
\end{equation}
with $\bar{g} = \frac{1}{B} \sum_{b=1}^B g^{(b)}$. Provided $F \approx \widehat{F}_n$ holds with sufficient accuracy, this will yield a reasonable approximation to $\mathrm{Var}(\widehat{h(F)})$.

By contrast, in the parametric bootstrap, we first fit a model using the data, and then simulate samples from this with the help of a random number generator. As usual, the parametric approach offers the advantage of efficiency if its assumptions are met, at the risk of increased error when they are violated. If we assume that $F$ belongs to some family $\left \{ F_{\bm{\theta}} \mid  \bm{\theta} \in \Theta \right \}$, then we can use the sample $X_1, \dots, X_n$ to produce an estimate $\bm{\widehat{\theta}}$ of the parameter. Plugging this in then gives us $F_{\bm{\widehat{\theta}}}$, from which we can simulate $X^{(b)}_1, \dots , X^{(b)}_n$ and $g^{(b)}$ as before. An estimate of the sampling variance is likewise obtained the same manner.

Although we have thus far only used it for the calculation of a single statistic, it is clear that the bootstrap produces a complete \emph{simulated distribution} of the estimator, which can be used for any arbitrary form of inference. This shows its tremendous potential as a tool for statistical analysis, which explains the rise in popularity of these methods with the advent of powerful personal computers capable of carrying out the requisite calculations. Let us give an example to illustrate this. \Cref{fig:boot-est} compares, for a $\Gamma(\alpha, \beta)$-distribution with $\alpha = 2$ and $\beta = 0.5$, the bootstrap distributions of the maximum likelihood estimators to their exact counterparts. We use a simulated sample of size $n = 1000$ and compute . The analytic distributions are based on the well-known fact from likelihood theory that the asymptotic distribution of the MLE is given by the multivariate normal distribution $\mathcal{N}(\bm{\theta}, I(\bm{\theta})^{-1})$, where $\bm{\theta}$ is the parameter vector and $I(\bm{\theta})$ the Fisher information matrix. For the gamma distribution, the latter is given by
\begin{equation}
  I(\alpha, \beta) = n
  \begin{pmatrix}
      \psi'(\alpha) & -1 / \beta \\
      -1 / \beta & \alpha / \beta^2
  \end{pmatrix} \,
\end{equation}
where $\psi(x) \coloneqq \odv{}{x} \log \Gamma(x)$ is the so-called digamma function. It is clear that 

\begin{landscape}
  \begin{figure}
    \begin{subfigure}{\linewidth}
      \includegraphics{boot_est_param}
      \subcaption{Parametric}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
      \includegraphics{boot_est_nonparam}
      \subcaption{Nonparametric}
    \end{subfigure}
    \caption{Analytic and bootstrap distribution of MLE for a gamma distribution with $\alpha = 2$ and $\beta = 0.5$.}
    \label{fig:boot-est}
  \end{figure}
\end{landscape}

\section{Bootstrapping a regression model} \label{sec:boot-reg}

Altough we have introduced the bootstrap in the context of a classical one-sample estimation problem, the same principles can be applied to data structures of arbitrary complexity, so long as we have a model for the probabilistic mechanism generating the observations (see \cite[Chapter 8]{efron:intro} for a general exposition of this methodology). In particular, bootstrap methods for regression models are well-established in the literature. We now turn our attention to these, as they will form the foundation for developing bootstrap methods for claims triangles.

Consider a set of covariates $X_1, \dots, X_p$ and a response variable $Y$ whose relationship we model by a parametrised mapping $f(X_1, \dots, X_p; \bm{\beta})$. Given a sample of pairs $(\bm{\mathrm{x}_1}, Y_1), \dots, (\bm{\mathrm{x}_n}, Y_N)$ and a choice of loss function, we can fit this model to obtain an estimate $\widehat{\bm{\beta}}$ of $\bm{\beta}$. For new values $x_1^+, \dots, x_N^+$ of the regressors, we can then predict the response $Y^+$ as $f(x_1^+, \dots, x_N^+; \widehat{\bm{\beta}})$. It is worth emphasising these as two distinct operations, which correspond to different bootstrap procedures (see \cite[Sections 6.3.3 and 7.2.4]{davison}). \emph{Estimation} seeks to \emph{identify} the value of a quantity which is \emph{fixed but unknown}; \emph{prediction} aims to \emph{forecast} the value of a \emph{random variable}. 

Under the least squares criterion, for example, we know that the optimal predictor for $Y$ is the conditional expectation $\condexp{Y}{X = x}$. This is an ordinary function which returns a real number for any $x \in \mathbb{R}^p$, and which can therefore be estimated from a sample. Such an estimate will contain some error, which we have to take into account when doing inference. If we additionally want to measure the error we make when predicting $Y$ using $\condexp{Y}{X = x}$, we also have to incorporate the intrinsic randomness or \emph{process variance} of the response variable. Prediction is therefore a two-stage procedure involving an intermediate estimation step.

Let's illustrate this in the case of the all-familiar linear regression model, which is given by
\begin{equation} \label{eq:linear-model}
    Y_i = \bm{\mathrm{x}_i}^T \bm{\beta} + \varepsilon_i \,, \qquad i \in {1, \dots, n}
\end{equation}
with $\expect{\varepsilon_i} = 0$, $\operatorname{Var}(\varepsilon_i) = \sigma$ and $\expect{\varepsilon_i \varepsilon_j} = 0$ for $i \neq j$. Considering the nonparametric bootstrap first, we need to identify a fundamental unit of resampling such that the resulting variables are interchangeable. One option would be to use some suitably standardised residuals which we obtain by fitting the model (see \cite[Algorithm 6.1]{davison}). This approach is sometimes referred to as \emph{semiparametric}, because it only uses the specification of certain aspects of the data distribution in terms of some parameters, but does not assume a specific form for it. Choosing, for example, the residuals
\begin{equation}
    r_i \coloneqq \frac{Y_i - \bm{\mathrm{x}}^T_i \widehat{\bm{\beta}}}{\widehat{\sigma} \sqrt{1 - h_{ii}}} \,,
\end{equation}
we resample these for $B$ times to obtain pseudo-residuals $r^{(b)}_1, \dots, r^{(b)}_n$, which in turn yield  pseudo-responses
\begin{equation}
    Y_i^{(b)} \coloneqq \mathbf{x}^T \widehat{\bm{\beta}} + \widehat{\sigma} \sqrt{1 - h_{ii}} r^{(b)}_i \,.
\end{equation}
By refitting the model to this new pseudo-data, we then obtain bootstrapped regression parameter estimates $\widehat{\bm{\beta}}^{(b)}$ for $1, \dots, B$. 

An alternative approach, which is fully nonparametric, is to resample the pairs $(\bm{\mathrm{x}_i}, Y_i)$ themselves (see \cites[Section 9.5]{efron:intro}[Algorithm 6.2]{davison}), which corresponds to approximating the multivariate distribution of $(X_1, \dots, X_n, Y)$ by the empirical distribution of the data. This has the significant benefit of parsimony, making no other assumption beside the i.i.d.-ness of the sample. The model is then fitted to the bootstrap samples $(\bm{\mathrm{x}_1}^{(b)}, Y^{(b)}_1), \dots, (\bm{\mathrm{x}_n}^{(b)}, Y^{(b)}_n)$ to produce pseudo-realisations $\widehat{\bm{\beta}}^{(b)}$ of the regression parameter estimator.

For the parametric case, we have to make an additional assumption about the distribution of $\epsilon$, the classical choice being the normal distribution. We then begin to fit \cref{eq:linear-model}, which gives us estimates $\widehat{\beta}$ for the regression parameters. With the help of a random number generator, we then produce bootstrap responses $Y^{(b)}_1, \dots, Y^{(b)}_n$ by drawing from the estimated distribution $\mathcal{N}(\bm{\mathrm{x}}^T \widehat{\bm{\beta}}, \widehat{\sigma}^2)$, and fit the model to this new data to obtain bootstrap samples $\widehat{\bm{\beta}}^{(b)}$ of the regression parameters.

\section{Process variance and predictive distributions} \label{sec:boot-proc}

If we want to do predictive inference on a regression model using the bootstrap, we additionally have to take into account the inherent variability of the response. Considering once again the example of the normal linear model, suppose we are interested in predicting the response $Y_+$ at new value $\mathbf{x_+}$ of the regressors. One way to quantify the accuracy of our forecast is to consider the \emph{prediction error}
\begin{equation}
  \delta \coloneqq Y_+ - \widehat{Y}_+
\end{equation}
(see \cite[Algorithm 6.4]{davison}). The second term in this expression can be bootstrapped using any one of the methods described in the previous section, as these yield replicates $\widehat{\bm{\beta}}^{(b)}$ of the parameter estimator and hence of the predictor $\widehat{Y}^{(b)}_+ = \mathbf{x_+}^T \widehat{\bm{\beta}}^{(b)}$. It then remains for us to produce bootstrap simulations of the response $Y_+$ itself. In the semiparametric approach, this can be achieved by resampling the residuals a second time to obtain pseudo-realisations $r^{(s)}$ for $s = 1, \dots, S$, and adding these (after correctly scaling them) to the value of the regression line at $x_+$. The resulting pseudo-responses $Y^{(s)}_+$ mimick the random fluctuations of $Y_+$, allowing us to approximate $\delta$ by the bootstrapped prediction errors
\begin{equation} \label{eq:pred-error-boot}
  \widehat{\delta}^{(b, s)} \coloneqq Y^{(s)}_+ - \widehat{Y}^{(b)}_+ = (\widehat{\mathbf{x}}^T_+ \widehat{\bm{\beta}} + r^{(s)}) - \widehat{\mathbf{x}}^T_+ \widehat{\bm{\beta}}^{(b)} \,.
\end{equation}

A similar procedure can be outlined for the parametric bootstrap by appropriately adjusting the method of generating bootstrap response replicates. The method cannot be applied to the pairs bootstrap, however, as it lacks a mechanism for simulating new response values by themselves; we must therefore borrow this part from one of the alternative approaches, forsaking part of its parsimony and robustness properties in the process. We can then use \Cref{eq:pred-error-boot} to do all kinds of inference, e.g. obtaining prediction intervals or computing the mean squared error of prediction.

While the prediction error offers an avenue of incorporating process variance into bootstrap procedures for predictive inference, it is not the most fitting one for our purposes. Recall from \Cref{chapter:intro} that we want to study how the reserve is impacted by violation of the assumptions of certain actuarial models, which can be framed in terms of regression, as we shall see in \Cref{chapter:mack,chapter:poisson}. In other words, we would like to simulate the distribution of the response itself. In \Cref{eq:pred-error-boot}, this was done by generating fluctuations around $\widehat{\bm{\beta}}$ through resampling (or in the parametric case, with the help of a random number generator). The trouble with this approach is that it fails to account for the error in our parameter estimates, leading to an underestimation of the prediction uncertainty. Although we will endeavour in this exposition to remain agnostic with respect to philosophical questions about the interpretation of probability, it will be necessary in this case, for reasons which will become apparent shortly, to borrow a concept from Bayesian school of statistics in order to address this problem.

Recall that the Bayesian point of view is premised on the idea that the parameters $\bm{\theta}$ governing a statistical model $p(y \mid \bm{\theta}, x_1, \dots, x_p)$ are themselves random variables. These are assumed to follow a so-called \emph{prior distribution} $p(\theta)$, which is a probabilistic expression of the beliefs we have about them before observing any data\sepfootnote{fn:bayes}. When presented with a sample $D = \{ (\bm{\mathrm{x}_1}, Y_1), \dots, (\bm{\mathrm{x}_n}, Y_n) \}$, we are then led to update our beliefs, and using the formula
\begin{equation}
  p(\bm{\theta} \mid D) \propto p(D \mid \bm{\theta}) p(\bm{\theta}) \,,
\end{equation}
which is known as \emph{Bayes' rule}, we obtain the \emph{posterior distribution} $p(\bm{\theta} \mid D)$ expressing the likelihood of different values of $\bm{\theta}$ given our observations. For any value of the parameters, the likelihood of the response at a new input, conditional on this value and the sample $D$, is now given by $p(y_+ \mid \bm{\theta}, D)$. By marginalising over the posterior distribution, we then incorporate all possible values of $\bm{\theta}$ in proportion to their likelihood under the data, resulting in the \emph{posterior predictive distribution}.
\begin{equation}
  p(y_+ \mid D) = \int p(y_+ \mid \bm{\theta}) p(\bm{\theta} \mid D) \, d \bm{\theta} \,.
\end{equation}
of $Y_+$ given $D$; this distribution incorporates both the intrinsic variability of the response as well as our uncertainty regarding the parameters (or, in the classical view, their estimates). Moreover, it can be very easily integrated into the bootstrap framework. Indeed, we can follow the same steps as when simulating the prediction errror, but instead of \Cref{eq:pred-error-boot}, we compute pseudo-realisations
\begin{equation} \label{eq:pred-dist}
  Y_+^{(b, s)} \coloneqq \mathbf{x}_+^T \widehat{\bm{\beta}}^{(b)} + \widehat{\sigma} \sqrt{1 - h_{ii}} r^{(s)} \,.
\end{equation}
where $h_{ii}$ is the leverage corresponding to the. 

\Cref{eq:pred-dist} also explains why it is difficult to fit this approach within a classical frequentist paradigm, as it is unclear in that case which theoretical quantity these bootstrap replicates would be approximating. We hasten to add that much work has been done to remedy this, leading, among other things, to the concept of a \emph{confidence distribution} (see, for example, \cites{barndorff-nielsen,lawless}). It is true, however, that there has been a multiplicity of disparate frequentist versions of the predictive distribution, as remarked for instance by \textcite{dickson}, lending credence to the idea that the notion fits more naturally into the Bayesian framework.

\chapter{Mack's model} \label{chapter:mack}

\section{Introduction} \label{sec:mack-intro}

In his seminal paper \cite{mack:chain-ladder}, Mack proposed the following model for cumulative claims triangles, which remains among the most influencial in actuarial reserving.
\begin{model}[Mack Chain Ladder] \label{model:mack} \leavevmode
    \begin{enumerate}[(i)]
        \item There exist development factors $f_1, \dots, f_{I - 1}$ such that
        \begin{equation} \label{eq:mack-expectation}
            \mathbb{E}[C_{ij} \ \Vert \ C_{i, j - 1}, \dots, C_{i1}] = \mathbb{E}[C_{ij} \ \Vert \ C_{i, j - 1}] = f_{j - 1} C_{i, j - 1}\,
        \end{equation}
        for $1 \leq i \leq I$.
        \item There exist parameters $\sigma_1, \dots, \sigma_{I - 1}$ such that
        \begin{equation} \label{eq:mack-variance}
            \mathrm{Var}[C_{ij} \ \Vert \ C_{i, j - 1}, \dots, C_{i1}] = \mathrm{Var}[C_{ij} \ \Vert \ C_{i, j - 1}] = \sigma_{j - 1}^2 C_{i, j - 1}\,,
        \end{equation}
        for $1 \leq i \leq I$.
        \item Cumulative claims processes $(C_{ij})_j, (C_{i'j})_j$ are independent for $i \neq i'$.
    \end{enumerate}
\end{model}
The development factors are estimated by
\begin{equation} \label{eq:devfac-estimator}
    \widehat{f}_j(\mathcal{D}_I) = \widehat{f}_j(C_{1j}, \dots, C_{I - j, j}, \dots, C_{1, j + 1}, \dots, C_{I - j, j + 1}) \coloneqq \frac{\sum_{i = 1}^{I - j} C_{i, j + 1}}{\sum_{i = 1}^{I - j} C_{i, j}} \,.
\end{equation}
If we define the \emph{single} or \emph{individual} development factors as
\begin{equation}
    F_{i, j + 1} \coloneqq \frac{C_{i, j + 1}}{C_{ij}} \,,
\end{equation}
then $\widehat{f}_j$ can be obtained as the weighted average
\begin{equation}
  \widehat{f}_j = \frac{\sum_{i = 1}^{I - j} C_{ij} F_{i, j}}{\sum_{i = 1}^{I - j} C_{ij}} \,.
\end{equation}
The $\sigma_j$ are estimated by
\begin{equation}
    \widehat{\sigma}_j \coloneqq \frac{1}{I-j}\sum_{i = 1}^{I-j} C_{ij}\left( F_{i, j + 1} - \widehat{f}_j \right)^2
\end{equation}
for $j < I - 1$. This formula does not work for $j = I - 1$, as we only have a single pair of observations in the last two columns of the triangle. To remedy this, Mack proposed a simple extrapolation from the previous development years, leading to the estimate
\begin{equation}
  \widehat{\sigma}^2_{I - 1} = \min{ \left \{ \frac{\widehat{\sigma}^4_{I - 2}}{\widehat{\sigma}^2_{I - 3}}, \widehat{\sigma}^2_{I - 2}, \widehat{\sigma}^2_{I - 3} \right \} }
\end{equation}
and this appears to be the most widely adopted solution in the literature.

Under the assumptions of \Cref{model:mack}, it can be shown (see \cite[17 \psqq]{wuthrich:stochastic-reserving}) that $\widehat{f}_j$ and $\widehat{\sigma}_j$ are (conditionally) unbiased, and moreover that the $\widehat{f}_j$ are uncorrelated. Predicted ultimate claim amounts $C_{iI}$ are obtained by substituting the estimates for the unknown development factors $f_j$ in the conditional expectation. In other words, we predict the ultimate loss using the conditional mean $\condexpp{C_{iI}}{C_{i, I + 1 - i}}$, and estimate the latter by plugging in $\widehat{f}_j$, yielding
\begin{equation}
    \widehat{C}_{iI} \coloneqq \widehat{\mathbb{E}}[C_{i, I} \ \Vert \ C_{i, I - i}] = C_{i, I + 1 - i} \prod_{j = I - i}^{I-1} \widehat{f}_j \,.
\end{equation}
From this, we then finally obtain the reserve predictor
\begin{equation} \label{eq:reserve-predictor}
    \widehat{R} = g(\mathcal{D}_I) \coloneqq \sum_{i = 2}^I (\widehat{C}_{i, I} - C_{i, I + 1- i}) \,. 
\end{equation}

\Cref{model:mack} is often referred to as "distribution-free" because it only makes assumptions about the first two moments of the claims triangle variables. Indeed, we will show that the Mack CL can be viewed as a series of linear regressions through the origin (i.e.\ without intercept term), hence these are same assumptions as for the Gauss-Markov theorem, i.e. the minimal ones\sepfootnote{fn:gauss-markov} required to guarantee optimality. Introduce, for any development year $j \in \{ 1, \dots, {I - 1} \}$, the notation
\begin{equation}
  \bm{\mathrm{c}_j} \coloneqq
  \begin{bmatrix}
    C_{1, j} \\
    \vdots \\
    C_{I - j, j}
  \end{bmatrix} \,,
\end{equation}
then the first two assumptions of \Cref{model:mack} can be equivalently stated as
\begin{equation}
  \bm{\mathrm{c}_{j + 1}} = f_j \bm{\mathrm{c}_j} + \bm{\varepsilon} \,,
\end{equation}
with $\bm{\varepsilon}$ a random vector satisfying
\begin{equation}
  \condexpp{\bm{\varepsilon}}{C_{1, j}, \dots, C_{i, I - j}} = \bm{0}
  \qquad%
  \condvarr{\bm{\varepsilon}}{C_{1, j}, \dots, C_{i, I - j}} = \sigma^2_j
  \begin{bmatrix}
    C_{1j} & & \\
    & \ddots & \\
    & & C_{I - j, j}
  \end{bmatrix} \,.
\end{equation}
Consequently, it follows (see \cite[Proposition 1.7]{hayashi}) that the weighted least squares method with weights matrix
\begin{equation}
  \mathbf{W} =
  \begin{bmatrix}
    1 / C_{1j} & & \\
    & \ddots & \\
    & & 1 / C_{I - j, j}
  \end{bmatrix} \,,
\end{equation}
leads to an estimator for $f_j$ which has minimal variance in the class of linear unbiased estimators. This estimator is given by
\begin{equation}
  \widehat{f}^{\mathrm{WLS}}_j = (\bm{\mathrm{c}_j}^T \mathbf{W} \bm{\mathrm{c}_j})^{-1} \bm{\mathrm{c}_j}^T \mathbf{W} = \frac{\sum_{i = 1}^{I - j} C_{i, j + 1}}{\sum_{i = 1}^{I - j} C_{i, j}} \,,
\end{equation}
which is the same expression as \cref{eq:devfac-estimator}.

\section{A challenging simulation} \label{sec:mack-challenge}

Owing to its recursive nature, Mack's model does not readily lend itself to application of the theory from \Cref{chapter:boot}. The actuarial literature on bootstrap methods is not very helpful in this regard either, as it has mostly tended to focus on generalised linear models---even papers like \cite{england:dist} which address the Mack CL do so by reframing it in this way. As will become clear shortly, this passes over some subtleties related to the particular structure of Mack's model, and we will therefore take a different approach. In particular, our starting point will be the problem of deriving a closed-form estimate of the so-called conditional \emph{mean square error of prediction} (MSEP) for the Mack predictor. While this might appear at first glance to be unrelated to the bootstrap, we will see that it furnishes us with the necessary theoretical framework to understand the special issues involved in resampling a recursive model.

The MSEP is a measure for the total uncertainty associated with a given predictive model. It is defined as the Euclidean distance between the predictor and the response in the underlying filtered probability space, i.e.\
\begin{equation}
    \underset{R \, \vert \, \mathcal{D}_I}{\mathrm{MSEP}}(\widehat{R}) \coloneqq \condexpp*{(\widehat{R} - R)^2}{\mathcal{D}_I}
\end{equation}
for our special case of predicting the reserve. The MSEP admits a decomposition, similar to the familiar bias-variance decomposition from classical statistics into so-called \emph{parameter} or \emph{estimation error} and \emph{process error}:
\begin{align}
    \begin{split}
        \condexpp*{(\widehat{R} - R)^2}{\mathcal{D}_I} &= \condexpp*{(R - \condexpp*{R}{\mathcal{D}_I})^2}{\mathcal{D}_I} + \condexpp*{(\condexpp*{R}{\mathcal{D}_I} - \widehat{R})^2}{\mathcal{D}_I} \\ &\phantom{{}=1} -2\condexpp*{(R - \condexpp*{R}{\mathcal{D}_I})(\condexpp*{R}{\mathcal{D}_I} - \widehat{R})}{\mathcal{D}_I}
    \end{split} \\
    \begin{split}
    &= \mathrm{Var}(R \, \Vert \, \mathcal{D}_I) + (\condexpp*{R}{\mathcal{D}_I} - \widehat{R})^2 \\
    &\phantom{{}=1} -2(\condexpp*{R}{\mathcal{D}_I} - \widehat{R})(\condexpp*{R - \condexpp*{R}{\mathcal{D}_I}}{\mathcal{D}_I})
    \end{split} \\
    &= \underbrace{\mathrm{Var}(R \, \Vert \, \mathcal{D}_I)}_\text{process error} + \underbrace{(\condexpp*{R}{\mathcal{D}_I} - \widehat{R})^2}_\text{estimation error} \,,
\end{align}
corresponding to the two stages of bootstrapping a predictor which we discussed in \Cref{sec:boot-reg}. Consider now, for any accident year $i \in \{ 1, \dots, I \}$, the MSEP for the associated ultimate
\begin{equation} \label{eq:msep}
    \underset{C_{iI} \, \vert \, \mathcal{D}_I}{\mathrm{MSEP}}(\widehat{C}_{iI}) = (\condexpp*{C_{iI}}{\mathcal{D}_I} - \widehat{C}_{iI})^2 + \condvarr{C_{iI}}{\mathcal{D}_I} \,,
\end{equation}
and suppose we are interested in obtaining a closed-form estimator for it. Such an expression can be derived relatively straightforwardly for the process error from the assumptions of \Cref{model:mack} in the following way. We begin by applying the law of total variance in conjunction with \cref{eq:mack-expectation,eq:mack-expectation} to obtain
\begin{align}
  \condvarr{C_{iI}}{\mathcal{D}_I} &= \condvarr{C_{iI}}{C_{i, I + 1 - i}} \\
  &= \condexpp{\condvarr{C_{iI}}{C_{i, I - 1}}}{C_{i, I + 1 - i}} + \condvarr{\condexpp{C_{iI}}{C_{i, I - 1}}}{C_{i, I + 1 - i}} \\
  &= \sigma^2_{I - 1} \condexpp{C_{i, I - 1}}{C_{i, I + 1 - i}} + f^2_{I - 1} \condvarr{C_{i, I - 1}}{C_{i, I + 1 - i}} \\
  &= \sigma^2_{I - 1} C_{i, I + 1 - i} \prod_{j = I + 1 - i}^{I - 2} f_j + f^2_{I - 1} \condvarr{C_{i, I - 1}}{C_{i, I + 1 - i}} \,,
\end{align}
which is a linear recurrence equation of the form
\begin{equation}
  x_n = a_{n - 1} x_{n - 1} + g_{n - 1}
\end{equation}
with $x_n = \condvarr{C_{in}}{C_{i, I + 1 - i}}$ and
\begin{equation}
  g_{n - 1} = \sigma^2_{n - 1} C_{i, I + 1 - i} \prod_{j = I + 1 - i}^{n - 1} f_j \,, \qquad a_{n - 1} = f^2_{n - 1} \,. 
\end{equation}
The general solution is given by
\begin{equation}
  x_n = \left( \prod_{j = n_0}^{n - 1} a_j \right) \left( x_{n_0} + \sum_{k = n_0}^{n - 1} \frac{g_k}{\prod_{l = n_0}^k a_l} \right)
\end{equation}
where $n_0$ denotes the first index of the sequence $x_n$, in our case $I + 1 - i$. Using the initial condition $x_{I + 1 - i} = \condvarr{C_{i, I + 1 - i}}{C_{i, I + 1 - i}} = 0$, we finally obtain
\begin{align}
  \condvarr{C_{iI}}{\mathcal{D}_I} 
  &= \left( \prod_{j = I + 1 - i}^{I - 1} f^2_j \right) \left( \sum_{k = I + 1 - i}^{I - 1} \frac{\sigma^2_k C_{i, I + 1 - i} \prod_{j = I + 1 - i}^{k - 1} f_j}{\prod_{j = I + 1 - i}^k f^2_j} \right) \\
  &= \left( \prod_{j = I + 1 - i}^{I - 1} f^2_j \right) C^2_{i, I + 1 - i} \left( \sum_{k = I + 1 - i}^{I - 1} \frac{\sigma^2_k / f^2_k}{\prod_{j = I + 1 - i}^{k - 1} f_j C_{i, I + 1 - i}} \right) \\
  &= \condexpp{C_{iI}}{C_{I + 1 - i}}^2 \sum_{k = I + 1 - i}^{I - 1} \frac{\sigma^2_k / f^2_k}{\condexpp{C_{ik}}{C_{i, I + 1 - i}}} \,,
\end{align}
which we can estimate by plugging in $\widehat{f}_j$ and $\widehat{\sigma}_j$ for $f_j$ and $\sigma_j$, respectively. 

For the parameter error, if we use the definitions from the previous section to rewrite it as
\begin{align} \label{eq:param-err}
  (\condexpp*{C_{iI}}{\mathcal{D}_I} - \widehat{C}_{iI})^2 &= C^2_{i, I+1-i} \left( \prod_{j=I+1-i}^{I-1}f_j - \prod_{j=I+1-i}^{I-1}\widehat{f}_j \right)^2 \\
  &= C^2_{i, I+1-i} \left( \prod_{j=I+1-i}^{I-1}f^2_j + \prod_{j=I+1-i}^{I-1}\widehat{f}^2_j - 2 \prod_{j=I+1-i}^{I-1}f_j \widehat{f}_j \right) \,,
\end{align}
it becomes clear that things are more complicated than with process error. Indeed, we cannot simply subtitute the $\widehat{f}_j$ for the unknown parameters in this expression as that would cause it to vanish, yielding an estimate which will generally not be accurate. This problem was recognised by Mack himself in \cite{mack:chain-ladder-variability}, and is caused by the fact that the claims triangle observations are used for both estimation and forecasting (see \cite[Section 2]{lindholm:msep} for a more general discussion). His suggested solution was to apply some kind of conditional averaging to the $\widehat{f}_j$. Ideally, one would like to condition on all available observations in $\mathcal{D}_I$, but the $\mathcal{D}_I$-measurability of the $\widehat{f}_j$ would then bring us right back where we started. We must therefore use a smaller set in order to allow $\widehat{f}_{I + 1 - i}, \dots, \widehat{f}_{I - 1}$ to fluctuate around $f_{I + 1 - i}, \dots, f_{I - 1}$. This corresponds to asking which other values $\widehat{f}_j$ could have taken, given that we fix a certain subset of the data---in other words, it's a resampling scheme on the parameter estimates. Thus, one can obtain an estimate of the parameter error by specifying a mechanism for generating new realisations of $\widehat{f}_j$ (see \cites{wuthrich:chain-ladder-msep}[44 \psqq]{wuthrich:stochastic-reserving}), with different mechanisms yielding different estimates. The literature uses this mostly as a theoretical device to facilitate analytical calculations; for the specific approach developed by Mack, it leads to the estimator
\begin{equation} \label{eq:mack-msep-estimator}
  \widehat{\mathrm{MSEP}}(\widehat{R_i}) \coloneqq \widehat{C}_{iI} \sum_{j = I + 1 - i}^{I - 1} \frac{\widehat{\sigma}^2_j}{\widehat{f}_j} \left( \frac{1}{\widehat{C}_{ij}} + \frac{1}{\sum_{i = 1}^{I - j} C_{ij}} \right)
\end{equation}
(see \cite[11]{mack:chain-ladder-variability}).
In this case, however, the theory happens to fit in perfectly with the resampling framework, and we can therefore employ it as a basis for bootstrap procedures. In the remainder of this section, we outline two approaches for estimating \cref{eq:param-err} and indicate the corresponding resampling methods.

Denote the subset of observations in $\mathcal{D}_I$ up to and including development year $k$ by
\begin{equation}
    \mathcal{B}_k \coloneqq \left \{ C_{ij} \in \mathcal{D}_I \mid j \leq k \right \} \,,
\end{equation}
and write
\begin{equation}
    \mathcal{D}^O_{I, k} \coloneqq \left \{ C_{ij} \in \mathcal{D}_I \mid j > I + 1 - k \right \}
\end{equation}
for its complement. One option would then be to take the conditional expectation of $\widehat{f}_j$ with respect to $\mathcal{B}_{I+1-i}$, leading to the estimate
\begin{align} \label{eq:uncond-param-err}
    \condexpp{(\condexpp{C_{iI}}{\mathcal{D}_I} - \widehat{C}_{iI})^2}{\mathcal{B}_{I+1-i}} &= C^2_{i, I+1-i} \condexpp*{\left( \prod_{j=I+1-i}^{I-1}f_j - \prod_{j=I+1-i}^{I-1}\widehat{f}_j \right)^2}{\mathcal{B}_{I+1-i}} \\
    &= C^2_{i, I+1-i} \left( \condexpp*{\prod_{j=I+1-i}^{I-1}\widehat{f}^2_j}{\mathcal{B}_{I+1-i}} - \prod_{j=I+1-i}^{I-1}f^2_j \right)\,,
\end{align}
where we used the fact that the $\widehat{f}_j$ are uncorrelated. This corresponds to averaging over the distribution of $\mathcal{D}^O_{I, k}$, or, expressed in terms of resampling, to generating new observations in the upper right triangle. Borrowing the nomenclature from \cite{wuthrich:chain-ladder-msep}, we call this the \emph{unconditional approach}. Alternatively, we could average each $\widehat{f}_j$ only over the observations after $j$. This is equivalent to fixing the denominator $\sum_{i = 1}^{I - j} C_{ij}$ in the development factor estimator \cref{eq:devfac-estimator} and allowing the numerator $\sum_{i = 1}^{I - j} C_{i, j + 1}$ to vary. Formally, it corresponds to taking the expectation with respect to the probability measure defined on $\mathcal{D}^O_{I, i}$ by
\begin{equation}
  \mathbb{P}^*_{\mathcal{D}_I}(\{ dz_{ij} \}_{i + j \leq I + 1}) \coloneqq \prod_{j = 1}^{I - 1} \prod_{i = 1}^{I - j}  \mathbb{P}_{C_{i, j + 1}}(dz_{i, j + 1} \mid C_{ij} = c_{ij}) \,,
\end{equation}
yielding the estimate
\begin{align}
    \expect[\mathbb{P}^*_{\mathcal{D}_I}]{(\condexpp{C_{iI}}{\mathcal{D}_I} - \widehat{C}_{iI})^2}&= C^2_{iI} \, \expect[\mathbb{P}^*_{\mathcal{D}_I}]{\left( \prod_{j=I+1-i}^{I-1}f_j - \prod_{j=I+1-i}^{I-1}\widehat{f}_j \right)^2} \\ 
    &= C^2_{i, I+1-i} \left( \expect[\mathbb{P}^*_{\mathcal{D}_I}]{\prod_{j=I+1-i}^{I-1}\widehat{f}^2_j} - \prod_{j=I+1-i}^{I-1}f^2_j \right) \\
    &= C^2_{i, I+1-i} \left( \prod_{j=I+1-i}^{I-1} \condexpp*{\widehat{f}^2_j}{\mathcal{B}_j} - \prod_{j=I+1-i}^{I-1}f^2_j \right) \,.
\end{align}
We refer to this as the \emph{conditional approach}, and it corresponds to a scheme in which only the observations from the next period are resampled to produce a new realisation of the parameter estimate for the current period.

There has been some controversy about which of these approaches should be preferred, leading to the vigorous discussion found in \cite{wuthrich:chain-ladder-msep, mack:msep, gisler:msep, venter:msep}. As we will see in \Cref{sec:mack-boot}, difference between the results which they produce is negligeable, and so the question is mainly of theoretical interest. Nevertheless, based on the previous exposition, it seems reasonable to prefer whichever method produces resampled parameter estimates approximating the original $\widehat{f}_j$ most closely. In particular, we note that these posses the following property, the proof of which can be found in \cite{mack:msep}.

\begin{theorem}
  The squares of two successive development factor estimates in the Mack chain ladder are negatively correlated:
  \begin{equation}
    \mathrm{Cov}(\widehat{f}_j, \widehat{f}_{j - 1}) < 0 \,.
  \end{equation}
\end{theorem}

\noindent In the conditional approach, the resampled parameter estimates are independent by construction, and so they cannot incorporate this covariance structure. In light of this, it would appear that the unconditional scheme has slightly better theoretical properties. As the empirical difference between the two is minimal, however, the conditional version is a reasonable approximation to fall back on when needed. In the next section, we will see how both approaches give rise to a variety of different bootstrap methods.

\section{Bootstrap methodology} \label{sec:mack-boot}

In \Cref{sec:boot-reg}, we introduced a taxonomy for the different kinds of bootstrap, distinguishing between the semiparameteric, nonparametric and parametric type. We now consider how each of these can be applied to \Cref{model:mack}. For comparison, \Cref{tab:mack-bench} shows the results of applying \Cref{model:mack} with the estimator \Cref{eq:mack-msep-estimator} to the dataset from \Cref{tab:uk-motor}.

\begin{table}[!htb]
  \centering
  \begin{tabular}{|m{5em}|m{5em}|m{5em}|m{5em}|m{5em}|}\hline%
    $i \,/ \,j$ & $\widehat{f}^\mathrm{CL}_j$ & $\widehat{\sigma}^\mathrm{CL}_i$ & $\widehat{R}_i^\mathrm{CL}$ & $\widehat{\mathrm{MSEP}}(\widehat{R}_j)$ \\ \hline
    \csvreader[
      head to column names,
      late after line = \\\hline
    ]{%
      ../results/example/mack_bench.csv
    }{}{%
      \idx & \devfacs & \sigmas & \reserve & \prederror
    }%
  \end{tabular}
\caption{Mack CL results for UK Motor triangle}
\label{tab:mack-bench}
\end{table} 

For the semiparametric bootstrap, the crucial step is to find a suitable definition for the residuals which ensures that they are interchangeable. The distribution-free nature of the model makes this difficult, however, as it limits the statements we can make about the errors to the first two moments. We can resolve this in one of two ways. The first option would be to extrapolate from homogeneity of the first two moments to homogeneity of the distributions. In that case, the \emph{raw residuals}
\begin{equation}
  e_{i, j + 1} \coloneqq C_{i, j + 1} - \widehat{C}_{i, j + 1} = C_{i, j + 1} - \widehat{f}_j C_{ij}
\end{equation}
are not an option, as these suffer from heteroscedasticity,
\begin{equation}
  \condvarr{e_{i, j + 1}}{C_{ij}} = \sigma^2_j \left( C_{ij} - \frac{C^2_{ij}}{\sum_{i = 1}^{I - j} C_{ij}} \right) \,.
\end{equation}
We can address this by dividing out this variance, i.e. we consider the errors
\begin{equation}
  \varepsilon_{i, j + 1} \coloneqq \frac{C_{i, j + 1} - f_j C_{ij}}{\sigma_j \sqrt{C_{ij}} \sqrt{1 - \frac{C_{ij}}{\sum_{i = 1}^{I - j} C_{ij}}}} \,,
\end{equation}
which satisfy $\condexpp{\varepsilon_{i, j + 1}}{C_{ij}} = 0$ and $\condvarr{\varepsilon_{i, j + 1}}{C_{ij}} = 1$. Provided the sampling variability of the $\widehat{f}_j$ and $\widehat{\sigma}_j$ is not too bad (which is not obvious given the small sample sizes we're usually dealing with), the same should hold approximately for the corresponding residuals
\begin{equation}
  r_{i, j + 1} \coloneqq \frac{C_{i, j + 1} - \widehat{f}_j C_{ij}}{\widehat{\sigma}_j \sqrt{C_{ij}} \sqrt{1 - \frac{C_{ij}}{\sum_{i = 1}^{I - j} C_{ij}}}} \,,
\end{equation}
obtained by substituting these estimators. Note that the factor $\sqrt{1 - \frac{C_{ij}}{\sum_{i = 1}^{I - j} C_{ij}}}$ in the denominator corresponds to the leverage adjustment, as can be seen by computing the hat matrix:
\begin{align}
  \mathbf{H} &= \bm{\mathrm{c}_j} (\bm{\mathrm{c}_j}^T \mathbf{W} \bm{\mathrm{c}_j})^{-1} \bm{\mathrm{c}_j}^T \mathbf{W} \\[4pt]
  &= \frac{1}{\sum_{i = 1}^{I - j} C_{ij}}
  \begin{bmatrix}
    C_{1j} & \dots & C_{1j} \\
    \vdots & \ddots & \vdots\\
    C_{I - j, j} & \dots & C_{I - j, j}
  \end{bmatrix} \,.
\end{align}
It's worth emphasising, however, that the above extrapolation should not be made lightly, as it is perfectly possible for the error distribution to exhibit heterogeneity in other ways than through its mean and variance (see \cite[114]{efron:intro} for an example where the \emph{percentiles} vary with the value of the regressor). In light of this, an alternative approach would be to augment our model with some explicit distributional assumptions, which is more transparent and allows us to make precise statements about errors and residuals. One such augmentation that has been studied in the literature (see \cite[49]{wuthrich:stochastic-reserving}) is the autoregressive Gaussian time series model
\begin{equation} \label{eq:time-series-model}
    C_{i, j + 1} = f_j C_{ij} + \sigma_j \, \sqrt{C_{ij}} \, \varepsilon_{i, j + 1}, \qquad \varepsilon \sim \mathcal{N}(0, 1) \,,
\end{equation}
which can easily be seen to be compatible with \Cref{model:mack}. Because the Mack CL can be viewed as a series of weighted linear regressions, as we say in \Cref{sec:mack-intro}, this has the benefit of making available to us the results of classical regression theory. We know, for example, that the \emph{externally studentised residuals}
\begin{equation}
  r_{i, j + 1} 
  \coloneqq \frac{e_{i, j + 1}}{\widehat{\sigma}_{j (i)} \sqrt{1 - \mathbf{H}_{ii}}} \sqrt{\mathbf{W}_{ii}} 
  = \frac{C_{i, j + 1} - \widehat{f}_j C_{ij}}{\widehat{\sigma}_{j (i)} \sqrt{C_{ij}} \sqrt{1 - \frac{C_{ij}}{\sum_{i = 1}^{I - j} C_{ij}}}} \,,
\end{equation}
with $\widehat{\sigma}_{j (i)}$ denoting the leave-$i$-out estimator of $\sigma_j$, follow a $t_{I - j - 1}$ distribution. Another option are the \emph{standardised} or \emph{internally studentised} residuals
\begin{equation}
  r_{i, j + 1} \coloneqq \frac{C_{i, j + 1} - \widehat{f}_j C_{ij}}{\widehat{\sigma}_j \sqrt{C_{ij}} \sqrt{1 - \frac{C_{ij}}{\sum_{i = 1}^{I - j} C_{ij}}}} \,
\end{equation}
which also share the same distribution, albeit a more complicated one (see \cite[267 \psqq]{seber}). 

The Gaussian model has a major shortcoming: it makes it possible to have a negative realisation in the next step of the time series, in which case all future observations from that point on are undefined, because of the square root factor appearing in the variance. This is not merely a theoretical problem: we have sometimes observed this phenomenon in our numerical implementation, where the resampling produces negative pseudo-realisations of certain claim amounts, particularly when the model has been severely perturbed. One obvious way of avoiding it would be to simply discard the current bootstrap iteration as soon as a negative value is produces. This has the drawback of requiring more computational power, sometimes beyond the realm of what is reasonable. Moreover, we have seen cases in which the probability of having no negative replicates was so vanishingly small as to cause the program to get stuck indefinitely. We must therefore develop an alternative approach, if only to have a fail-safe method to fall back upon in case of problems with \Cref{eq:time-series-model}.

Fundamentally, we must find of guaranteeing
\begin{equation} \label{eq:lower-limit-err}
  \varepsilon_{i, j + 1} > -\frac{f_j C_{ij}}{\sigma_j} \,.
\end{equation}
This can be achieved by choosing an alternative error distribution in \cref{eq:time-series-model}, one whose support is bounded from below. For example, we could use a shifted lognormal distribution, i.e.
\begin{equation}
  \log(\varepsilon_{i, j + 1} - \alpha_{ij}) \sim \mathcal{N}(\beta_{ij}, \gamma_{ij}^2)
\end{equation} 
for certain parameters $\alpha_{ij}$, $\beta_{ij}$ and $\gamma_{ij}$. This has support $(\alpha_{ij}, +\infty)$, so that the requirement \cref{eq:lower-limit-err} is satisfied with $\alpha_{ij} = -\frac{f_j C_{ij}}{\sigma_j}$. It then remains for us to determine $\beta_{ij}$ and $\gamma_{ij}$ such that the assumptions of \Cref{model:mack} are satisfied, meaning
\begin{equation}
  \condexpp{\varepsilon_{i, j + 1}}{C_{ij}} = \exp \left( \beta_{ij} + \frac{\gamma_{ij}^2}{2} \right) + \alpha_{ij} = 0, \qquad \condvarr{\varepsilon_{i, j + 1}}{C_{ij}} = (\exp{\gamma_{ij}^2} - 1) \exp(\gamma_{ij}^2 + 2 \beta_{ij}) = 1 \,.
\end{equation}
Solving these equations, we obtain
\begin{equation}
  \gamma_{ij} = \sqrt{\log \left( 1 - \frac{1}{\alpha^2_{ij}} \right)}, \qquad \beta_{ij} = \log(-\alpha_{ij}) - \frac{\gamma_{ij}}{2} \,.
\end{equation}
Provided the sampling variability of $\widehat{f}_j$ and $\widehat{\sigma}_j$ is not too severe, this means that the residuals
\begin{align}
  r_{i, j + 1} 
  &\coloneqq \frac{\log \left( C_{i, j + 1} - \widehat{f}_j C_{ij} + \widehat{f}_j C_{ij} / \widehat{\sigma}_j \right) - \widehat{\beta}_{ij}}{\widehat{\gamma}_{ij}} \\
  &= \frac{\log \left( C_{i, j + 1} - \widehat{f}_j C_{ij} + \widehat{f}_j C_{ij} / \widehat{\sigma}_j \right) - \log(\widehat{f}_j \sqrt{C_{ij}} / \widehat{\sigma}_j) + \frac{1}{2} \left( \sqrt{\log \left( 1 + \widehat{\sigma}_j / \widehat{f}_j C_{ij} \right)} \right)}{\sqrt{\log \left( 1 + \widehat{\sigma}_j / \widehat{f}_j C_{ij} \right)}}
\end{align}
are approximately $\mathcal{N}(0, 1)$-distributed.

After selecting a particular type of residual, the next step is to fit the model and compute the residuals from it. We then resample these to generate bootstrap residuals $r^{(b)}_{ij}$, from which a bootstrap triangle is obtained by inverting the appropriate residuals formula. In the conditional approach, the inversion is based on the original triangle, whereas the unconditional version uses the previously generated bootstrap observations. Finally, the model is refitted to the generated triangle to obtain bootstrap development factor and dispersion parameter estimators $\widehat{\bm{f}}$ and $\widehat{\bm{\sigma}}$. The entire procedure is outlined in \Cref{alg:cond-semiparam-mack,alg:uncond-semiparam-mack} for the case of standardised residuals, and the results for the example data from \Cref{tab:uk-motor} are given in \Cref{tab:semiparam-mack-res}.

\begin{algorithm}[!htb]
  \caption{Conditional semiparametric bootstrap for Mack CL} 
  \label{alg:cond-semiparam-mack}
  \begin{algorithmic} 
    \Require{Cumulative claims triangle $\mathcal{D}_I$, required number of bootstrap samples $B$}
    \vspace{3.5pt}
    \State $(\{ r_{ij} \mid i + j \leq I + 1 \}, \bm{\widehat{f}}, \bm{\widehat{\sigma}}) \gets$ \Call{fit}{$\mathcal{D}_I$} 
    \vspace{3.5pt}
    \For{$b \gets 1, B$}
      \vspace{3.5pt}
      \State $\{ r^{(b)}_{ij} \mid i + j \leq I + 1 \} \gets$ \Call{resample}{$\{ r_{ij} \mid i + j \leq I + 1 \}$}
      \vspace{3.5pt}
      \For{$j \gets 1, I - 1$}
        \For{$i \gets 1, I - j$}
          \vspace{3.5pt}
          \State $C^{(b)}_{i, j + 1} \gets \widehat{f}_j C_{ij} + \widehat{\sigma}_j \sqrt{C_{ij}} \sqrt{1 - \frac{{C_{ij}}}{\sum_{i = 1}^{I - j} C_{ij}}} r_{i, j + 1}$
          \vspace{3.5pt}
          \State $\displaystyle F^{(b)}_{i, j + 1} \gets C^{(b)}_{i, j + 1} / C_{ij}$
          \vspace{3.5pt}
        \EndFor
        \vspace{3.5pt}
        \State $\widehat{f}^{(b)}_j \gets \sum_{i = 1}^{I - j} C^{(b)}_{i, j + 1} / \sum_{i = 1}^{I - j} C_{ij}$
        \vspace{3.5pt}
        \If{$j < I - 1$}
          \vspace{3.5pt}
          \State $\displaystyle \widehat{\sigma}^{(b)}_j \gets \frac{1}{I - j - 1}\sum_{i = 1}^{I - j} C_{ij} \left( F^{(b)}_{i, j + 1} - \widehat{f}^{(b)}_j \right)^2$
          \vspace{3.5pt}
        \Else
          \vspace{3.5pt}
          \State $\widehat{\sigma}^{(b)}_{I - 1} \gets \sqrt{\min{ \left \{ \displaystyle \frac{(\widehat{\sigma}^{(b)}_{I - 2})^4}{(\widehat{\sigma}^{(b)}_{I - 3})^2}, (\widehat{\sigma}^{(b)}_{I - 2})^2, (\widehat{\sigma}^{(b)}_{I - 3})^2 \right \} }}$
          \vspace{3.5pt}
          \EndIf
        \EndFor
    \EndFor
    \State \Return $\{ (\widehat{\bm{f}}^{(b)}, \widehat{\bm{\sigma}}^{(b)}) \mid b = 1, \dots, B \}$
  \end{algorithmic}
\end{algorithm}
%
\begin{algorithm}[!htb]
  \caption{Unconditional semiparametric bootstrap for Mack CL} 
  \label{alg:uncond-semiparam-mack}
  \begin{algorithmic} 
    \Require{Cumulative claims triangle $\mathcal{D}_I$, required number of bootstrap samples $B$}
    \vspace{5pt}
    \State $(\{ r_{ij} \mid i + j \leq I + 1 \}, \bm{\widehat{f}}, \bm{\widehat{\sigma}}) \gets$ \Call{fit}{$\mathcal{D}_I$} 
    \vspace{5pt}
    \For{$b \gets 1, B$}
      \For{$i \gets 1, I$}
        \vspace{5pt}
        \State $C^{(b)}_{i1} \gets C_{i1}$
        \vspace{5pt}
      \EndFor
      \For{$j \gets 1, I - 1$}
        \For{$i \gets 1, I - j$}
          \vspace{5pt}
          \State $C^{(b)}_{i, j + 1} \gets \widehat{f}_j C^{(b)}_{ij} + \widehat{\sigma}_j \sqrt{C^{(b)}_{ij}} \sqrt{1 - \frac{{C^{(b)}_{ij}}}{\sum_{i = 1}^{I - j} C^{(b)}_{ij}}} r_{i, j + 1}$
          \vspace{5pt}
          \State $\displaystyle F^{(b)}_{i, j + 1} \gets C^{(b)}_{i, j + 1} / C^{(b)}_{ij}$
          \vspace{5pt}
        \EndFor
        \vspace{5pt}
        \State $\widehat{f}^{(b)}_j \gets \sum_{i = 1}^{I - j} C^{(b)}_{i, j + 1} / \sum_{i = 1}^{I - j} C^{(b)}_{ij}$
        \vspace{5pt}
        \If{$j < I - 1$}
          \State $\displaystyle \widehat{\sigma}^{(b)}_j \gets \frac{1}{I - j - 1}\sum_{i = 1}^{I-j} C^{(b)}_{ij}\left( F^{(b)}_{i, j + 1} - \widehat{f}^{(b)}_j \right)^2$
        \Else
          \vspace{5pt}
          \State $\widehat{\sigma}^{(b)}_{I - 1} \gets \sqrt{\min{ \left \{ \displaystyle \frac{(\widehat{\sigma}^{(b)}_{I - 2})^4}{(\widehat{\sigma}^{(b)}_{I - 3})^2}, (\widehat{\sigma}^{(b)}_{I - 2})^2, (\widehat{\sigma}^{(b)}_{I - 3})^2 \right \} }}$
          \vspace{5pt}
        \EndIf
      \EndFor
      \vspace{5pt}
    \EndFor
    \State \Return $\{ (\widehat{\bm{f}}^{(b)}, \widehat{\bm{\sigma}}^{(b)}) \mid b = 1, \dots, B \}$
  \end{algorithmic}
\end{algorithm}

\begin{table}[!htb]
  \centering
  \begin{subtable}{0.45\textwidth}
    \centering
    \begin{tabular}{|m{1em}|m{2em}|m{2em}|m{3.5em}|m{4em}|}\hline%
      $j$ & $\widehat{f}^B_j$ & $\widehat{\sigma}^B_j$ & $\widehat{R}_j^B$ & \resizebox{4em}{!}{$\widehat{\mathrm{MSEP}}(\widehat{R}_j)$} \\ \hline
      \csvreader[
        head to column names,
        late after line = \\\hline
      ]{%
        ../results/example/mack_semiparam_cond.csv
      }{}{%
        \idx & \devfacs & \sigmas & \reserve & \prederror
      }%
    \end{tabular}
    \subcaption{Conditional}
  \end{subtable}
  \begin{subtable}{0.45\textwidth}
    \centering
    \begin{tabular}{|m{1em}|m{2em}|m{2em}|m{3.5em}|m{4em}|}\hline%
      $j$ & $\widehat{f}^B_j$ & $\widehat{\sigma}^B_j$ & $\widehat{R}_j^B$ & \resizebox{4em}{!}{$\widehat{\mathrm{MSEP}}(\widehat{R}_j)$} \\ \hline
      \csvreader[
        head to column names,
        late after line = \\\hline
      ]{%
        ../results/example/mack_semiparam_uncond.csv
      }{}{%
        \idx & \devfacs & \sigmas & \reserve & \prederror
      }%
    \end{tabular}
    \subcaption{Unconditional}
  \end{subtable}
\caption{Semiparametric bootstrap results}
\label{tab:semiparam-mack-res}
\end{table}

Next, we consider the fully nonparametric bootstrap, in which we resample the pairs \linebreak
$(C_{ij}, C_{i, j + 1})$ at every development year index $j$. For this procedure, we have no choice regarding the resampling scheme which is used: the only possibility is conditional resampling. To see why this is the case, consider what it would mean to implement unconditional resampling. If the resampled pairs for the first two columns are denoted by
\begin{displaymath}
\{ (C^*_{11}, C^*_{12}), \dots, (C^*_{I - j, 1}, C^*_{I - j, 2}) \} \,,
\end{displaymath}
this would mean using the generated $C^*_{i2}$ as the regressor column in the second step. However, as the pairs are fundamental i.i.d. unit for this method, we have to ensure that these remain paired to the same response from the third column. In effect, this means that we are forced to permute the rows of the triangle at every stage. But this creates a problem: the last point in the second column does not have a successor in the triangle, and we therefore become stuck in the second step if we had previously drawn it, as illustrated in \Cref{fig:uncond-pairs-resample}. 
\begin{figure}[!htb]
  \centering
  \begin{tikzpicture}
    \node (a) at (0, 0){
      \begin{tabular}{c c c c} \cline{1-2}
        \multicolumn{1}{|c}{$C_{11}$} & \multicolumn{1}{c|}{$C_{12}$} & $C_{13}$ & $C_{14}$ \\ 
        \multicolumn{1}{|c}{$C_{21}$} & \multicolumn{1}{c|}{$C_{22}$} & $C_{23}$ & $C_{24}$ \\ 
        \multicolumn{1}{|c}{$C_{31}$} & \multicolumn{1}{c|}{$C_{32}$} & $C_{33}$ & \\ 
        \multicolumn{1}{|c}{$C_{41}$} & \multicolumn{1}{c|}{$C_{42}$} & \\ \cline{1-2}
        $C_{51}$ &  &
      \end{tabular}
    };
    \node (b) at (5, 0) {
      \begin{tabular}{c c c c} \cline{2-3}
        $C_{41}$ & \multicolumn{1}{|c}{$C_{42}$} & \multicolumn{1}{c|}{} & \\
        $C_{31}$ & \multicolumn{1}{|c}{$C_{32}$} & \multicolumn{1}{c|}{$C_{33}$} & $C_{34}$ \\
        $C_{11}$ & \multicolumn{1}{|c}{$C_{12}$} & \multicolumn{1}{c|}{$C_{13}$} & $C_{14}$ \\ \cline{2-3}
        $C_{31}$ & $C_{32}$ & $C_{33}$\\ 
        $C_{51}$ &  
      \end{tabular}
    };
    \node (c) at (8, 0) {\large \bf{?}};
    \path[->, thick]
        (a.east) edge (b.west)
        (b.east) edge (c.west);      
  \end{tikzpicture}
  \caption{Failure of unconditional pairs resampling}
  \label{fig:uncond-pairs-resample}
\end{figure}
Hence the only option is to carry out the resampling of each column pairs independently of the others, using the original data, and compute bootstrapped development factor and dispersion parameter estimates from these. The entire procedure is outlined in \Cref{alg:pairs-mack}, and the results for the UK Motor dataset are given in \Cref{tab:pairs-mack-res}.

\begin{algorithm}[!htb]
  \caption{Pairs bootstrap for Mack CL}
  \label{alg:pairs-mack}
  \begin{algorithmic}
    \Require{Cumulative claims triangle $\mathcal{D}_I$, required number of bootstrap samples $B$, parameter \textsc{conditional} specifying the resampling approach}
    \For{$b \gets 1, B$}
      \For{$j \gets 1, I - 1$}
        \vspace{5pt}
        \State $\{ (C^{(b)}_{i, j}, C^{(b)}_{i, j + 1}) \mid i = 1, \dots, I - j \} \gets$ \Call{resample}{$\{ (C_{i, j}, C_{i, j + 1}) \mid i = 1, \dots, I - j \}$}
        \vspace{5pt}
        \State $\widehat{f}^{(b)}_j \gets \sum_{i = 1}^{I - j} C^{(b)}_{i, j + 1} / \sum_{i = 1}^{I - j} C^{(b)}_{ij}$
        \vspace{5pt}
        \For{$i \gets 1, I - j$}
          \State $\displaystyle F^{(b)}_{i, j + 1} \gets C^{(b)}_{i, j + 1} / C^{(b)}_{ij}$
          \vspace{5pt}
        \EndFor
        \State $\displaystyle \widehat{\sigma}^{(b)}_j \gets \frac{1}{I-j}\sum_{i = 1}^{I-j} C^{(b)}_{ij}\left( F^{(b)}_{i, j + 1} - \widehat{f}^{(b)}_j \right)^2$
      \EndFor
    \EndFor
    \State \Return $\{ (\widehat{\bm{f}}^{(b)}, \widehat{\bm{\sigma}}^{(b)}) \mid b = 1, \dots, B \}$
  \end{algorithmic}
\end{algorithm}

\begin{table}[!htb]
  \centering
  \begin{tabular}{|m{5em}|m{5em}|m{5em}|m{5em}|m{5em}|}\hline%
    $j$ & $\widehat{f}^B_j$ & $\widehat{\sigma}^B_j$ & $\widehat{R}_j^B$ & \resizebox{5em}{!}{$\widehat{\mathrm{MSEP}}(\widehat{R}_j)$} \\ \hline
    \csvreader[
      head to column names,
      late after line = \\\hline
    ]{%
      ../results/example/mack_pairs.csv
    }{}{%
      \idx & \devfacs & \sigmas & \reserve & \prederror
    }%
  \end{tabular}
\caption{Pairs bootstrap results}
\label{tab:pairs-mack-res}
\end{table}

Finally, we discuss the parametric bootstrap, in which we simulate directly from the fitted model. Based on \Cref{eq:time-series-model}, we might be tempted to simply substitute $\mathcal{N}(0, 1)$-distributed draws from a random number generator for the residuals in \Cref{alg:cond-semiparam-mack,alg:uncond-param-mack}. The problem with this approach, is that it doesn't extend easily to other distributions, because it is not possible, in general, to write these as the sum of a mean and a scaled error term. A better idea is therefore to generate bootstrap responses directly from the fitted distribution. Depending on whether the conditional or the unconditional scheme is used, this will either depend on the original triangle observations or the ones generated at the previous step. The Mack CL is then refitted to the bootstrapped triangle in order to obtain $\widehat{\bm{f}}^{(b)}$ and $\widehat{\bm{\sigma}}^{(b)}$.

\begin{algorithm}[!htb]
  \caption{Conditional parametric bootstrap for Mack CL} 
  \label{alg:cond-param-mack}
  \begin{algorithmic} 
    \Require{Cumulative claims triangle $\mathcal{D}_I$, required number of bootstrap samples $B$}
    \vspace{5pt}
    \State $(\bm{\widehat{f}}, \bm{\widehat{\sigma}}) \gets$ \Call{fit}{$\mathcal{D}_I$} 
    \vspace{5pt}
    \For{$b \gets 1, B$}
      \vspace{5pt}
      \For{$j \gets 1, I - 1$}
        \For{$i \gets 1, I - j$}
          \vspace{5pt}
          \State $C^{(b)}_{i, j + 1} \gets$ \Call{sample}{$ \, \mathcal{N}(\widehat{f}_j C_{ij}, \widehat{\sigma}^2_j C_{ij}) \, $}
          \vspace{5pt}
          \State $\displaystyle F^{(b)}_{i, j + 1} \gets C^{(b)}_{i, j + 1} / C_{ij}$
          \vspace{5pt}
        \EndFor
        \vspace{5pt}
        \State $\widehat{f}^{(b)}_j \gets \sum_{i = 1}^{I - j} C^{(b)}_{i, j + 1} / \sum_{i = 1}^{I - j} C_{ij}$
        \vspace{5pt}
        \If{$j < I - 1$}
          \vspace{5pt}
          \State $\displaystyle \widehat{\sigma}^{(b)}_j \gets \frac{1}{I - j - 1}\sum_{i = 1}^{I - j} C_{ij} \left( F^{(b)}_{i, j + 1} - \widehat{f}^{(b)}_j \right)^2$
          \vspace{5pt}
        \Else
          \vspace{5pt}
          \State $\widehat{\sigma}^{(b)}_{I - 1} \gets \sqrt{\min{ \left \{ \displaystyle \frac{(\widehat{\sigma}^{(b)}_{I - 2})^4}{(\widehat{\sigma}^{(b)}_{I - 3})^2}, (\widehat{\sigma}^{(b)}_{I - 2})^2, (\widehat{\sigma}^{(b)}_{I - 3})^2 \right \} }}$
          \vspace{5pt}
          \EndIf
        \EndFor
    \EndFor
    \State \Return $\{ (\widehat{\bm{f}}^{(b)}, \widehat{\bm{\sigma}}^{(b)}) \mid b = 1, \dots, B \}$
  \end{algorithmic}
\end{algorithm}
%
\begin{algorithm}[!htb]
  \caption{Unconditional parametric bootstrap for Mack CL} 
  \label{alg:uncond-param-mack}
  \begin{algorithmic} 
    \Require{Cumulative claims triangle $\mathcal{D}_I$, required number of bootstrap samples $B$}
    \vspace{5pt}
    \State $(\{ r_{ij} \mid i + j \leq I + 1 \}, \bm{\widehat{f}}, \bm{\widehat{\sigma}}) \gets$ \Call{fit}{$\mathcal{D}_I$} 
    \vspace{5pt}
    \For{$b \gets 1, B$}
      \vspace{5pt}
      \For{$i \gets 1, I$}
        \vspace{5pt}
        \State $C^{(b)}_{i1} \gets C_{i1}$
        \vspace{5pt}
      \EndFor
      \vspace{5pt}
      \For{$j \gets 1, I - 1$}
        \For{$i \gets 1, I - j$}
          \vspace{5pt}
          \State $C^{(b)}_{i, j + 1} \gets$ \Call{sample}{$ \, \mathcal{N}(\widehat{f}_j C^{(b)}_{ij}, \widehat{\sigma}^2_j C^{(b)}_{ij}) \, $}
          \vspace{5pt}
          \State $\displaystyle F^{(b)}_{i, j + 1} \gets C^{(b)}_{i, j + 1} / C^{(b)}_{ij}$
          \vspace{5pt}
        \EndFor
        \vspace{5pt}
        \State $\widehat{f}^{(b)}_j \gets \sum_{i = 1}^{I - j} C^{(b)}_{i, j + 1} / \sum_{i = 1}^{I - j} C^{(b)}_{ij}$
        \vspace{5pt}
        \If{$j < I - 1$}
          \vspace{5pt}
          \State $\displaystyle \widehat{\sigma}^{(b)}_j \gets \frac{1}{I - j - 1}\sum_{i = 1}^{I - j} C^{(b)}_{ij} \left( F^{(b)}_{i, j + 1} - \widehat{f}^{(b)}_j \right)^2$
          \vspace{5pt}
        \Else
          \vspace{5pt}
          \State $\widehat{\sigma}^{(b)}_{I - 1} \gets \sqrt{\min{ \left \{ \displaystyle \frac{(\widehat{\sigma}^{(b)}_{I - 2})^4}{(\widehat{\sigma}^{(b)}_{I - 3})^2}, (\widehat{\sigma}^{(b)}_{I - 2})^2, (\widehat{\sigma}^{(b)}_{I - 3})^2 \right \} }}$
          \vspace{5pt}
          \EndIf
        \EndFor
    \EndFor    
    \State \Return $\{ (\widehat{\bm{f}}^{(b)}, \widehat{\bm{\sigma}}^{(b)}) \mid b = 1, \dots, B \}$
  \end{algorithmic}
\end{algorithm}

\begin{table}[!htb]
  \centering
  \begin{subtable}{0.45\textwidth}
    \begin{tabular}{|m{1em}|m{2em}|m{2em}|m{3.5em}|m{4em}|}\hline%
      $j$ & $\widehat{f}^B_j$ & $\widehat{\sigma}^B_j$ & $\widehat{R}_j^B$ & \resizebox{4em}{!}{$\widehat{\mathrm{MSEP}}(\widehat{R}_j)$} \\ \hline
      \csvreader[
        head to column names,
        late after line = \\\hline
      ]{%
        ../results/example/mack_param_cond.csv
      }{}{%
        \idx & \devfacs & \sigmas & \reserve & \prederror
      }%
    \end{tabular}
    \subcaption{Conditional}
  \end{subtable}
  \begin{subtable}{0.45\textwidth}
    \begin{tabular}{|m{1em}|m{2em}|m{2em}|m{3.5em}|m{4em}|}\hline%
      $j$ & $\widehat{f}^B_j$ & $\widehat{\sigma}^B_j$ & $\widehat{R}_j^B$ & \resizebox{4em}{!}{$\widehat{\mathrm{MSEP}}(\widehat{R}_j)$} \\ \hline
      \csvreader[
        head to column names,
        late after line = \\\hline
      ]{%
        ../results/example/mack_param_uncond.csv
      }{}{%
        \idx & \devfacs & \sigmas & \reserve & \prederror
      }%
    \end{tabular}
    \subcaption{Unconditional}
  \end{subtable}
\caption{Parametric bootstrap results}
\label{tab:param-mack-res}
\end{table}

\section{Incorporating the process error}

We end this chapter by discussing how the process error can be incorporated into these bootstrap procedures. As described in \Cref{sec:boot-proc}, our aim is to obtain a predictive distribution of the reserve which incorporates both parameter and process error. Following the procedure outlined there, we can achieve this by simulating the lower triangle $\mathcal{D}^{\mathsf{c}}_I = \{ C_{ij} \mid i + j > I + 1 \}$, giving us pseudo-realisations $C_{ij}$ of the relevant claim amounts. This, in turn, yields bootstrap replicates
\begin{equation}
    R^{(b)} \coloneqq \sum_{i = 2}^I (C^{(b)}_{iI} - C_{i, I + 1 - i})
\end{equation}
for the reserve. In view of \eqref{eq:time-series-model}, one way of achieving this would be to start from the antidiagonal of $\mathcal{D}_I$ and successively sample
\begin{equation} \label{eq:normal-sampling}
    C^{(b)}_{i, j + 1} \sim \mathcal{N}(\widehat{f}^{(b)}_j \, C^{(b)}_{ij}, \widehat{\sigma}^{(b)}_j \, C_{ij}) \,.
\end{equation}
As with the parameter error in \Cref{sec:mack-boot}, however, we are faced with the problem of possibly drawing negative samples. Again, we could solve this by simply discard a simulated triangle as soon as it contains a negative value, but this can lead to computationally infeasible situations. 

we can follow the suggestion given in \cite[238]{england:dist} and subtitute in place of \eqref{eq:normal-sampling} a gamma distribution with the same mean and variance. If we write $C_{ij} \sim \Gamma(\alpha, \beta)$, this means that $\alpha, \beta$ must satisfy
\begin{equation}
\frac{\alpha}{\beta} = f_{j-1} C_{i, j-1} \quad \text{and} \quad \frac{\alpha}{\beta^2} = \sigma^2_{j-1} C_{i, j-1} \,,
\end{equation}
from which it follows that
\begin{equation}
\alpha = \frac{f_{j-1}^2 C_{i, j-1}}{\sigma_{j-1}^2} \quad \text{and} \quad \beta = \frac{f_{j-1}}{\sigma_{j-1}^2} \,.
\end{equation}

% \begin{figure}
%   \includegraphics{pred_dist_semiparam}
% \end{figure}

\begin{figure}
  \includegraphics{mack_pairs_dist}
  \caption{Comparison of the simulated distribution of the reserve with and without parameter error}
\end{figure}

\begin{figure}
  \includegraphics{mack_dist_param}
  \caption{Comparison of the simulated distribution of the reserve with and without parameter error}
\end{figure}

\chapter{Poisson GLM} \label{chapter:poisson}

The (overdispersed) Poisson model (ODP), proposed by Renshaw and Verrall in \cite{renshaw}, belongs to the family of so-called \emph{generalised linear models} (GLM). In contrast to the Mack CL, it describes the incremental claims $X_{ij}$. As the concept of overdispersion is explained in \Cref{sec:glm}, we only state the assumptions of the ordinary variant at this point.

\begin{model}[Poisson GLM] \leavevmode \label{model:poisson}
  \begin{enumerate}
      \item \label{assump:poisson1}
      The incremental claims are independent from each other.
      \item \label{assump:poisson2}
      There exist parameters $c$, $a_1, \dots, a_I$ and $b_1, \dots, b_I$ such that
      \begin{equation} \label{eq:linear-predictor-poisson}
          \log(\expect{X_{ij}}) = c + a_i + b_j \,,
      \end{equation}
      with $a_1 = b_1 = 0$.
      \item \label{assump:poisson3}
      The incremental claims follow a Poisson distribution with $\mu_{ij} = \expect{X_{ij}}$:
      \begin{equation}
          X_{ij} \sim \mathrm{Pois}(e^{c + a_i + b_j}) \,.
      \end{equation}
  \end{enumerate}
\end{model}

The condition $a_1 = b_1 = 0$ is necessary to obtain an identifiable model. Without it, any set of parameters $c, a_1, \dots, a_I, b_1, \dots, b_I$ satisfying the assumptions would yield an infinite number of alternatives $c + a_0 + b_0, a_1 - a_0, \dots, a_I - a_0, b_1 - b_0, \dots, b_I - b_0$ for $a_0, b_0 \in \mathbb{R}$. We can therefore see that we have two superfluous degrees of freedom, which we can get rid of by imposing two conditions on the parameters. 

By defining $\xi_i \coloneqq e^{c + a_i}$ and $\gamma_j \coloneqq e^{b_j}$, we can obtain a different parametrisation of the model with a multiplicative structure for the mean,
\begin{equation}
  \expect{X_{ij}} = \xi_i \gamma_j \,,
\end{equation}
which is often preferred to the previous one for reasons of interpretability. Indeed, it is clear that the multiplicative form has one fewer degree of freedom than the linear one, and if we remove it by imposing the constraint
\begin{equation}
  \sum_{j = 1}^I \gamma_j = 1
\end{equation}
then we can view the $\xi_i$ as expected ultimate claim amounts, and the $\gamma_j$ as the expected development pattern.

As mentioned in the introduction, stochastic claims reserving models have to reproduce the chain ladder point predictions in order to be acceptable to practitioners. While less obvious than for the Mack CL, it can be shown that the Poisson model also satisfies this requirement (see \cite[Lemma 2.16]{wuthrich:stochastic-reserving}).

\section{Generalised linear models} \label{sec:glm}

GLMs were first conceived by Nelder and Wedderburn in \cite{nelder} as a way of unifying the many disparate generalisations of linear regression with Gaussian errors which were then in existence. These sought to extend the classical model by allowing the use of different functional forms for the conditional mean and different distributions for the response, thus making it suited to modelling counts data (Poisson regression) or the probability of binary events (logistic regression), among others. For a set of covariates $X_1, \dots, X_p$ and a response variable $Y$, a GLM consists of three parts:
\begin{enumerate}
  \item The \emph{random component}, a distribution for response $Y$ belonging to the so-called \emph{exponential dispersion model} family (EDM), which consists of all probability distributions whose density (with respect either to the Lebesgue or counting measure) has the form
  \begin{equation} \label{eq:exp-disp-fam}
    p(y \mid \theta, \phi) = \exp \left \{ \frac{y \theta - b(\theta)}{a(\phi)} + c(y, \phi) \right \} \,,
  \end{equation}
  where $a$, $b$ and $c$ are known functions, and $b$ is at least twice differentiable. We call $\theta$ the \emph{canonical parameter} of the distribution and $\phi$ the \emph{dispersion parameter}.
  \item The \emph{systematic component}, a predictor $\eta \coloneqq \bm{\mathrm{x}}^T \bm{\beta}$ which is a linear function of the covariates.
  \item A monotonic differentiable link function $g: \mathbb{R} \rightarrow \mathbb{R}$ giving the relation between the conditional expectation and the linear predictor,
  \begin{equation}
    \mu \coloneqq \condexpp{Y}{X_1, \dots, X_p} = g^{-1}(\eta) \,.
  \end{equation}
\end{enumerate}
The Gaussian distribution $\mathcal{N}(\mu, \sigma^2)$ can be seen to belong to the EDM family by rewriting its density as
\begin{align}
  \frac{1}{\sqrt{2 \pi} \sigma} \exp \left \{ -\frac{1}{2} \left( \frac{y - \mu}{\sigma} \right)^2\right \} &= \exp \left \{ -\frac{y^2}{2 \sigma^2} + \frac{y \mu}{\sigma^2} - \frac{\mu^2}{2 \sigma^2} - \log(\sqrt{2 \pi} \sigma) \right \} \\
  &= \exp \left \{ \frac{y \mu - \mu^2 / 2}{\sigma^2} - \frac{y^2}{2 \sigma^2} - \log(\sqrt{2 \pi} \sigma) \right \} \,,
\end{align}
which is of the form \cref{eq:exp-disp-fam} with $\theta = \mu$, $b(\theta) = \frac{\theta^2}{2}$, $\phi = \sigma^2$, $a(\phi) = \phi$ and $c(y, \sigma) = -\frac{y^2}{2 \sigma^2} - \log(\sqrt{2 \pi} \sigma)$. Thus, the familiar normal linear model can be obtained from the GLM framework with response distribution $\mathcal{N}(\mu, \sigma^2)$ and identity link $g(\mu) = \mu$.

The EDM family has a number of properties which greatly facilitate the computations involved in estimation. Recall from likelihood theory that $l(\theta \mid y, \phi) \coloneqq \log p(y \mid \theta, \phi)$ satisfies
\begin{equation} \label{eq:exp-var-likelihood}
  \expect{\frac{\partial l(\theta \mid Y)}{\partial \theta}} = 0 \,, \qquad \var{\frac{\partial l(\theta \mid Y)}{\partial \theta}} = -\expect{\frac{\partial^2 l(\theta \mid Y)}{\theta^2}} \,,
\end{equation}
where $\frac{\partial l(\theta \mid Y)}{\partial \theta}$ is known as the \emph{score function}. Using \cref{eq:exp-disp-fam}, we then find that
\begin{equation}
  \expect{\frac{Y - b'(\theta)}{a(\phi)}} = 0 \,, \qquad \var{\frac{Y - b'(\theta)}{a(\phi)}} = -\expect{\frac{-b''(\theta)}{a(\phi)}} \,,
\end{equation}
from which we obtain the elegant relations
\begin{equation}
  \mu = b'(\theta) \,, \qquad \var{Y} = a(\phi) b''(\theta) \,.
\end{equation}
Observe that this implies that $\frac{d \mu}{d \theta} = b''(\theta) > 0$ (because the variance is always positive), which means that $\theta \mapsto \mu(\theta)$ is one-to-one and therefore invertible. In particular, we can always write the likelihood as function of the mean. The function $V(\mu) \coloneqq b''((b')^{-1}(\mu))$ is called the \emph{variance function} and determines how the scale of the response varies as a function of its mean. 

Special care has to be taken with the parameter $\phi$, as it occupies a rather awkward position in GLM theory. The trouble is that we want to incorporate two-parameter distributions, such as the normal and gamma distribution, into the GLM framework which can fundamentally only handle a single parameter gracefully (the more flexible framework of \emph{vector GLMs} is an attempt to remedy this; see \cite[Chapter 2]{yee} for a general discussion). The dispersion is therefore regelated to the role of nuisance parameter and subjected to severe (and often unrealistic) constraints. Basically, we $\phi$ to be the constant as a function of the covariates, but this would preclude certain special cases such as binomial regression with a different number of trials for each observation in the sample. To take this into account, we allow the function $a$ in the denominator of \cref{eq:exp-disp-fam} to vary across different sample responses as $a_i(\phi) = \phi / w_i$, where $w_i$ is a known weight. Not a very elegant solution, perhaps, but one which is foisted upon us by the limitations of the theory. The parameter $\phi$ itself is then considered as known, and estimated outside of the GLM framework, most commonly using the Pearson statistic
\begin{equation}
  \widehat{\phi} \coloneqq \frac{1}{n - p} \sum_{i = 1}^N \frac{(Y_i - \widehat{\mu_i})^2}{V(\widehat{\mu_i})} \,.
\end{equation}

Given a sample $(\bm{\mathrm{x_1}}, Y_1), \dots, (\bm{\mathrm{x_N}}, Y_N)$, the standard way to fit a GLM is by means of maximum likelihood estimation (MLE). The joint log-likelihood of the sample is given by
\begin{equation} \label{eq:joint-log-likelihood}
  l(\bm{\beta} \mid \mathbf{y}, \phi) = \sum_{i = 1}^N \frac{y_i \theta_i - b(\theta_i)}{a_i(\phi)} + c(y_i, \phi) \,,
\end{equation}
which we must differentiate with respect to $\beta_j$ to obtain the likelihood equations. An application of the chain rule gives us 
\begin{align}
  \frac{\partial l(\bm{\beta} \mid \mathbf{y}, \phi)}{\partial \beta_j} &= \sum_{i = 1}^N \frac{\partial l(\bm{\beta} \mid y_i, \phi)}{\partial \theta_i} \frac{\partial \theta_i}{\partial \mu_i} \pdv{\mu_i}{\eta_i} \frac{\partial \eta_i}{\partial \beta_j} \\
  &= \sum_{i = 1}^N \frac{y_i - b'_i(\theta)}{a_i(\phi)} \frac{1}{b_i''(\theta_i)} \pdv{\mu_i}{\eta_i} x_{ij} \\
  &= \sum_{i = 1}^N \frac{y_i - \mu_i}{\var{Y_i}} \pdv{\mu_i}{\eta_i} x_{ij} \,, \label{eq:grad-likelihood}
\end{align}
and setting this equal to $0$ yields a system of $p$ (usually nonlinear) equations. It is generally impossible to solve these analytically, and so we must resort to numerical methods. In particular, we use a modified version of the Newton-Raphson algorithm known as \emph{Fisher scoring}, which replaces the negative Hessian of the log-likelihood, called the \emph{observed information}, by its expectation
\begin{equation}
  \mathcal{I}_{jk} \coloneqq \expect{-\frac{\partial^2 l(\bm{\beta} \mid \mathbf{y}, \phi)}{\partial \beta_j \partial \beta_k}} \,,
\end{equation}
which is known as the \emph{Fisher information matrix}. Thus, starting from an initial guess $\bm{\widehat{\beta}}^{(0)}$ for the parameters, we compute a successive approximations via
\begin{equation} \label{eq:fisher-scoring}
  \bm{\widehat{\beta}}^{(k + 1)} = \bm{\widehat{\beta}}^{(k)} + \mathcal{I}(\bm{\widehat{\beta}}^{(k)})^{-1} \nabla l(\bm{\widehat{\beta}}^{(k)} \mid \mathbf{y}, \phi) \,.
\end{equation}
Similarly to \cref{eq:exp-var-likelihood}, it can be shown that
\begin{equation}
  \expect{\pdv{l(\bm{\beta} \mid \mathbf{y}, \phi)}{\bm{\beta}, \bm{\beta}^T}} = -\var{\nabla l(\bm{\beta} \mid \mathbf{y}, \phi) \nabla l(\bm{\beta} \mid \mathbf{y}, \phi)^T} < 0 \,,
\end{equation}
from which we also see that the log-likelihood is concave, and will therefore have a global maximum. Using the fact that the $Y_i$ are independent, so that $\expect{(Y_i - \mu_i)(Y_l - \mu_l)} = 0$ for $i \neq l$, we then obtain
\begin{align}
  I_{jk} &= \expect{\left( \sum_{i = 1}^N \frac{Y_i - \mu_i}{\var{Y_i}} \pdv{\mu_i}{\eta_i} x_{ij}\right) \left( \sum_{l = 1}^N \frac{Y_l - \mu_l}{\var{Y_l}} \frac{\partial \mu_l}{\partial \eta_l} x_{lk} \right)} \\
  &= \sum_{i = 1}^N \left( \pdv{\mu_i}{\eta_i} \right)^2 \frac{\expect{(Y_i - \mu_i)^2}}{\var{Y_i}^2} x_{ij} x_{ik} \\
  &= \sum_{i = 1}^N \left( \pdv{\mu_i}{\eta_i} \right)^2 \frac{x_{ij} x_{ik}}{\var{Y_i}} \\
  & = \bm{\mathrm{x}_j}^T \mathbf{W} \bm{\mathrm{x}_k} \label{eq:information-comps}
\end{align}
where $\mathbf{W}^{(k)}$ is a diagonal matrix with
\begin{equation} \label{eq:irwls-weight-mat}
  \mathbf{W}^{(k)}_{ii} = \frac{1}{\var{Y_i}} \pdv{\mu_i}{\eta_i}_{\bm{\widehat{\beta}}^{(k)}}^2 \,.
\end{equation}
Hence we have $\mathcal{I}(\bm{\widehat{\beta}}^{(k)}) = \mathbf{X}^T \mathbf{W}^{(k)} \mathbf{X}$ and we see from \cref{eq:grad-likelihood} that
\begin{equation} \label{eq:irwls-grad}
  \nabla l(\bm{\beta} \mid \mathbf{y}, \phi) = \mathbf{X}^T \mathbf{W} \mathbf{\tilde{z}}
\end{equation}
with $\mathbf{\tilde{z}}_i = (y_i - \mu_i) \left( \pdv{\eta_i}{\mu_i} \right)$. Multiplying both sides of \ref{eq:fisher-scoring} by $\mathcal{I}(\bm{\beta}^{(k)})$ and using \cref{eq:irwls-weight-mat,eq:irwls-grad,eq:information-comps}, we finally obtain
\begin{align} \label{eq:irwls-iter}
  \mathbf{X}^T \mathbf{W}^{(k)} \mathbf{X} \bm{\widehat{\beta}}^{(k + 1)} =  \mathbf{X}^T \mathbf{W}^{(k)} \mathbf{z}
\end{align}
with $\mathbf{z} = \mathbf{X} \bm{\widehat{\beta}}^{(k)} + \mathbf{\tilde{z}}$ and all quantities evaluated at the current estimate $\bm{\beta}^{(k)}$ of the parameter vector. In other words, the Fisher scoring is equivalent to a series of weighted least squares problems, where the new parameter estimates are obtained by regressing the vector $\mathbf{z}$ on the original covariates $\bm{\mathrm{x}_1}, \dots, \bm{\mathrm{x}_N}$ using weight matrix $\mathbf{W}$, and $\mathbf{z}$ and $\mathbf{W}$ are determined by the current estimate $\bm{\beta}^{(k)}$---hence why the algorithm is called \emph{iteratively reweighted least squares} (IRWLS).

This procedure can be specialised to the particular case of \Cref{model:poisson} in the following way. First, in order to obtain the matrix-vector form used above, we must flatten the tabular response (using, for example, the colexicographical ordering $(i, j) \mapsto j I + i$, i.e.\ column-major order). If we define the parameter vector
\begin{equation}
  \bm{\beta} \coloneqq \begin{bmatrix} c & a_2 & \cdots & a_I & b_2 & \cdots & b_I \end{bmatrix}^T \,,
\end{equation}
then \cref{eq:linear-predictor-poisson} can be rewritten as
\begin{equation} \label{eq:poisson-covariates}
  \log(\mu_{ij}) = c + a_i + b_j = (\bm{\mathrm{e}_1} + \bm{\mathrm{e}_i} + \bm{\mathrm{e}_{I + j - 1}})^T \bm{\beta}
\end{equation}
where $\bm{\mathrm{e}_k}$ denotes the $k$th standard basis vector in $\mathbb{R}^{(2I - 1)}$. Hence we see that the covariates are binary vectors of length $2I - 1$, with the position of the nonzero entries determined by the indices of the observation in the triangle, forming the rows of a very sparse design matrix. As the Poisson model uses the log link, we have $\mu_{ij} = e^{\eta_{ij}}$ and
\begin{equation}
  \frac{\partial \mu_{ij}}{\partial \eta_{ij}} = e^{\eta_{ij}} \,,
\end{equation}
from which, using \cref{eq:irwls-weight-mat}, we finally obtain
\begin{equation}
  \mathbf{W}_{ii} = \frac{1}{e^{\eta_{ij}}} (e^{\eta_{ij}})^2 = e^{\eta_{ij}} \,,
\end{equation}
giving us all the components of the IRWLS algorithm. \Cref{fig:glm-mat-eq} shows in matrix form.

\begin{figure}
  \centering
  \large
  \begin{displaymath}
    \begin{split}
      &\underbrace{
      \begin{bmatrix}
        1 & 0 & 0 & \dots & 0 \\
        1 & 1 & 0 & \dots & 0 \\
        \vdots & \vdots & \vdots &  & \vdots \\
        1 & 0 & 0 & \dots & 1 \\
      \end{bmatrix}
      }_{\mathbf{X}^T}
      \underbrace{
        \begin{bmatrix}
        e^{\widehat{c}^{(k)}} & & & \\
        & e^{\widehat{c}^{(k)} + \widehat{a}^{(k)}_1} & & & \\
        & & \ddots & \\
        & & & e^{\widehat{c}^{(k)} + \widehat{a}^{(k)}_I + \widehat{b}^{(k)}_I} \\
      \end{bmatrix}
      }_{\mathbf{W}^{(k)}}
      \underbrace{
      \begin{bmatrix}
        1 & 1 & 1 & \dots & 1 \\
        0 & 1 & 0 & \dots & 0 \\
        \vdots & \vdots & \vdots &  & \vdots \\
        0 & 0 & 0 & \dots & 1 \\
      \end{bmatrix}
      }_{\mathbf{X}}
      \underbrace{
      \begin{bmatrix} 
        c \\ a^{(b)}_2 \\ \vdots \\ a^{(b)}_I \\ b^{(b)}_2 \\ \vdots \\ b^{(b)}_I 
      \end{bmatrix}
      }_{\bm{\widehat{\beta}}^{(b)}} \\
      &=%
      \underbrace{
        \begin{bmatrix}
          1 & 0 & 0 & \dots & 0 \\
          1 & 1 & 0 & \dots & 0 \\
          \vdots & \vdots & \vdots &  & \vdots \\
          1 & 0 & 0 & \dots & 1 \\
        \end{bmatrix}
        }_{\mathbf{X}^T}
        \underbrace{
          \begin{bmatrix}
          e^{c} & & & \\
          & e^{c + a_1} & & & \\
          & & \ddots & \\
          & & & e^{c + a_I + b_I} \\
        \end{bmatrix}
        }_{\mathbf{W}^{(k)}}
        \mathbf{z}
    \end{split}
  \end{displaymath}
  \caption{IRWLS equation for Poisson GLM in matrix form}
  \label{fig:glm-mat-eq}
\end{figure}

We have assumed up to this point that a GLM requires us to specify an exact distribution for the response variable. In many practical situations, however, this is either infeasible or leads to unrealistic models. An example which is particularly common with count data is a phenomenon known as \emph{overdispersion}, where the variability of the data is greater than would be suggested by e.g.\ the Poisson or binomial distribution. Recall that the variance of a $\mathrm{Pois}(\lambda)$ distribution is $\lambda$, and that of a $B(n, p)$ distribution is $np(1 - p)$; in both cases, it is fully determined by the mean, and we have no degree of freedom with which to adjust it in order to obtain a better fit to the data, as would be the case with the normal distribution, for example.

To remedy this, an extension can be made to the GLM framework, which only relies on the specification of a relation between mean and variance. Recall from above that the MLE works by setting the score equal to $0$. If we write the likelihood in terms of $\bm{\mu}$, this will have components
\begin{equation} \label{eq:lhood-mean-param}
  \frac{\partial l(\bm{\mu} \mid \mathbf{y}, \phi)}{\partial \mu_j} = \frac{y_j - \mu_j}{\phi V(\mu_j)} \,,
\end{equation}
and is therefore completely determined by $V(\cdot)$. Suppose now, conversely, that we start from $V(\cdot)$. We could then define functions
\begin{equation}
  Q_i(\mu \mid y_i, \phi) \coloneqq \int_{y_i}^{\mu} \frac{y_i - u}{\phi V(u)} \, du \,,
\end{equation}
and estimate $\bm{\mu}$ (and therefore $\bm{\beta}$) by minimising
\begin{equation}
  Q(\bm{\mu} \mid \mathbf{y}, \phi) \coloneqq \sum_{i = 1}^N Q_i(\mu_i \mid y_i, \phi) \,.
\end{equation}
It must be stressed that $Q$ has no probabilistic significance: it does not, in general, correspond to the log-likelihood of any distribution. Rather, it functions a device to obtain estimates of the desired parameters, fulfilling in this a similar role to that of the log-likelihood, which is why we refer to it as a \emph{quasi-likelihood function}\sepfootnote{fn:quasi-likelihood}. It is usual to identify quasi-likelihood models derived from specific distributions (i.e.\ using the corresponding variance function) by prefixing 'quasi' to the name of said distribution, e.g.\ quasi-Poisson or quasi-binomial. The derived quasi-model yields the same parameter estimates as the classical GLM if the data follow the original distribution.

We are now finally in a position to describe the overdispersed variant of the Poisson GLM.

\begin{model}[Overdispersed quasi-Poisson] \leavevmode \label{model:odp}
  \begin{enumerate}
      \item The incremental claims are independent from each other.
      \item There exist parameters $c$, $a_1, \dots, a_I$ and $b_1, \dots, b_I$ such that
      \begin{equation}
          \log(\mu_{ij}) = c + a_i + b_j \,,
      \end{equation}
      with $\mu_{ij} \coloneqq \expect{X_{ij}}$ and $a_1 = b_1 = 0$.
      \item There exists a parameter $\phi$ such that
      \begin{equation}
          \var{X_{ij}} = \phi \mu_{ij} \,.
      \end{equation}
  \end{enumerate}
\end{model}

When the data consists entirely of positive integers (e.g. a triangle of claims counts), it follows from the previous remark that this model yields the same predictions as the chain ladder. More generally, the CL results will be reproduced as long as the additional condition
\begin{equation}
  \sum_{i = 1}^I X_{ij} \geq 0
\end{equation}
is satisfied for $j \in \{ 1, \dots, I \}$ (see \cite[Section 2]{renshaw}). The quasi-Poisson is therefore robust to the presence of a limited number of negative claim amounts, which is sometimes observed in practice. Moreover, it lifts the unrealistic restriction that the response values must be integers, and gives us a way of accounting for overdispersion, which is a feature of many claims triangles. The absence of a likelihood also poses some difficulties, however, notably in the area of inference and diagnostics.

\section{Bootstrap methodology}

Developing a bootstrap procedure for the Poisson and quasi-Poisson models is in some respects easier than for the Mack CL. The absence of a recursive structure makes it more straightforward to reason about resampling. Furthermore, bootstrap methods for claims triangle GLMs have seen more discussion in the literature (see e.g.\ \cite{pinheiro} and \cite{england:dist}), and so we can draw upon this material for our exposition. 

As with Mack's model, we shall take \Cref{sec:boot-reg} as our starting point. We distinguished there between nonparametric, semiparametric and parametric approaches to bootstrapping. Of these, the nonparametric bootstrap, which involves resampling predictor-response pairs from the original data, cannot be applied to the Poisson model. To understand why, recall from \Cref{eq:irwls-iter} that the IRWLS algorithm fits a linear model at each step of the iteration in order to obtain a new estimate of the parameters, which requires the matrix on the left-hand side to be invertible. Hence, it follows that the design matrix must have full rank. We also saw in \Cref{eq:poisson-covariates} that the rows of $\mathbf{X}$ are binary vectors indicating the origin and development year (row and column in the claims triangle) to which an observation belongs. Because of the structure of the triangle, every observation corresponds

This leaves us with only the semiparameteric and parametric variants to consider.

% \begin{table}
%   \centering
%   \begin{tabular}{|m{5em}|m{5em}|m{5em}|m{5em}|m{5em}|}\hline%
%     $j$ & $\widehat{f}^B_j$ & $\widehat{\sigma}^B_j$ & $\widehat{R}^B$ & $\widehat{\mathrm{MSEP}}(\widehat{R})$ \\ \hline
%     \csvreader[
%       head to column names,
%       late after line = \\\hline
%     ]{%
%       ../results/example/mack_bench.csv
%     }{}{%
%       \idx & \a & \b & \reserve & \reserveMSE
%     }%
%   \end{tabular}
%   \begin{tabular}{m{5em} m{5em}}\hline%
%     $\widehat{c}^B$ \\ $\widehat{\phi}^B$ \\ \hline
%     \csvreader[
%       head to column names,
%       late after line = \\\hline
%     ]{%
%       ../results/example/mack_bench.csv
%     }{}{%
%       \intercept \\ \disp
%     }%
%   \end{tabular}
%   \caption{Poisson GLM results for UK Motor triangle}
%   \label{tab:pois-bench}
% \end{table}

For the semiparameteric bootstrap, the essential step is to find a satisfactory definition for the residuals such that they are i.i.d. Things are more complicated here than for Mack's model, as there generally exists no natural seperation of the response into mean and additive error for a non-Gaussian response (this problem was recognised early on in the literature on GLM bootstrapping, see \cite{moulton}). Consequently, a multitude of different residual types are available. We will consider three of these in particular.

The \emph{Pearson residuals}
\begin{equation} \label{eq:pearson-resid}
  r_{ij} \coloneqq \frac{X_{ij} - \widehat{\mu}_{ij}}{\sqrt{V(\widehat{\mu}_{ij})}} \,,
\end{equation}
attempt to deal with the inherent heteroscedasticity of the GLM response by dividing out the component of the variance which is specific to each observation. In this, they resemble the standardised residuals in the context of weighted linear regression. Extending this analogy further, we can adjust \cref{eq:pearson-resid} for the leverage of the observation, i.e.
\begin{equation}
  \tilde{r}_{ij} \coloneqq \frac{X_{ij} - \widehat{\mu}_{ij}}{\sqrt{V(\widehat{\mu}_{ij}) (1 - h_{ij})}} \,,
\end{equation}
where $h_{ij}$ is the appropriate diagonal element in the hat matrix
\begin{equation}
  \mathbf{H} = \mathbf{X} (\mathbf{XWX})^{-1} \mathbf{X}^T \mathbf{W} \mathbf{z}
\end{equation}
corresponding to the final iteration of the IRWLS algorithm.

Another kind of residuals are based on a goodness-of-fit measure for GLMs known as the \emph{deviance}. It can be derived from \cref{eq:lhood-mean-param} by noticing that the mean parametrisation of the log-likelihood is maximised at $\bm{\mu} = \mathbf{y}$, so that the quantity
\begin{equation}
  D(\mathbf{y}, \bm{\mu}) \coloneqq \sum_{i = 1}^N d(y_i, \mu_i) \coloneqq 2 \sum_{i = 1}^N (l(y_i \mid y_i) - l(\widehat{\mu}_i \mid y_i)) 
\end{equation}
expresses the departure of our model from a perfect fit. The functions $D(\mathbf{y}, \bm{\mu})$ and $d(y_i, \mu_i)$ are called the \emph{total} and \emph{unit deviance}, respectively. The \emph{deviance residuals} are then defined as
\begin{equation}
  r_{ij} \coloneqq \mathrm{sign}(x_{ij} - \mu_{ij}) \sqrt{d(x_{ij}, \mu_{ij})} \,.
\end{equation}

Finally, we consider a third type known as \emph{quantile residuals} (see \cite{dunn} for a general discussion), which are most easily explained for continuous response distributions. In that case, an elementary fact from probability theory states that
\begin{equation}
  F(Y \mid \mu, \phi) \sim U(0, 1) \,,
\end{equation}
and it should therefore follow that the empirical distribution of the transformed sample
\begin{equation}
  r_i \coloneqq F(Y_i \mid \widehat{\mu_i}, \widehat{\phi})
\end{equation}
is approximately uniform, provided the sampling variability of $\bm{\widehat{\mu}}$ and $\widehat{\phi}$ is not too severe. If $F(\cdot \mid \bm{\mu}, \phi)$ is discrete, the definition is amended as follows: for every observation $y_i$, set
\begin{equation}
  a_i \coloneqq \lim_{y \uparrow y_i} F(y \mid \widehat{\mu_i}, \widehat{\phi}), \qquad b_i \coloneqq F(y_i \mid \widehat{\mu_i}, \widehat{\phi}) \,,
\end{equation}
and define the $r_i$ as mutually independent random variables which are uniformly distributed on $(a_i, b_i]$.

\chapter{Numerical implementation}

\chapter{Results}

% \footnote{Specifically, we require that the derivative with respect to the parameter exists and is dominated by an integrable function. For a proof that the EDM family satisfies this condition, see \cite[Theorem 2.7.1]{lehmann}.}

\backmatter%
\chapter{Conclusion}

\printbibliography%

\end{document}
