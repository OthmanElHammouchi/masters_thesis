%!LW recipe=pdflatex -> biber -> makeindex -> pdflatex * 2
\documentclass[a4paper]{book}

\input{preamble.tex}

\title{Sensitivity analysis of stochastic reserving models using bootstrap simulations}
\pretitle{\flushleft{Graduation thesis submitted in partial fulfilment of the requirements for the degree of Master of Science in Mathematics}}
\author{Othman El Hammouchi}
\date{June~2023}
\promotors{Promotors: dr.\ Robin Van Oirbeek \and prof.\ dr.\ Tim Verdonck \and prof.\ dr.\ Mark Sioen}
\faculty{Sciences and Bioengineering Sciences}%

\begin{document}
\frontmatter

\thispagestyle{empty}
\maketitle%

\renewcommand\title{Gevoeligheidsanalyse van stochastische schadereserveringsmodellen aan de hand van bootstrap-simulaties}
\renewcommand\pretitle{\flushleft{Proefschrift ingediend met het oog op het behalen van de graad van Master of Science in de Wiskunde}}%
\renewcommand\date{Juni~2023}%
\renewcommand\promotors{Promotors: dr.\ Robin Van Oirbeek \and prof.\ dr.\ Tim Verdonck \and prof.\ dr.\ Mark Sioen}
\renewcommand\faculty{Wetenschappen en Bio-ingenieurswetenschappen}%

\thispagestyle{empty}
\maketitle%

\input{footnotes.tex}%

\chapter{Acknowledgements}

It has become almost obligatory to quote the immortal words of John Donne at the start of any paragraph of acknowledgements: "No man is an island." A shibboleth though it may be, yet it does have the considerable virtue of being true, which is why I have allowed myself to indulge in it here. The list of those to whom I have become indebted in the course of writing this thesis could fill a seperate bibliography, so that I can perforce only mention its most significant entries. Chief among these are my parents and sister, who have been my support and reliance throughout my life; words cannot express the gratitude I feel towards them. I would also like to extend a warm thanks to my active supervisor Dr.\ Robin Van Oirbeek, whose direction, encouragement and generosity have been absolutely wonderful; it was truly a joy to work with him.

\chapter{\abstractname}
The problem of estimating the reserve needed to cover outstanding claims liabilities, and of quantifying the uncertainty of such estimates, has been the subject of much research within actuarial science. The insurance sector distinguishes itself from other model-heavy fields in the high degree of deference it accords to expert judgement. Reserving actuaries are reluctant to move away from established techniques, which are based on macro-level models that aggregate claims data over certain periods. The resulting datasets contain relatively few observations, limiting the efficacy of classical diagnostic methods, and making it difficult to verify model assumptions. We investigate the use of the statistical bootstrap to remedy this, studying how the simulatied predictive distribution of the reserve is impacted by the presence of observations which deviate from the assumptions of two major stochastic reserving models: the Mack chain ladder and the Overdispersed Poisson GLM. Our findings indicate that certain types of bootstraps are very promising for the purpose of distinguishing contaminated points from normal observations.

\begin{otherlanguage}{dutch}
  \chapter{\abstractname}
  Het schatten van de reserve die nodig is om openstaande schadeverplichtingen te dekken, evenals het kwantificeren van de onzekerheid van dergelijke schattingen, is het voorwerp van veel onderzoek binnen de actuari\"ele wetenschap. De verzekeringssector onderscheidt zich van andere domeinen waarin wiskundige modellen een belangrijke rol spelen door de hoge achting die het toekent aan het oordeel van deskundigen. Actuarissen die verantwoordelijk zijn voor het schadereserveringsproces willen typisch niet afwijken van gevestigde technieken, en deze zijn gebaseerd op macro-modellen die schadegegevens aggregeren over bepaalde periodes. De datasets die men hieruit verkrijgt bevatten relatief weinig observaties, hetgeen de doeltreffendheid van klassieke diagnostische methodes beperkt en de verificatie van modelaannames beperkt. We onderzoeken het gebruik van de statistische bootstrap om deze moeilijkheid te remedi\"eren. Dit doen we door te bestuderen hoe de gesimuleerde voorspellende verdeling van de reserve be\"invloed wordt door de aanwezigheid van observaties die afwijken van de aannames van twee belangrijke stochastische schadereserveringsmodellen: de Mack chain ladder en de Overdispersed Poisson GLM. Onze bevindingen geven aan dat bepaalde types bootstrap beloftevol zijn voor het onderscheiden van gecontamineerde datapunten van gewone waarnemingen.
\end{otherlanguage}

\tableofcontents%
\listoftables%
\listoffigures%
\input{nomenclature.tex}
\printnomenclature%

\mainmatter%

\chapter{Introduction} \label{chapter:intro}

The most defining characteristic of the insurance industry is the inverted nature of its production cycle. In manufacturing, commerce, transport, etc., payment is usually received only upon delivery of goods or services. By contrast, insurance products are purchased long before the adverse events which they protect against have occured, if they ever do. Insurers therefore face the challenge of forecasting the amount and variability of funds needed to settle outstanding contracts, a process known as \emph{claims reserving}\index{reserving}. The reserving actuary must rely here on historical data, which is most often presented in the form of a \emph{loss} or \emph{run-off triangle}\index{triangle} $\mathcal{D}_I$ consisting of either cumulative\index{claims triangle!cumulative} or incremental\index{claims triangle!incremental} amounts of some actuarial variable (payments, number of claims, etc.), respectively denoted by $C_{ij}$ and $X_{ij}$. The row index $1 \leq i \leq I$ denotes the \emph{cohort}\index{cohort}, \emph{origin year}\index{origin year} or \emph {accident year}\index{accident year}, and the column index $1 \leq j \leq J$ gives the \emph{development year}\index{development year}, so that
\begin{equation}
  \mathcal{D}_I = \left  \{ C_{ij} \mid 1 \leq j \leq J, i + j \leq I + 1 \right \}
  %
  \quad \text{or} \quad
  %
  \mathcal{D}_I = \left  \{ X_{ij} \mid 1 \leq j \leq J, i + j \leq I + 1 \right \} \,.
\end{equation}
To simplify the formulas, we assume throughout this exposition that $I = J$. Embedding $\mathcal{D}_I$ into a matrix on and above the anti-diagonal, the actuary then seeks to predict the \emph{total outstanding loss liabilities}\index{reserve!total outstanding loss liabilities}
\begin{equation}
  R = \sum_{i = 2}^I (C_{i, I} - C_{i, I + 1- i})
\end{equation}
by forecasting the values in the lower triangle $\mathcal{D}^{\mathsf{c}}_I$. A special difficulty arising in the actuarial context is the relatively small number of observations which is typically available.

\begin{table}
  \centering
  \begin{subtable}{0.45\textwidth}
    \centering
    \large
    \begin{tabular}{c c c c c}
      $C_{11}$ & $C_{12}$ & $C_{13}$ & $C_{14}$ & $C_{15}$ \\
      $C_{21}$ & $C_{22}$ & $C_{23}$ & $C_{24}$ &          \\
      $C_{31}$ & $C_{32}$ & $C_{33}$ &          &          \\
      $C_{41}$ & $C_{42}$ &          &          &          \\
      $C_{51}$ &          &          &          &
    \end{tabular}
    \subcaption{Cumulative}
  \end{subtable}
  \hfill
  \begin{subtable}{0.45\textwidth}
    \centering
    \large
    \begin{tabular}{c c c c c}
      $X_{11}$ & $X_{12}$ & $X_{13}$ & $X_{14}$ & $X_{15}$ \\
      $X_{21}$ & $X_{22}$ & $X_{23}$ & $X_{24}$ &          \\
      $X_{31}$ & $X_{32}$ & $X_{33}$ &          &          \\
      $X_{41}$ & $X_{42}$ &          &          &          \\
      $X_{51}$ &          &          &          &
    \end{tabular}
    \subcaption{Incremental}
  \end{subtable}
  \caption{General notation for a 5 by 5 claims triangle}
\end{table}

One of the most frequently used loss reserving techniques in practice is the so-called \emph{chain ladder} (CL)\index{chain ladder}, which predicts the cumulative claim in development year $j$ by multiplying the previous year's amount by a so-called \emph{age-to-age factor}, \emph{link ratio} or \emph{development factor}\index{development factor}. It was originally conceived as a purely computational algorithm, but has since been framed as a stochastic model in a variety of ways. Its central assumption is that the pattern observed in earlier cohorts is applicable to later ones. In one sense, this is of course perfectly reasonable: all models ultimately use the past as a guide to the future. The dearth of data typically available to the actuary makes it challenging to verify its validity, however, as it limits the efficacy of classical statistical techniques.

To illustrate this point, consider \cref{tab:uk-motor}, which contains the dataset of cumulative payments for a motor insurance account from the UK given in \cite{christofides} (this will serve as the running example throughout this text). It consists of a 7 by 7 claims triangle with a total of 28 observations. \cref{fig:diag-plot-original} shows the diagnostic plot of standardised residuals against fitted value for \modelref{model:mack}, which will be discussed in \cref{chapter:mack}. The details are not important at this point; all that matters at present is that it should be symmetric around the x-axis and exhibit no structural patterns if the model gives a good fit.

\begin{table}[!htb]
  \centering
  \begin{tabularx}{0.7\linewidth}{XXXXXXXX} \toprule
    \bf{Origin} & \multicolumn{7}{c}{\bf{Dev}}                                                \\ \cmidrule{2-8}
                & 1                            & 2    & 3     & 4     & 5     & 6     & 7     \\ \midrule
    2007        & 3511                         & 6726 & 8992  & 10704 & 11763 & 12350 & 12690 \\
    2008        & 4001                         & 7703 & 9981  & 11161 & 12117 & 12746 &       \\
    2009        & 4355                         & 8287 & 10233 & 11755 & 12993 &       &       \\
    2010        & 4295                         & 7750 & 9773  & 11093 &       &       &       \\
    2011        & 4150                         & 7897 & 10217 &       &       &       &       \\
    2012        & 5102                         & 9650 &       &       &       &       &       \\
    2013        & 6283                         &      &       &       &       &       &       \\
    \bottomrule
  \end{tabularx}
  \caption{UK Motor cumulative claims triangle from \textcite{christofides}}
  \label{tab:uk-motor}
\end{table}

\begin{figure}[!htb]
  \centering
  \includegraphics[scale=0.9]{original_resids}
  \caption{Diagnostic plot for original triangle}
  \label{fig:diag-plot-original}
\end{figure}

The same diagnostic plots are shown in \cref{fig:diag-plot-perturbed} for the case where the points $(2, 5)$ and $(4, 4)$ have been perturbed by a factor $1.5$, either by direct multiplication or through simulation from the underlying model. The residual corresponding to the pathological observation has been highlighted in red. As these examples demonstrate, it is not always feasible to identify deviations from the model assumptions by examining such plots, even for the trained eye.

\begin{figure}[!htb]
  \centering
  \begin{subfigure}{\textwidth}
    \includegraphics{perturbed_resids}
    \caption{Perturbed directly}
  \end{subfigure}
  \begin{subfigure}{\textwidth}
    \includegraphics{model_perturbed_resids}
    \caption{Perturbed according to model}
  \end{subfigure}
  \caption{Diagnostic plots for perturbed triangles}
  \label{fig:diag-plot-perturbed}
\end{figure}

Our aim in the present study is to investigate whether it is possible to use bootstrap simulations to remedy this problem. The next chapter introduces the bootstrap and explains how it can be used for inference on regression models, which is how we will frame the reserving methods under consideration. In the following two chapters, we will then apply this theory to the most widespread stochastic claims reserving models, \modelref{model:mack} and \modelref{model:odp}, in order to study whether it is possible to identify pattern breaks using bootstrap simulation. Specifically, we simulate claims triangles which perfectly follow the model assumptions, perturb these, and generate a bootstrap reserve from the resulting dataset. This will allow us to investigate how the simulated reserve is impacted by deviations from the model assumptions.

\chapter{The bootstrap method} \label{chapter:boot}

When using a statistical model to describe some dataset in terms of a reduced number of parameters, we are not only interested in obtaining point estimates of these, but also in quantifying their \emph{uncertainty}. In classical statistics, this is usually achieved by starting from the model assumptions and deriving from them the sampling distribution of the estimators analytically. With the exception of the normal distribution, however, the resulting calculations invariably turn out to be intractible, so that one is forced to rely on approximations or asymptotic results. Such estimates tend to be highly sensitive to the simplifying assumptions made in order to obtain them, leading to gross errors when these are violated.

The bootstrap\index{bootstrap} method aims to remedy this problem through the use of numerical simulation. At its core, it is premised on the idea that the empirical distribution of the sample forms a good proxy for that of the population. Consequently, we can approximate sampling from the population by \emph{resampling our data}\index{resampling}; to the casual observer, this can give the impression that we're creating new data \emph{ex nihilo}, using our single sample to `pull ourselves up by our own bootstraps', like Baron Munchausen in the fairy tale.

\section{Bootstrapping an estimator}

To ease the exposition, we begin by describing the bootstrap in the simple case of a classical one-sample estimation problem. Let $X_1, \dots, X_n$ be an i.i.d. sample drawn from a distribution $F$, and consider an estimator $\widehat{h(F)} = g(X_1, \dots, X_n)$ of some quantity $h(F)$ whose uncertainty we wish to estimate using e.g.\ the variance of the sampling distribution. Depending on which assumptions we are willing to make, we can broadly distinguish between \emph{parameteric}\index{bootstrap!parametric} and \emph{nonparametric}\index{bootstrap!nonparametric} approaches to bootstrapping.

The nonparametric bootstrap makes direct use of the sample data, drawing with replacement to produce replicates $X^{(b)}_1, \dots, X^{(b)}_n$. In other words, the population distribution $F$ is approximated by the \emph{empirical cumulative distribution function}\index{cumulative distribution function!empirical}
\begin{equation}
  \widehat{F}_n(x) \coloneqq \sum_{k=1}^n I_{\{ X_k \leq x \}} \,,
\end{equation}
which is used to generate pseudo-observations. We can then compute the statistic of interest on these, yielding pseudo-realisations $g^{(b)} = g(X^{(b)}_1, \dots, X^{(b)}_n)$ which approximate the sampling distribution of $\widehat{h(F)}$, and can be used for inference about the uncertainty of the estimate. For instance, if $B$ denotes the total number of bootstrap samples, we can esimate the variance of $\widehat{h(F)}$ by
\begin{equation}
  \frac{1}{B-1}\sum_{b=1}^B(g^{(b)} - \bar{g})^2 \,,
\end{equation}
with $\bar{g} = \frac{1}{B} \sum_{b=1}^B g^{(b)}$. The accuracy of such estimates will ultimately depend on the accuracy of the approximation $F \approx \widehat{F}_n$.

By contrast, the parametric bootstrap starts by fitting a distributional model to the sample data, and then simulates replicates from this with the help of a random number generator. If we assume that $F$ belongs to some family $\left \{ F_{\bm{\theta}} \mid  \bm{\theta} \in \Theta \right \}$, then we can use the sample $X_1, \dots, X_n$ to produce an estimate $\bm{\widehat{\theta}}$ of the parameter. Plugging this in then gives us $F_{\bm{\widehat{\theta}}}$, from which we can simulate $X^{(b)}_1, \dots , X^{(b)}_n$ and $g^{(b)}$ as before. An estimate of the sampling variance can be obtained in the same manner. When the assumptions of its underlying model are met, this leads to estimates which are more efficient; the downside, of course, is an increased error when they are not.

Although we have so far only used it for calculating a single statistic, it is clear that the bootstrap produces a complete \emph{simulated distribution}\index{simulated distribution} of the estimator, which can be used for many different forms of inference. This highlights its tremendous potential as a tool for statistical analysis, which explains the rise in popularity of bootstrap methods once personal computers powerful enough to carry out the requisite calculations became widely available. We illustrate this with an example. \Cref{fig:boot-est} compares the analytic and bootstrap distributions of the maximum likelihood estimators for simulated sample of size $n = 1000$ drawn from a $\Gamma(\alpha, \beta)$-distribution with $\alpha = 2$ and $\beta = 0.5$. The analytic distributions are based on the well-known fact from likelihood theory that the asymptotic distribution of the MLE is given by the multivariate normal distribution $\mathcal{N}(\bm{\theta}, I(\bm{\theta})^{-1})$, where $\bm{\theta}$ is the parameter vector and $I(\bm{\theta})$ the Fisher information matrix\index{Fisher information}. For the gamma distribution, the latter is given by
\begin{equation}
  I(\alpha, \beta) = n
  \begin{pmatrix}
    \psi'(\alpha) & -1 / \beta       \\
    -1 / \beta    & \alpha / \beta^2
  \end{pmatrix} \,
\end{equation}
where $\psi(x) \coloneqq \odv{}{x} \log \Gamma(x)$ is the so-called digamma function. We can clearly see that the shape of the bootstrap distribution approaches the analytic one, although it still exhibits an unmistakable bias even at this sample size.

\begin{landscape}
  \begin{figure}
    \begin{subfigure}{\linewidth}
      \includegraphics{boot_est_param}
      \subcaption{Parametric}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
      \includegraphics{boot_est_nonparam}
      \subcaption{Nonparametric}
    \end{subfigure}
    \caption{Analytic and bootstrap distribution of MLE for a gamma distribution with $\alpha = 2$ and $\beta = 0.5$}
    \label{fig:boot-est}
  \end{figure}
\end{landscape}

\section{Bootstrapping a regression model} \label{sec:boot-reg}

The principles of the previous section can be applied to bootstrap any kind of data structure, as long as we have a model for the underlying data-generating process\index{data-generating process} (see \cite[Chapter 8]{efron:intro}). In particular, it is perfectly possible to use the boostrap with regressions; indeed, such methods are well-established in the literature, and will provide the foundation for bootstrapping claims triangles in \cref{chapter:mack,chapter:poisson}.

Consider a set of covariates $X_1, \dots, X_p$ and a response variable $Y$ whose relationship is modelled by a parametrised mapping $f(X_1, \dots, X_p; \bm{\beta})$. Given a sample of pairs $(\bm{\mathrm{x}_1}, Y_1), \dots, (\bm{\mathrm{x}_n}, Y_N)$ and a choice of loss function, we can fit this model to the data, obtaining an estimate $\widehat{\bm{\beta}}$. For a new value $\mathbf{x_+}$ of the regressors, we then predict the response $Y_+$ as $f(\mathbf{x_+}; \widehat{\bm{\beta}})$. It's worth emphasising that there are two different processes at play here, which will correspond to different kinds of bootstrap (cfr.\ \cite[Sections 6.3.3 and 7.2.4]{davison}). \emph{Estimation}\index{estimation} seeks to \emph{identify} the value of a quantity which is \emph{fixed but unknown}; \emph{prediction}\index{prediction} aims to \emph{forecast} the value of a \emph{random variable}.

Under the least squares criterion, for example, we know that the optimal predictor for $Y$ is the conditional expectation $\condexp{Y}{X = x}$. This is an ordinary function which returns a real number for any $x \in \mathbb{R}^p$ and can therefore be estimated from a sample. Such an estimate will contain some error, and we need to take this into account when doing inference. If we additionally want to account for the error made when predicting $Y$ using $\condexp{Y}{X = x}$, we also have to incorporate the intrinsic stochasticity or \emph{process variance}\index{process variance} of the response variable. Prediction is therefore a two-stage procedure involving an intermediate estimation step.

We will illustrate this using the all-familiar linear regression model given by
\begin{equation} \label{eq:linear-model}
  Y_i = \bm{\mathrm{x}_i}^T \bm{\beta} + \varepsilon_i \,, \qquad i \in \{ 1, \dots, n \} \,,
\end{equation}
with $\expect{\varepsilon_i} = 0$, $\operatorname{Var}(\varepsilon_i) = \sigma$ and $\expect{\varepsilon_i \varepsilon_j} = 0$ for $i \neq j$. Considering the nonparametric bootstrap first, we need to identify a fundamental unit of resampling such that the corresponding variables are interchangeable. One option would be to use the residuals obtained by fitting the model (cfr.\ \cite[Algorithm 6.1]{davison}). This approach is sometimes referred to as \emph{semiparametric}\index{bootstrap!semiparametric}, because it only uses the specification of certain aspects of the data distribution in terms of some parameters, but does not assume a specific form for it. Choosing for example
\begin{equation}
  r_i \coloneqq Y_i - \bm{\mathrm{x}}^T_i \widehat{\bm{\beta}} \,,
\end{equation}
we resample these to obtain bootstrap replicates $r^{(b)}_1, \dots, r^{(b)}_n$, which in turn yield pseudo-responses
\begin{equation}
  Y_i^{(b)} \coloneqq \bm{\mathrm{x}}^T_i \widehat{\bm{\beta}} + r^{(b)}_i \,.
\end{equation}
Refitting the model to the simulated data then gives us bootstrap replicates of the $\widehat{\bm{\beta}}^{(b)}$ of the regression parameter estimator.

An alternative approach, which is fully nonparametric\index{bootstrap!nonparametric}, is to resample the pairs $(\bm{\mathrm{x}_i}, Y_i)$ themselves (cfr.\ \cites[Section 9.5]{efron:intro}[Algorithm 6.2]{davison}), thereby approximating the multivariate distribution of $(X_1, \dots, X_n, Y)$ with the empirical distribution of the data. This has the significant benefit of \emph{parsimony}\index{parsimony}, making no other assumption beside the i.i.d.-ness of the sample. The model is then refitted to the bootstrap samples $(\bm{\mathrm{x}_1}^{(b)}, Y^{(b)}_1), \dots, (\bm{\mathrm{x}_n}^{(b)}, Y^{(b)}_n)$ to produce pseudo-realisations of the regression parameter estimator.

For the parametric case, we have to make an additional assumption regarding the distribution of $\epsilon$ in \cref{eq:linear-model}, the classic choice being the Gaussian. We then fit the model and use a random number generator to produce bootstrap responses $Y^{(b)}_i$ by drawing from the estimated distribution $\mathcal{N}(\bm{\mathrm{x}_i}^T \widehat{\bm{\beta}}, \widehat{\sigma}^2)$. Performing the regression a second time on this pseudo-data then gives us pseudo-replicates of the regression parameter estimator.

\section{Process variance and predictive distributions} \label{sec:boot-proc}

If we want to use the bootstrap for predictive inference on regression models, we must additionally take into account the inherent variability of the dependent variable. Returning once again to the example of normal linear regression, suppose we are interested in predicting the response $Y_+$ at new value $\mathbf{x_+}$ of the regressors. One way to quantify the accuracy of our forecast is to consider the \emph{prediction error}\index{prediction error}
\begin{equation}
  \delta \coloneqq Y_+ - \widehat{Y}_+ \,.
\end{equation}
In this expression, the second term can be easily bootstrapped using the methods described in the previous section, which yield replicates $\widehat{\bm{\beta}}^{(b)}$ for the parameter estimator and hence the predictor via $\widehat{Y}^{(b)}_+ = \mathbf{x_+}^T \widehat{\bm{\beta}}^{(b)}$. It then remains for us to produce bootstrap realisations of the response itself. In the semiparametric approach\index{bootstrap!semiparametric}, this can be achieved by resampling the residuals a second time to obtain pseudo-realisations $r^{(s)}$ for $s = 1, \dots, S$ and adding these (after correct scaling) to the height of the regression hyperplane at $\mathbf{x_+}$. The resulting pseudo-responses $Y^{(s)}_+$ mimick the random fluctuations of $Y_+$, allowing us to approximate $\delta$ with the bootstrap replicates
\begin{equation} \label{eq:pred-error-boot}
  \widehat{\delta}^{(b, s)} \coloneqq Y^{(s)}_+ - \widehat{Y}^{(b)}_+ = (\mathbf{x}^T_+ \widehat{\bm{\beta}} + r^{(s)}) - \mathbf{x}^T_+ \widehat{\bm{\beta}}^{(b)}
\end{equation}
(see \cite[Algorithm 6.4]{davison}). A parametric\index{bootstrap!parametric} variant can be obtained by generating pseudo-responses from the fitted model, i.e.\
\begin{equation}
  Y^{(s)}_+ \sim \mathcal{N}(\mathbf{x}^T_+ \widehat{\bm{\beta}}, \widehat{\sigma}^2) \,.
\end{equation}
It is difficult to see how this method could be adapted directly to the pairs bootstrap, however, as it lacks a mechanism for simulating new response values on their own; we will therefore borrow this part from one of the other variants, losing part of its parsimony and robustness in the process. 

The bootstrap replicates of the prediction error in \cref{eq:pred-error-boot} can be used for many kinds of predictive inference, from computing prediction intervals to estimating the mean squared error of prediction. While this offers an avenue for incorporating the process variance, it is not the most suitable one for our purposes. Recall from \cref{chapter:intro} that we want to investigate how the reserve is impacted by violations of the assumptions of certain actuarial models which can be framed in terms of regression, as we shall see in \cref{chapter:mack,chapter:poisson}; in other words, we need to simulate the distribution of the response itself to study how it is affected. However, we cannot do this just by generating fluctuations around $\mathbf{x}^T_+ \widehat{\bm{\beta}}$, as in \cref{eq:pred-error-boot}, as we would then fail to account for the error in our parameter estimates, leading to an underestimation of the prediction uncertainty. 

To address this difficulty, we have to borrow a concept from Bayesian school of statistics\sepfootnote{fn:bayes-necessary}. Recall that the Bayesian point of view is premised on the idea that the parameters $\bm{\theta}$ governing a statistical model $p(y \mid \bm{\theta}, x_1, \dots, x_p)$ are themselves random variables, which are assumed to follow a so-called \emph{prior distribution}\index{prior distribution} $p(\theta)$ expressing the beliefs we have about them before observing any data\sepfootnote{fn:bayes}. When presented with a sample $D = \{ (\bm{\mathrm{x}_1}, Y_1), \dots, (\bm{\mathrm{x}_n}, Y_n) \}$, we are then led to \emph{update our beliefs} using the formula
\begin{equation}
  p(\bm{\theta} \mid D) \propto p(D \mid \bm{\theta}) p(\bm{\theta}) \,,
\end{equation}
which is known as \emph{Bayes' rule}, thereby obtaining a \emph{posterior distribution} $p(\bm{\theta} \mid D)$ expressing the new likelihood of different values of $\bm{\theta}$ in light of the observed evidence. For any value of the parameters, the likelihood of the response at a new input, conditional on this value and the sample $D$, is now given by $p(y_+ \mid \bm{\theta}, D)$. By marginalising over the posterior distribution, we then incorporate all possible values of $\bm{\theta}$ in proportion to their likelihood under the data, resulting in the \emph{posterior predictive distribution}\index{posterior predictive distribution}.
\begin{equation}
  p(y_+ \mid D) = \int p(y_+ \mid \bm{\theta}) p(\bm{\theta} \mid D) \, d \bm{\theta} \,.
\end{equation}
of $Y_+$ given $D$. This incorporates both the intrinsic variability of the response as well as our uncertainty regarding the parameters (or, in the classical view, their estimates). By contrast, the \emph{fitted distribution}\index{fitted distribution} $p(y_+ \mid \widehat{\bm{\theta}})$, obtained by plugging in an estimate of the parameters, does not possess this virtue. Moreover, the predictive distribution can be easily integrated into the bootstrap framework; indeed, we can follow the same steps for simulating the prediction errror, but instead of \cref{eq:pred-error-boot} compute pseudo-responses
\begin{equation} \label{eq:pred-dist}
  Y_+^{(b, s)} \coloneqq \mathbf{x}_+^T \widehat{\bm{\beta}}^{(b)} + r^{(s)} \,.
\end{equation}
This explains why it is difficult to fit this approach within a classical frequentist paradigm, as it is unclear which theoretical quantity these bootstrap replicates would be approximating in that case. We hasten to add that there have been many suggestions to remedy this discrepancy, leading among other things to the idea of a \emph{confidence distribution}\index{confidence distribution} which appears to be the solution most widely adopted (see, for example, \cites{barndorff-nielsen,lawless}). As noted by \textcite{dickson}, however, the fact remains that there exists a veritable cornucopia of `frequentist predictive distributions', lending credence to the notion that this concept fits in more naturally with the Bayesian framework.

Similar to \cref{eq:pred-dist}, we can obtain a bootstrap version of the fitted distribution using
\begin{equation} \label{eq:fit-dist}
  Y_+^{(s)} \coloneqq \mathbf{x}_+^T \widehat{\bm{\beta}} + r^{(s)} \,,
\end{equation}
which will allow us to compare the two. For the following example, we consider a simulated dataset generated from a normal linear model with $p = 10$, $n = 1000$, $\sigma = 5.7$ and
\begin{displaymath}
  \bm{\beta} = \begin{bmatrix} 4.1 & 4.4 & -2.1 & 3.3 & 1.4 & 0.2 & 2.4 & -3.7 & 1.6 & 2.1 \end{bmatrix} \,.
\end{displaymath}
The predictive and fitted distributions for both the parametric as well as non-parametric bootstrap are shown in \cref{fig:boot-reg}. We can see in this case that the two are nigh indistinguishable, reflecting the high precision of the parameter estimates on a sample constructed in perfect accordance with the model assumptions. As we shall see in \cref{sec:mack-proc}, the results for real data will be quite different.

\begin{figure}[!htb]
  \begin{subfigure}{\linewidth}
    \includegraphics{boot_reg_param}
    \subcaption{Parametric}
  \end{subfigure}
  \begin{subfigure}{\linewidth}
    \includegraphics{boot_reg_semiparam}
    \subcaption{Semiparametric}
  \end{subfigure}
  \caption{Comparison of the fitted and predictive bootstrap distributions for a normal linear model}
  \label{fig:boot-reg}
\end{figure}

\chapter{Mack's model} \label{chapter:mack}

\section{Introduction} \label{sec:mack-intro}

In his seminal paper \cite{mack:chain-ladder}, Mack proposed the following model for cumulative claims triangles, which remains among the most influential in actuarial reserving.
\begin{model}[Mack chain ladder] \label{model:mack} \leavevmode
  \begin{enumerate}[label=\bf{\textup{(}Mack\arabic*\textup{)}},ref=\textup{(}Mack\arabic*\textup{)}, wide]
    \item \label{assump:mack-expectation} There exist development factors $f_1, \dots, f_{I - 1}$ such that
          \begin{equation}
            \mathbb{E}[C_{ij} \ \Vert \ C_{i, j - 1}, \dots, C_{i1}] = \mathbb{E}[C_{ij} \ \Vert \ C_{i, j - 1}] = f_{j - 1} C_{i, j - 1}\,
          \end{equation}
          for $1 \leq i \leq I$.
    \item \label{assump:mack-variance} There exist variance parameters $\sigma_1, \dots, \sigma_{I - 1}$ such that
          \begin{equation}
            \mathrm{Var}[C_{ij} \ \Vert \ C_{i, j - 1}, \dots, C_{i1}] = \mathrm{Var}[C_{ij} \ \Vert \ C_{i, j - 1}] = \sigma_{j - 1}^2 C_{i, j - 1}\,,
          \end{equation}
          for $1 \leq i \leq I$.
    \item \label{assump:mack-independence} The cumulative claims processes $(C_{ij})_j, (C_{i'j})_j$ are independent for $i \neq i'$.
  \end{enumerate}
\end{model}\index{Mack chain ladder}
The development factors\index{development factor} are estimated by
\begin{equation} \label{eq:devfac-estimator}
  \widehat{f}_j(\mathcal{D}_I) = \widehat{f}_j(C_{1j}, \dots, C_{I - j, j}, \dots, C_{1, j + 1}, \dots, C_{I - j, j + 1}) \coloneqq \frac{\sum_{i = 1}^{I - j} C_{i, j + 1}}{\sum_{i = 1}^{I - j} C_{i, j}} \,.
\end{equation}
If we define the \emph{single} or \emph{individual} development factors\index{development factor!single} as
\begin{equation}
  F_{i, j + 1} \coloneqq \frac{C_{i, j + 1}}{C_{ij}} \,,
\end{equation}
then $\widehat{f}_j$ can be obtained as the weighted average
\begin{equation}
  \widehat{f}_j = \frac{\sum_{i = 1}^{I - j} C_{ij} F_{i, j + 1}}{\sum_{i = 1}^{I - j} C_{ij}} \,.
\end{equation}
The variance parameters\index{variance parameters} are estimated by
\begin{equation}
  \widehat{\sigma}_j \coloneqq \sqrt{\frac{1}{I - j - 1}\sum_{i = 1}^{I-j} C_{ij}\left( F_{i, j + 1} - \widehat{f}_j \right)^2}
\end{equation}
for $j < I - 1$. This formula does not work for $j = I - 1$, as we only have a single pair of observations in the last two columns of the triangle. To remedy this, Mack proposed a simple extrapolation from the previous development years, leading to the estimator
\begin{equation}
  \widehat{\sigma}_{I - 1} = \sqrt{\min{ \left \{ \frac{\widehat{\sigma}^4_{I - 2}}{\widehat{\sigma}^2_{I - 3}}, \widehat{\sigma}^2_{I - 2}, \widehat{\sigma}^2_{I - 3} \right \} }} \,,
\end{equation}
which appears to be the most widely adopted one in the literature.

Under the assumptions of \modelref{model:mack}, it can be shown (see \cite[Lemma 2.5 and Lemma 3.5]{wuthrich:stochastic-reserving}) that $\widehat{f}_j$ and $\widehat{\sigma}_j$ are (conditionally) unbiased, and that the $\widehat{f}_j$ are uncorrelated. Predictions for the ultimate claim amounts $C_{iI}$ are obtained by substituting these estimates for the unknown development factors $f_j$ in the conditional expectation. In other words, we predict the ultimate loss using the conditional mean $\condexpp{C_{iI}}{C_{i, I + 1 - i}}$, and estimate the latter by plugging in $\widehat{f}_j$, yielding
\begin{equation}
  \widehat{C}_{iI} \coloneqq \widehat{\mathbb{E}}[C_{i, I} \ \Vert \ C_{i, I + 1 - i}] = C_{i, I + 1 - i} \prod_{j = I + 1 - i}^{I - 1} \widehat{f}_j \,.
\end{equation}
From this, we then finally obtain the reserve predictor \index{reserve!predictor}
\begin{equation} \label{eq:reserve-predictor}
  \widehat{R} = g(\mathcal{D}_I) \coloneqq \sum_{i = 2}^I (\widehat{C}_{i, I} - C_{i, I + 1- i}) \,.
\end{equation}

\Modelref{model:mack} is often referred to as "distribution-free"\index{distribution-free} because it only makes assumptions about the first two moments of the random variables in the claims triangle. Indeed, we will now show that this model can be viewed as a series of linear regressions through the origin (i.e.\ without intercept term), in which case \cref{assump:mack-expectation,assump:mack-variance,assump:mack-independence} correspond to the assumptions of the Gauss-Markov theorem\sepfootnote{fn:gauss-markov}\index{Gauss-Markov} guaranteeing optimality of the estimator \cref{eq:devfac-estimator}. If we introduce, for any development year $j \in \{ 1, \dots, {I - 1} \}$, the notation
\begin{equation}
  \bm{\mathrm{c}_j} \coloneqq
  \begin{bmatrix}
    C_{1, j} \\
    \vdots   \\
    C_{I - j, j}
  \end{bmatrix} \,,
\end{equation}
then the first two assumptions of \modelref{model:mack} can be equivalently stated as
\begin{equation}
  \bm{\mathrm{c}_{j + 1}} = f_j \bm{\mathrm{c}_j} + \bm{\varepsilon} \,,
\end{equation}
with $\bm{\varepsilon}$ a random vector satisfying
\begin{equation}
  \condexpp{\bm{\varepsilon}}{C_{1, j}, \dots, C_{I - j, j}} = \bm{0}
  \qquad%
  \condvarr{\bm{\varepsilon}}{C_{1, j}, \dots, C_{I - j, j}} = \sigma^2_j
  \begin{bmatrix}
    C_{1j} &        &              \\
           & \ddots &              \\
           &        & C_{I - j, j}
  \end{bmatrix} \,.
\end{equation}
Consequently, it follows (see \cite[Proposition 1.7]{hayashi}) that the weighted least squares\index{weighted least squares} method with weights matrix
\begin{equation}
  \mathbf{W} =
  \begin{bmatrix}
    1 / C_{1j} &        &                  \\
               & \ddots &                  \\
               &        & 1 / C_{I - j, j}
  \end{bmatrix} \,,
\end{equation}
leads to an estimator for $f_j$ which has minimal variance in the class of linear unbiased estimators. This estimator is given by
\begin{equation}
  \widehat{f}^{\mathrm{WLS}}_j = (\bm{\mathrm{c}_j}^T \mathbf{W} \bm{\mathrm{c}_j})^{-1} \bm{\mathrm{c}_j}^T \mathbf{W} = \frac{\sum_{i = 1}^{I - j} C_{i, j + 1}}{\sum_{i = 1}^{I - j} C_{i, j}} \,,
\end{equation}
which is the same expression as \cref{eq:devfac-estimator}.

\section{A challenging simulation} \label{sec:mack-challenge}

Owing to its recursive\index{recursive} nature, Mack's model does not readily lend itself to application of the theory from \cref{chapter:boot}. Unfortunately, previous work in this area is rather scarce, as studies of \modelref{model:mack} have tended to focus on deriving closed-form expressions for the uncertainty of the reserve; in particular, much attention has been spent on finding approximations of the conditional \emph{mean square error of prediction}\index{mean squared error of prediction} (MSEP). The actuarial literature on bootstrap methods is not very helpful either, as it has mostly been concerned with generalised linear models---even papers such as \cite{england:dist} which address \modelref{model:mack} frame it as a GLM. As will become clear shortly, this passes over some subtleties related to the particular structure of Mack's model, and we therefore choose a different approach here. In particular, we will take as our starting point the problem of analytically estimating the MSEP. While this might appear at first glance to be unrelated (even antithetical) to the bootstrap, we will see that theoretical framework associated with it can be easily subverted for our purposes.

The MSEP is a measure for the uncertainty of a given predictive model. It is defined as the Euclidean distance between the predictor and the response in the underlying filtered probability space, i.e.\
\begin{equation}
  \underset{R \, \vert \, \mathcal{D}_I}{\mathrm{MSEP}}(\widehat{R}) \coloneqq \condexpp*{(\widehat{R} - R)^2}{\mathcal{D}_I}
\end{equation}
for our special case of predicting the reserve. The MSEP admits a decomposition, similar to the familiar bias-variance decomposition from classical statistics, into so-called \emph{parameter}\index{parameter error} or \emph{estimation error} and \emph{process error}\index{process error}:
\begin{align}
  \begin{split}
    \condexpp*{(\widehat{R} - R)^2}{\mathcal{D}_I} &= \condexpp*{(R - \condexpp*{R}{\mathcal{D}_I})^2}{\mathcal{D}_I} + \condexpp*{(\condexpp*{R}{\mathcal{D}_I} - \widehat{R})^2}{\mathcal{D}_I} \\ &\phantom{{}=1} -2\condexpp*{(R - \condexpp*{R}{\mathcal{D}_I})(\condexpp*{R}{\mathcal{D}_I} - \widehat{R})}{\mathcal{D}_I}
  \end{split} \\
  \begin{split}
    &= \mathrm{Var}(R \, \Vert \, \mathcal{D}_I) + (\condexpp*{R}{\mathcal{D}_I} - \widehat{R})^2 \\
    &\phantom{{}=1} -2(\condexpp*{R}{\mathcal{D}_I} - \widehat{R})(\condexpp*{R - \condexpp*{R}{\mathcal{D}_I}}{\mathcal{D}_I})
  \end{split}                                                                                                                                                                                                                                               \\
   & = \underbrace{\mathrm{Var}(R \, \Vert \, \mathcal{D}_I)}_\text{process error} + \underbrace{(\condexpp*{R}{\mathcal{D}_I} - \widehat{R})^2}_\text{estimation error} \,,
\end{align}
corresponding to the two stages of bootstrapping a predictor which we discussed in \cref{sec:boot-reg}. Consider now, for any accident year $i \in \{ 1, \dots, I \}$, the MSEP for the associated ultimate
\begin{equation} \label{eq:msep}
  \underset{C_{iI} \, \vert \, \mathcal{D}_I}{\mathrm{MSEP}}(\widehat{C}_{iI}) = (\condexpp*{C_{iI}}{\mathcal{D}_I} - \widehat{C}_{iI})^2 + \condvarr{C_{iI}}{\mathcal{D}_I} \,,
\end{equation}
and suppose we are interested in obtaining a closed-form estimator for it. Such an expression can be derived relatively straightforwardly for the process error from the assumptions of \modelref{model:mack} in the following way. We begin by applying the law of total variance in conjunction with \cref{assump:mack-expectation,assump:mack-variance} to obtain
\begin{align}
  \condvarr{C_{iI}}{\mathcal{D}_I} & = \condvarr{C_{iI}}{C_{i, I + 1 - i}}                                                                                             \\
                                   & = \condexpp{\condvarr{C_{iI}}{C_{i, I - 1}}}{C_{i, I + 1 - i}} + \condvarr{\condexpp{C_{iI}}{C_{i, I - 1}}}{C_{i, I + 1 - i}}     \\
                                   & = \sigma^2_{I - 1} \condexpp{C_{i, I - 1}}{C_{i, I + 1 - i}} + f^2_{I - 1} \condvarr{C_{i, I - 1}}{C_{i, I + 1 - i}}              \\
                                   & = \sigma^2_{I - 1} C_{i, I + 1 - i} \prod_{j = I + 1 - i}^{I - 2} f_j + f^2_{I - 1} \condvarr{C_{i, I - 1}}{C_{i, I + 1 - i}} \,,
\end{align}
which is a linear recurrence equation of the form
\begin{equation}
  x_n = a_{n - 1} x_{n - 1} + g_{n - 1}
\end{equation}
with $x_n = \condvarr{C_{in}}{C_{i, I + 1 - i}}$ and
\begin{equation}
  g_{n - 1} = \sigma^2_{n - 1} C_{i, I + 1 - i} \prod_{j = I + 1 - i}^{n - 1} f_j \,, \qquad a_{n - 1} = f^2_{n - 1} \,.
\end{equation}
The general solution is given by
\begin{equation}
  x_n = \left( \prod_{j = n_0}^{n - 1} a_j \right) \left( x_{n_0} + \sum_{k = n_0}^{n - 1} \frac{g_k}{\prod_{l = n_0}^k a_l} \right)
\end{equation}
where $n_0$ denotes the first index of the sequence $x_n$, in our case $I + 1 - i$. Using the initial condition $x_{I + 1 - i} = \condvarr{C_{i, I + 1 - i}}{C_{i, I + 1 - i}} = 0$, we finally obtain
\begin{align}
  \condvarr{C_{iI}}{\mathcal{D}_I}
   & = \left( \prod_{j = I + 1 - i}^{I - 1} f^2_j \right) \left( \sum_{k = I + 1 - i}^{I - 1} \frac{\sigma^2_k C_{i, I + 1 - i} \prod_{j = I + 1 - i}^{k - 1} f_j}{\prod_{j = I + 1 - i}^k f^2_j} \right) \\
   & = \left( \prod_{j = I + 1 - i}^{I - 1} f^2_j \right) C^2_{i, I + 1 - i} \left( \sum_{k = I + 1 - i}^{I - 1} \frac{\sigma^2_k / f^2_k}{\prod_{j = I + 1 - i}^{k - 1} f_j C_{i, I + 1 - i}} \right)    \\
   & = \condexpp{C_{iI}}{C_{I + 1 - i}}^2 \sum_{k = I + 1 - i}^{I - 1} \frac{\sigma^2_k / f^2_k}{\condexpp{C_{ik}}{C_{i, I + 1 - i}}} \,,
\end{align}
which we can estimate by plugging in $\widehat{f}_j$ and $\widehat{\sigma}_j$ for $f_j$ and $\sigma_j$, respectively.

For the parameter error, if we use the definitions from the previous section to rewrite it as
\begin{align} \label{eq:param-err}
  (\condexpp*{C_{iI}}{\mathcal{D}_I} - \widehat{C}_{iI})^2 & = C^2_{i, I+1-i} \left( \prod_{j=I+1-i}^{I-1}f_j - \prod_{j=I+1-i}^{I-1}\widehat{f}_j \right)^2                                                  \\
                                                           & = C^2_{i, I+1-i} \left( \prod_{j=I+1-i}^{I-1}f^2_j + \prod_{j=I+1-i}^{I-1}\widehat{f}^2_j - 2 \prod_{j=I+1-i}^{I-1}f_j \widehat{f}_j \right) \,,
\end{align}
it becomes clear that things are more complicated than with process error. Indeed, we cannot simply subtitute the $\widehat{f}_j$ for the unknown parameters in this expression as that would cause it to vanish, yielding an estimate which will generally not be accurate. This problem was recognised by Mack himself in \cite{mack:chain-ladder-variability}, and is caused by the fact that the claims triangle observations are used for both estimation and forecasting (see \cite[Section 2]{lindholm:msep} for a more general discussion). His suggested solution was to apply some kind of conditional averaging to the $\widehat{f}_j$. Ideally, one would like to condition on all available observations in $\mathcal{D}_I$, but the $\mathcal{D}_I$-measurability of the $\widehat{f}_j$ would then bring us right back where we started. We must therefore use a smaller set in order to allow $\widehat{f}_{I + 1 - i}, \dots, \widehat{f}_{I - 1}$ to fluctuate around $f_{I + 1 - i}, \dots, f_{I - 1}$. This corresponds to asking which other values $\widehat{f}_j$ could have taken, given that we fix a certain subset of the data---in other words, it's a resampling scheme on the parameter estimates. Thus, one can obtain an estimate of the parameter error by specifying a mechanism for generating new realisations of $\widehat{f}_j$ (see \cites{wuthrich:chain-ladder-msep}[44 \psqq]{wuthrich:stochastic-reserving}), with different mechanisms yielding different estimates. The literature uses this mostly as a theoretical device to facilitate analytical calculations; for the specific approach developed by Mack, it leads to the estimator
\begin{equation} \label{eq:mack-msep-estimator}
  \widehat{\mathrm{MSEP}}(\widehat{R_i}) \coloneqq \widehat{C}_{iI} \sum_{j = I + 1 - i}^{I - 1} \frac{\widehat{\sigma}^2_j}{\widehat{f}_j} \left( \frac{1}{\widehat{C}_{ij}} + \frac{1}{\sum_{i = 1}^{I - j} C_{ij}} \right)
\end{equation}
(see \cite[11]{mack:chain-ladder-variability}).
In this case, however, the theory happens to fit in perfectly with the resampling framework, and we can therefore employ it as a basis for bootstrap procedures. In the remainder of this section, we outline two approaches for estimating \cref{eq:param-err} and indicate the corresponding resampling methods.

Denote the subset of observations in $\mathcal{D}_I$ up to and including development year $k$ by
\begin{equation}
  \mathcal{B}_k \coloneqq \left \{ C_{ij} \in \mathcal{D}_I \mid j \leq k \right \} \,,
\end{equation}
and write
\begin{equation}
  \mathcal{D}^O_{I, k} \coloneqq \left \{ C_{ij} \in \mathcal{D}_I \mid j > I + 1 - k \right \}
\end{equation}
for its complement. One option would then be to take the conditional expectation of $\widehat{f}_j$ with respect to $\mathcal{B}_{I+1-i}$, leading to the estimate
\begin{align} \label{eq:uncond-param-err}
  \condexpp{(\condexpp{C_{iI}}{\mathcal{D}_I} - \widehat{C}_{iI})^2}{\mathcal{B}_{I+1-i}} & = C^2_{i, I+1-i} \condexpp*{\left( \prod_{j=I+1-i}^{I-1}f_j - \prod_{j=I+1-i}^{I-1}\widehat{f}_j \right)^2}{\mathcal{B}_{I+1-i}}      \\
                                                                                          & = C^2_{i, I+1-i} \left( \condexpp*{\prod_{j=I+1-i}^{I-1}\widehat{f}^2_j}{\mathcal{B}_{I+1-i}} - \prod_{j=I+1-i}^{I-1}f^2_j \right)\,,
\end{align}
where we used the fact that the $\widehat{f}_j$ are uncorrelated. This corresponds to averaging over the distribution of $\mathcal{D}^O_{I, k}$, or, expressed in terms of resampling, to generating new pseudo-observations in the upper right triangle. Borrowing the nomenclature from \cite{wuthrich:chain-ladder-msep}, we call this the \emph{unconditional approach}\index{unconditional approach}. Alternatively, we could average each $\widehat{f}_j$ only over the observations after $j$. This is equivalent to fixing the denominator $\sum_{i = 1}^{I - j} C_{ij}$ in the development factor estimator \cref{eq:devfac-estimator} and allowing the numerator $\sum_{i = 1}^{I - j} C_{i, j + 1}$ to vary. Formally, it corresponds to taking the expectation with respect to the probability measure defined on $\mathcal{D}^O_{I, i}$ by
\begin{equation}
  \mathbb{P}^*_{\mathcal{D}_I}(\{ dz_{ij} \}_{i + j \leq I + 1}) \coloneqq \prod_{j = 1}^{I - 1} \prod_{i = 1}^{I - j}  \mathbb{P}_{C_{i, j + 1}}(dz_{i, j + 1} \mid C_{ij} = c_{ij}) \,,
\end{equation}
yielding the estimate
\begin{align}
  \expect[\mathbb{P}^*_{\mathcal{D}_I}]{(\condexpp{C_{iI}}{\mathcal{D}_I} - \widehat{C}_{iI})^2} & = C^2_{iI} \, \expect[\mathbb{P}^*_{\mathcal{D}_I}]{\left( \prod_{j=I+1-i}^{I-1}f_j - \prod_{j=I+1-i}^{I-1}\widehat{f}_j \right)^2}      \\
                                                                                                 & = C^2_{i, I+1-i} \left( \expect[\mathbb{P}^*_{\mathcal{D}_I}]{\prod_{j=I+1-i}^{I-1}\widehat{f}^2_j} - \prod_{j=I+1-i}^{I-1}f^2_j \right) \\
                                                                                                 & = C^2_{i, I+1-i} \left( \prod_{j=I+1-i}^{I-1} \condexpp*{\widehat{f}^2_j}{\mathcal{B}_j} - \prod_{j=I+1-i}^{I-1}f^2_j \right)
\end{align}
(see \cite[46]{wuthrich:stochastic-reserving}). This is referred to as the \emph{conditional approach}\index{conditional approach}, and it corresponds to a scheme in which only the observations from the next period are resampled to produce a new pseudo-realisation of the parameter estimate for the current period.

There has been some controversy about which of these approaches should be preferred, leading to a vigorous discussion in \cite{wuthrich:chain-ladder-msep, mack:msep, gisler:msep, venter:msep}. As we shall see in \cref{sec:mack-boot}, the difference between the results which they produce is negligeable, and so the question is mainly of theoretical interest. Nevertheless, based on the previous exposition, it seems reasonable to prefer whichever method produces resampled parameter estimates approximating the original $\widehat{f}_j$ most closely. In particular, we note that these posses the following property, the proof of which can be found in \cite{mack:msep}.

\begin{theorem}
  The squares of two successive development factor estimates in \modelref{model:mack} are negatively correlated:
  \begin{equation}
    \mathrm{Cov}(\widehat{f}_j, \widehat{f}_{j - 1}) < 0 \,.
  \end{equation}
\end{theorem}

\noindent In the conditional approach, the resampled parameter estimates are independent by construction, and so they cannot incorporate this covariance structure. In light of this, it would appear that the unconditional scheme has slightly better theoretical properties. As the empirical difference between the two is minimal, however, the conditional version is a reasonable approximation to fall back on when needed. In the next section, we will see how both approaches give rise to a variety of different bootstrap methods.

\section{Bootstrap methodology} \label{sec:mack-boot}

In \cref{sec:boot-reg}, we introduced a taxonomy for the different types of bootstrap, distinguishing between semiparameteric, nonparametric and parametric variants; we now consider how each of these can be applied to \modelref{model:mack}. For comparison, the analytical results for the example dataset from \cref{tab:uk-motor} (using the estimator in \cref{eq:mack-msep-estimator}) are provided in \cref{tab:mack-bench}.

\begin{table}[!htb]
  \centering
  \begin{tabularx}{0.7\linewidth}{XXXXX}\toprule%
    $i \,/ \,j$ & $\widehat{f}^\mathrm{CL}_j$ & $\widehat{\sigma}^\mathrm{CL}_i$ & $\widehat{R}_i^\mathrm{CL}$ & $\widehat{\mathrm{MSEP}}(\widehat{R}_j)$ \\ \midrule
    \csvreader[
      head to column names,
    late after line =                                                                                                                                     \\
    ]{%
      ../results/example/mack_bench.csv
    }{}{%
    \idx        & \devfacs                    & \sigmas                          & \reserve                    & \prederror
    } \bottomrule%
  \end{tabularx}
  \caption{Mack chain ladder results for the UK Motor triangle from \cref{tab:uk-motor}}
  \label{tab:mack-bench}
\end{table}

For the semiparametric bootstrap\index{bootstrap!semiparametric}, the crucial step is to find a suitable definition for the residuals which ensures that they are interchangeable. The distribution-free nature of the model makes this difficult, however, as it limits the statements we can make about the errors to the first two moments. This can be resolved in one of two ways. On the one hand, we could decide to extrapolate from homogeneity of the mean and variance to homogeneity of the distributions. In that case, the \emph{raw residuals}\index{residuals!raw}
\begin{equation}
  r_{i, j + 1} \coloneqq C_{i, j + 1} - \widehat{C}_{i, j + 1} = C_{i, j + 1} - \widehat{f}_j C_{ij}
\end{equation}
are not an option, as they suffer from heteroscedasticity\index{heteroscedasticity}:
\begin{equation}
  \condvarr{r_{i, j + 1}}{C_{ij}} = \sigma^2_j \left( C_{ij} - \frac{C^2_{ij}}{\sum_{i = 1}^{I - j} C_{ij}} \right) \,.
\end{equation}
We can address this by dividing out the variance and considering the errors
\begin{equation}
  \varepsilon_{i, j + 1} \coloneqq \frac{C_{i, j + 1} - f_j C_{ij}}{\sigma_j \sqrt{C_{ij}} \sqrt{1 - \frac{C_{ij}}{\sum_{i = 1}^{I - j} C_{ij}}}} \,,
\end{equation}
which satisfy $\condexpp{\varepsilon_{i, j + 1}}{C_{ij}} = 0$ and $\condvarr{\varepsilon_{i, j + 1}}{C_{ij}} = 1$. Provided the sampling variability of the $\widehat{f}_j$ and $\widehat{\sigma}_j$ is not too bad (a non-trivial assumption given the small sample sizes we're usually dealing with), the same should approximately hold for the residuals
\begin{equation}
  r_{i, j + 1} \coloneqq \frac{C_{i, j + 1} - \widehat{f}_j C_{ij}}{\widehat{\sigma}_j \sqrt{C_{ij}} \sqrt{1 - \frac{C_{ij}}{\sum_{i = 1}^{I - j} C_{ij}}}}
\end{equation}
obtained by plugging in these estimators. Note that the factor $\sqrt{1 - \frac{C_{ij}}{\sum_{i = 1}^{I - j} C_{ij}}}$ in the denominator corresponds to the leverage adjustment\index{leverage}, as can be seen by computing the hat matrix:
\begin{align}
  \mathbf{H} & = \bm{\mathrm{c}_j} (\bm{\mathrm{c}_j}^T \mathbf{W} \bm{\mathrm{c}_j})^{-1} \bm{\mathrm{c}_j}^T \mathbf{W} \\[4pt]
             & = \frac{1}{\sum_{i = 1}^{I - j} C_{ij}}
  \begin{bmatrix}
    C_{1j}       & \dots  & C_{1j}       \\
    \vdots       & \ddots & \vdots       \\
    C_{I - j, j} & \dots  & C_{I - j, j}
  \end{bmatrix} \,.
\end{align}
It's worth emphasising, however, that such an extrapolation should not be made lightly: it is perfectly possible for the error distribution to exhibit heterogeneity in ways other than through its mean and variance (see \cite[114]{efron:intro} for an example where the \emph{percentiles} vary with the value of the regressor). In light of this, an alternative approach would be to augment our model with some explicit distributional assumptions, which is more transparent and allows us to make precise statements about errors and residuals. One such augmentation that has been studied in the literature (see \cite[49]{wuthrich:stochastic-reserving}) is the autoregressive Gaussian time series model\index{time series model}
\begin{equation} \label{eq:time-series-model}
  C_{i, j + 1} = f_j C_{ij} + \sigma_j \, \sqrt{C_{ij}} \, \varepsilon_{i, j + 1}, \qquad \varepsilon_{i, j + 1} \sim \mathcal{N}(0, 1) \,,
\end{equation}
which can easily be seen to be compatible with \cref{assump:mack-expectation,assump:mack-variance,assump:mack-independence}. In light of the fact that \modelref{model:mack} can be viewed as a series of weighted linear regressions, as we say in \cref{sec:mack-intro}, this has the benefit of making available to us the results of the classical theory. We know, for example, that the \emph{externally studentised residuals}\index{residuals!externally studentised}
\begin{equation}
  r_{i, j + 1}
  \coloneqq \frac{e_{i, j + 1}}{\widehat{\sigma}_{j (i)} \sqrt{1 - \mathbf{H}_{ii}}} \sqrt{\mathbf{W}_{ii}}
  = \frac{C_{i, j + 1} - \widehat{f}_j C_{ij}}{\widehat{\sigma}_{j (i)} \sqrt{C_{ij}} \sqrt{1 - \frac{C_{ij}}{\sum_{i = 1}^{I - j} C_{ij}}}} \,,
\end{equation}
where $\widehat{\sigma}_{j (i)}$ denotes the leave-$i$-out estimator of $\sigma_j$, follow a $t_{I - j - 1}$ distribution. Similarly, the \emph{standardised}\index{residuals!standardised} or \emph{internally studentised} residuals
\begin{equation}
  r_{i, j + 1} \coloneqq \frac{C_{i, j + 1} - \widehat{f}_j C_{ij}}{\widehat{\sigma}_j \sqrt{C_{ij}} \sqrt{1 - \frac{C_{ij}}{\sum_{i = 1}^{I - j} C_{ij}}}} \,
\end{equation}
also share a common (although more complicated) distribution (see \cite[267 \psqq]{seber}).

A major shortcoming of \cref{eq:time-series-model}, however, is that it makes it possible to have a negative realisation in the next step of the time series, in which case all future observations from that point on are undefined because of the square root factor in the second term. This is not merely a theoretical problem either: we have observed this phenomenon in our numerical implementation, where the resampling produces negative pseudo-realisations of certain claim amounts, particularly when the model has been severely perturbed. An obvious way of avoiding this difficulty would be to simply discard the current bootstrap iteration as soon as a negative value is produced. This has the drawback of requiring more computational power, sometimes beyond the realm of what is reasonable. Moreover, we have seen cases in which the probability of having no negative replicates was so vanishingly small as to cause the program to get stuck indefinitely. It is therefore imperative that we develop a different approach, if only as a fall-back in case of problems with \cref{eq:time-series-model}.

\Textcite[238]{england:dist} discuss an similar difficulty in the context of generating bootstrap realisations of future claim amounts, which we discuss in \cref{sec:mack-proc}. To avoid negative draws, the authors recommend using a gamma distribution in place of the Gaussian, choosing parameters that lead to the same mean and variance. This suggests that our woes may be remedied by choosing a different distribution in \cref{eq:time-series-model} which is capable of guaranteeing
\begin{equation} \label{eq:lower-limit-err}
  \varepsilon_{i, j + 1} > -\frac{f_j \sqrt{C_{ij}}}{\sigma_j} \,.
\end{equation}
Clearly, the support of such a distribution has to be bounded from below. Moreover, it must allow for normalisation, so that the resulting residuals can be made interchangeable by an algebraic transformation. A good candidate satisfying these conditions which readily comes to mind is the \emph{shifted log-normal distribution}\index{shifted log-normal distribution}
\begin{equation}
  \log(\varepsilon_{i, j + 1} - \xi_{ij}) \sim \mathcal{N}(m_{ij}, s_{ij}^2)
\end{equation}
for certain parameters $\xi_{ij}$, $m_{ij}$ and $s_{ij}$. his has support $(\xi_{ij}, +\infty)$, leading us to choose $\xi_{ij} = -\frac{f_j \sqrt{C_{ij}}}{\sigma_j}$, and we must determine $m_{ij}$ and $s_{ij}$ to satisfy
\begin{equation}
  \begin{cases}
    \displaystyle \condexpp{\varepsilon_{i, j + 1}}{C_{ij}} = \exp \left( m_{ij} + \frac{s_{ij}^2}{2} \right) + \xi_{ij} = 0 \\
    \displaystyle \condvarr{\varepsilon_{i, j + 1}}{C_{ij}} = (\exp{s_{ij}^2} - 1) \exp(s_{ij}^2 + 2 m_{ij}) = 1 \,.
  \end{cases}
\end{equation}
Solving these equations, we obtain
\begin{equation}
  s_{ij} = \sqrt{\log \left( 1 + \frac{1}{\xi^2_{ij}} \right)}, \qquad m_{ij} = \log(-\xi_{ij}) - \frac{s^2_{ij}}{2} \,.
\end{equation}
Provided the sampling variability of $\widehat{f}_j$ and $\widehat{\sigma}_j$ is not too severe, this means that the residuals
\begin{align} \label{eq:log-normal-resids}
  r_{i, j + 1}
   & \coloneqq \frac{\log \left( C_{i, j + 1} - \widehat{f}_j C_{ij} + \widehat{f}_j \sqrt{C_{ij}} / \widehat{\sigma}_j \right) - \widehat{m}_{ij}}{\widehat{s}_{ij}}                                                                                                                                                                               \\
   & = \frac{\log \left( C_{i, j + 1} - \widehat{f}_j C_{ij} + \widehat{f}_j \sqrt{C_{ij}} / \widehat{\sigma}_j \right) - \log(\widehat{f}_j \sqrt{C_{ij}} / \widehat{\sigma}_j) + \frac{1}{2} \log \left( 1 + \widehat{\sigma}^2_j / \widehat{f}^2_j C_{ij} \right)}{\sqrt{\log \left( 1 + \widehat{\sigma}^2_j / \widehat{f}^2_j C_{ij} \right)}}
\end{align}
are approximately $\mathcal{N}(0, 1)$-distributed.

After selecting one of these variants, the next step is to fit the model and compute the residuals from it. These are then resampled to generate pseudo-residuals $r^{(b)}_{ij}$, from which a bootstrap triangle is obtained by inverting the appropriate formula. The conditional approach employs the original triangle here, whereas the unconditional one uses the pseudo-realisations generated in the previous step. Finally, the model is refitted to the simulated triangle to obtain bootstrap replicates of the development factor and variance parameter estimators $\widehat{\bm{f}}$ and $\widehat{\bm{\sigma}}$. The entire procedure is outlined in \cref{alg:cond-semiparam-mack,alg:uncond-semiparam-mack} for the special case of standardised residuals; the others are completely analogous. Results for the example data from \cref{tab:uk-motor} are given in \cref{tab:semiparam-mack-res-standard,tab:semiparam-mack-res-student,tab:semiparam-mack-res-log-normal} for all three types. Note that these feature a bootstrap estimate for the MSEP; this is obtained using the method from \cref{sec:boot-proc} (see \cref{eq:pred-error-boot}), which we will adapt to \modelref{model:mack} in \cref{sec:mack-proc}. To preserve the continuity of the exposition, however, we have chosen to include them here already. All bootstrap results in chapter were computed with $B = 1000$ and $S = 100$.

\begin{table}[!htb]
  \begin{subtable}{0.5\linewidth}
    \input{../results/example/mack_semiparam_cond_standardised.tex}
    \subcaption{Conditional}
  \end{subtable}
  \hfill
  \begin{subtable}{0.5\linewidth}
    \input{../results/example/mack_semiparam_uncond_standardised.tex}
    \subcaption{Unconditional}
  \end{subtable}
  \caption{Results from the semiparameteric bootstrap with standardised residuals for the data in \cref{tab:uk-motor}}
  \label{tab:semiparam-mack-res-standard}
\end{table}

\begin{table}[!htb]
  \begin{subtable}{0.5\linewidth}
    \input{../results/example/mack_semiparam_cond_studentised.tex}
    \subcaption{Conditional}
  \end{subtable}
  \hfill
  \begin{subtable}{0.5\linewidth}
    \input{../results/example/mack_semiparam_uncond_studentised.tex}
    \subcaption{Unconditional}
  \end{subtable}
  \caption{Results from the semiparameteric bootstrap with studentised residuals for the data in \cref{tab:uk-motor}}
  \label{tab:semiparam-mack-res-student}
\end{table}

\begin{table}[!htb]
  \begin{subtable}{0.5\linewidth}
    \input{../results/example/mack_semiparam_cond_log-normal.tex}
    \subcaption{Conditional}
  \end{subtable}
  \hfill
  \begin{subtable}{0.5\linewidth}
    \input{../results/example/mack_semiparam_uncond_log-normal.tex}
    \subcaption{Unconditional}
  \end{subtable}
  \caption{Results from the semiparameteric bootstrap with log-normal residuals for the data in \cref{tab:uk-motor}}
  \label{tab:semiparam-mack-res-log-normal}
\end{table}

\begin{figure}[p]
  \begin{algorithm}[H]
    \caption{Conditional semiparametric bootstrap for \modelref{model:mack}}
    \label{alg:cond-semiparam-mack}
    \begin{algorithmic}
      \Require{Cumulative claims triangle $\mathcal{D}_I$, number of bootstrap samples $B$}
      \State $(\{ r_{ij} \mid i + j \leq I + 1 \}, \bm{\widehat{f}}, \bm{\widehat{\sigma}}) \gets$ \Call{fit}{$\mathcal{D}_I$}
      \For{$b \gets 1, B$}
        \State $\{ r^{(b)}_{ij} \mid i + j \leq I + 1 \} \gets$ \Call{resample}{$\{ r_{ij} \mid i + j \leq I + 1 \}$}
        \For{$j \gets 1, I - 1$}
          \For{$i \gets 1, I - j$}
            \State $C^{(b)}_{i, j + 1} \gets \widehat{f}_j C_{ij} + \widehat{\sigma}_j \sqrt{C_{ij}} r^{(b)}_{i, j + 1}$
            \State $F^{(b)}_{i, j + 1} \gets C^{(b)}_{i, j + 1} / C_{ij}$
          \EndFor
          \State $\widehat{f}^{(b)}_j \gets \sum_{i = 1}^{I - j} C^{(b)}_{i, j + 1} / \sum_{i = 1}^{I - j} C_{ij}$
          \If{$j < I - 1$}
            \State $\widehat{\sigma}^{(b)}_j \gets \sqrt{\frac{1}{I - j - 1}\sum_{i = 1}^{I - j} C_{ij} \left( F^{(b)}_{i, j + 1} - \widehat{f}^{(b)}_j \right)^2}$
          \Else
            \State $\widehat{\sigma}^{(b)}_{I - 1} \gets \sqrt{\min{ \left \{ \frac{(\widehat{\sigma}^{(b)}_{I - 2})^4}{(\widehat{\sigma}^{(b)}_{I - 3})^2}, (\widehat{\sigma}^{(b)}_{I - 2})^2, (\widehat{\sigma}^{(b)}_{I - 3})^2 \right \} }}$
          \EndIf
        \EndFor
      \EndFor
      \State \Return $\{ (\widehat{\bm{f}}^{(b)}, \widehat{\bm{\sigma}}^{(b)}) \mid b = 1, \dots, B \}$
    \end{algorithmic}
  \end{algorithm}
  %
  \begin{algorithm}[H]
    \caption{Unconditional semiparametric bootstrap for \modelref{model:mack}}
    \label{alg:uncond-semiparam-mack}
    \begin{algorithmic}
      \Require{Cumulative claims triangle $\mathcal{D}_I$, number of bootstrap samples $B$}
      \State $(\{ r_{ij} \mid i + j \leq I + 1 \}, \bm{\widehat{f}}, \bm{\widehat{\sigma}}) \gets$ \Call{fit}{$\mathcal{D}_I$}
      \For{$b \gets 1, B$}
        \State $\{ r^{(b)}_{ij} \mid i + j \leq I + 1 \} \gets$ \Call{resample}{$\{ r_{ij} \mid i + j \leq I + 1 \}$}
        \For{$i \gets 1, I$}
          \State $C^{(b)}_{i1} \gets C_{i1}$
        \EndFor
        \For{$j \gets 1, I - 1$}
          \For{$i \gets 1, I - j$}
            \State $C^{(b)}_{i, j + 1} \gets \widehat{f}_j C^{(b)}_{ij} + \widehat{\sigma}_j \sqrt{C^{(b)}_{ij}} r^{(b)}_{i, j + 1}$
            \State $F^{(b)}_{i, j + 1} \gets C^{(b)}_{i, j + 1} / C^{(b)}_{ij}$
          \EndFor
          \State $\widehat{f}^{(b)}_j \gets \sum_{i = 1}^{I - j} C^{(b)}_{i, j + 1} / \sum_{i = 1}^{I - j} C^{(b)}_{ij}$
          \If{$j < I - 1$}
            \State $\widehat{\sigma}^{(b)}_j \gets \sqrt{\frac{1}{I - j - 1}\sum_{i = 1}^{I-j} C^{(b)}_{ij}\left( F^{(b)}_{i, j + 1} - \widehat{f}^{(b)}_j \right)^2}$
          \Else
            \State $\widehat{\sigma}^{(b)}_{I - 1} \gets \sqrt{\min{ \left \{ \frac{(\widehat{\sigma}^{(b)}_{I - 2})^4}{(\widehat{\sigma}^{(b)}_{I - 3})^2}, (\widehat{\sigma}^{(b)}_{I - 2})^2, (\widehat{\sigma}^{(b)}_{I - 3})^2 \right \} }}$
          \EndIf
        \EndFor
      \EndFor
      \State \Return $\{ (\widehat{\bm{f}}^{(b)}, \widehat{\bm{\sigma}}^{(b)}) \mid b = 1, \dots, B \}$
    \end{algorithmic}
  \end{algorithm}
\end{figure}

Next, we move on to the fully nonparametric bootstrap\index{bootstrap!nonparametric}, which is based on direct resampling of the observed predictor-response pairs. Once again viewing \modelref{model:mack} as a series of weighted linear regressions, the most obvious way of adapting this would be to resample the pairs ${\{ (C_{ij}, C_{i, j + 1}) \mid 1 \leq i \leq I - j \}}$ at every development index $j \in \{ 1, \dots, I - 1 \}$. In this case, however, the only resampling scheme which can be used is the conditional one. To see why, consider what an implementation of the unconditional approach would look like. Starting at $j = 1$, if the result of resampling the first two columns is given by
\begin{displaymath}
  \{ (C^*_{11}, C^*_{12}), \dots, (C^*_{I - 1, 1}, C^*_{I - 1, 2}) \} \,,
\end{displaymath}
then this would mean that we have to use $(C^*_{i2})_{1 \leq i \leq I - 1}$ as the regressors in the next step. However, because the pairs are the fundamental unit of resampling for this method, we have to ensure that every bootstrap replicate remain tied to the same response, effectively forcing us to permute entire rows $\{ (C_{ik})_{j \leq k \leq I} \mid 1 \leq i \leq I - j\}$ at the $j$th step of the process. But this creates a problem: the last point in the second column does not have a successor in the triangle, and we therefore become stuck in the next step if we had drawn it previously, as illustrated by \cref{fig:uncond-pairs-resample}.

\begin{figure}[!htb]
  \centering
  \begin{tikzpicture}
    \node (a) at (0, 0){
      \begin{tabular}{c c c c} \cline{1-2}
        \multicolumn{1}{|c}{$C_{11}$} & \multicolumn{1}{c|}{$C_{12}$} & $C_{13}$ & $C_{14}$ \\
        \multicolumn{1}{|c}{$C_{21}$} & \multicolumn{1}{c|}{$C_{22}$} & $C_{23}$ & $C_{24}$ \\
        \multicolumn{1}{|c}{$C_{31}$} & \multicolumn{1}{c|}{$C_{32}$} & $C_{33}$ &          \\
        \multicolumn{1}{|c}{$C_{41}$} & \multicolumn{1}{c|}{$C_{42}$} &                     \\ \cline{1-2}
        $C_{51}$                      &                               &
      \end{tabular}
    };
    \node (b) at (5, 0) {
      \begin{tabular}{c c c c} \cline{2-3}
        $C_{41}$ & \multicolumn{1}{|c}{$C_{42}$} & \multicolumn{1}{c|}{}         &          \\
        $C_{31}$ & \multicolumn{1}{|c}{$C_{32}$} & \multicolumn{1}{c|}{$C_{33}$} & $C_{34}$ \\
        $C_{11}$ & \multicolumn{1}{|c}{$C_{12}$} & \multicolumn{1}{c|}{$C_{13}$} & $C_{14}$ \\ \cline{2-3}
        $C_{31}$ & $C_{32}$                      & $C_{33}$                                 \\
        $C_{51}$ &
      \end{tabular}
    };
    \node (c) at (8, 0) {\large \bf{?}};
    \path[->, thick]
    (a.east) edge (b.west)
    (b.east) edge (c.west);
  \end{tikzpicture}
  \caption{Example of the failure of unconditional resampling for the pairs bootstrap}
  \label{fig:uncond-pairs-resample}
\end{figure}

Hence, our only option is to resample each pair of successive columns from the original data and use it to compute bootstrap replicates for the corresponding development factor and variance parameter estimators. This procedure is outlined in \cref{alg:pairs-mack}, and the results for the data in \cref{tab:uk-motor} are given in \cref{tab:mack-pairs}. Note that the `different methods in the simulation step' refer to the peculiarity of the pairs bootstrap discussed in \cref{sec:boot-proc}, which will be clarified further in \cref{sec:mack-proc}.

\begin{algorithm}[!htb]
  \caption{Pairs bootstrap for \modelref{model:mack}}
  \label{alg:pairs-mack}
  \begin{algorithmic}
    \Require{Cumulative claims triangle $\mathcal{D}_I$, number of bootstrap samples $B$}
    \For{$b \gets 1, B$}
      \For{$j \gets 1, I - 1$}
        \vspace{5pt}
        \State $\{ (C^{(b)}_{i, j}, C^{(b)}_{i, j + 1}) \mid i = 1, \dots, I - j \} \gets$ \Call{resample}{$\{ (C_{i, j}, C_{i, j + 1}) \mid i = 1, \dots, I - j \}$}
        \vspace{5pt}
        \State $\widehat{f}^{(b)}_j \gets \sum_{i = 1}^{I - j} C^{(b)}_{i, j + 1} / \sum_{i = 1}^{I - j} C^{(b)}_{ij}$
        \vspace{5pt}
        \For{$i \gets 1, I - j$}
          \State $F^{(b)}_{i, j + 1} \gets C^{(b)}_{i, j + 1} / C^{(b)}_{ij}$
          \vspace{5pt}
        \EndFor
        \If{$j < I - 1$}
          \State $\widehat{\sigma}^{(b)}_j \gets \sqrt{\frac{1}{I - j - 1}\sum_{i = 1}^{I-j} C^{(b)}_{ij}\left( F^{(b)}_{i, j + 1} - \widehat{f}^{(b)}_j \right)^2}$
        \Else
          \State $\widehat{\sigma}^{(b)}_{I - 1} \gets \sqrt{\min{ \left \{ \frac{(\widehat{\sigma}^{(b)}_{I - 2})^4}{(\widehat{\sigma}^{(b)}_{I - 3})^2}, (\widehat{\sigma}^{(b)}_{I - 2})^2, (\widehat{\sigma}^{(b)}_{I - 3})^2 \right \} }}$
        \EndIf
      \EndFor
    \EndFor
    \State \Return $\{ (\widehat{\bm{f}}^{(b)}, \widehat{\bm{\sigma}}^{(b)}) \mid b = 1, \dots, B \}$
  \end{algorithmic}
\end{algorithm}

\begin{table}[!htb]
  \begin{subtable}{0.5\linewidth}
    \input{../results/example/mack_pairs_semiparametric.tex}
    \subcaption{Semiparametric}
  \end{subtable}
  \hfill
  \begin{subtable}{0.5\linewidth}
    \input{../results/example/mack_pairs_parametric.tex}
    \subcaption{Parametric}
  \end{subtable}
  \caption{Results from the pairs bootstrap with different methods in the simulation step for the data in \cref{tab:uk-motor}}
  \label{tab:mack-pairs}
\end{table}

Finally, we consider the parametric bootstrap\index{bootstrap!parametric}, which relies on simulation from the underlying model to produce pseudo-observations. In light of \cref{eq:time-series-model}, we might be tempted to implement such a procedure by simply adapting \cref{alg:cond-semiparam-mack,alg:uncond-semiparam-mack} to use $\mathcal{N}(0, 1)$-distributed draws from a random number generator in place of the residuals. The drawback of this approach is that it does not generalise well to other distributions, because these rarely admit an additive decomposition into mean and error terms. It is therefore preferable to generate bootstrap pseudo-responses directly from the fitted distribution, i.e.\
\begin{equation}
  C^{(b)}_{i, j + 1} \sim \mathcal{N}(\widehat{f}_j C_{ij}, \widehat{\sigma}^2_j C_{ij}) \quad \text{or} \quad C^{(b)}_{i, j + 1} \sim \mathcal{N}(\widehat{f}_j C^{(b)}_{ij}, \widehat{\sigma}^2_j C^{(b)}_{ij}) \,,
\end{equation}
depending on whether the conditional or the unconditional approach is used. The model is then refitted to the bootstrapped triangle in order to obtain $\widehat{\bm{f}}^{(b)}$ and $\widehat{\bm{\sigma}}^{(b)}$. The algorithms and results are given in \cref{alg:cond-param-mack,alg:uncond-param-mack,tab:mack-param-normal,tab:mack-param-gamma}.

\begin{figure}[p]
  \begin{algorithm}[H]
    \caption{Conditional parametric bootstrap for \modelref{model:mack}}
    \label{alg:cond-param-mack}
    \begin{algorithmic}
      \Require{Cumulative claims triangle $\mathcal{D}_I$, number of bootstrap samples $B$}
      \State $(\bm{\widehat{f}}, \bm{\widehat{\sigma}}) \gets$ \Call{fit}{$\mathcal{D}_I$}
      \For{$b \gets 1, B$}
        \For{$j \gets 1, I - 1$}
          \For{$i \gets 1, I - j$}
            \State $C^{(b)}_{i, j + 1} \gets$ \Call{sample}{$ \, \mathcal{N}(\widehat{f}_j C_{ij}, \widehat{\sigma}^2_j C_{ij}) \, $}
            \State $F^{(b)}_{i, j + 1} \gets C^{(b)}_{i, j + 1} / C_{ij}$
          \EndFor
          \State $\widehat{f}^{(b)}_j \gets \sum_{i = 1}^{I - j} C^{(b)}_{i, j + 1} / \sum_{i = 1}^{I - j} C_{ij}$
          \If{$j < I - 1$}
            \State $\widehat{\sigma}^{(b)}_j \gets \sqrt{\frac{1}{I - j - 1}\sum_{i = 1}^{I - j} C_{ij} \left( F^{(b)}_{i, j + 1} - \widehat{f}^{(b)}_j \right)^2}$
          \Else
            \State $\widehat{\sigma}^{(b)}_{I - 1} \gets \sqrt{\min{ \left \{ \frac{(\widehat{\sigma}^{(b)}_{I - 2})^4}{(\widehat{\sigma}^{(b)}_{I - 3})^2}, (\widehat{\sigma}^{(b)}_{I - 2})^2, (\widehat{\sigma}^{(b)}_{I - 3})^2 \right \} }}$
          \EndIf
        \EndFor
      \EndFor
      \State \Return $\{ (\widehat{\bm{f}}^{(b)}, \widehat{\bm{\sigma}}^{(b)}) \mid b = 1, \dots, B \}$
    \end{algorithmic}
  \end{algorithm}
  %
  \begin{algorithm}[H]
    \caption{Unconditional parametric bootstrap for \modelref{model:mack}}
    \label{alg:uncond-param-mack}
    \begin{algorithmic}
      \Require{Cumulative claims triangle $\mathcal{D}_I$, number of bootstrap samples $B$}
      \State $(\bm{\widehat{f}}, \bm{\widehat{\sigma}}) \gets$ \Call{fit}{$\mathcal{D}_I$}
      \For{$b \gets 1, B$}
        \For{$i \gets 1, I$}
          \State $C^{(b)}_{i1} \gets C_{i1}$
        \EndFor
        \For{$j \gets 1, I - 1$}
          \For{$i \gets 1, I - j$}
            \State $C^{(b)}_{i, j + 1} \gets$ \Call{sample}{$ \, \mathcal{N}(\widehat{f}_j C^{(b)}_{ij}, \widehat{\sigma}^2_j C^{(b)}_{ij}) \, $}
            \State $F^{(b)}_{i, j + 1} \gets C^{(b)}_{i, j + 1} / C^{(b)}_{ij}$
          \EndFor
          \State $\widehat{f}^{(b)}_j \gets \sum_{i = 1}^{I - j} C^{(b)}_{i, j + 1} / \sum_{i = 1}^{I - j} C^{(b)}_{ij}$
          \If{$j < I - 1$}
            \State $\widehat{\sigma}^{(b)}_j \gets \sqrt{\frac{1}{I - j - 1}\sum_{i = 1}^{I - j} C^{(b)}_{ij} \left( F^{(b)}_{i, j + 1} - \widehat{f}^{(b)}_j \right)^2}$
          \Else
            \State $\widehat{\sigma}^{(b)}_{I - 1} \gets \sqrt{\min{ \left \{\frac{(\widehat{\sigma}^{(b)}_{I - 2})^4}{(\widehat{\sigma}^{(b)}_{I - 3})^2}, (\widehat{\sigma}^{(b)}_{I - 2})^2, (\widehat{\sigma}^{(b)}_{I - 3})^2 \right \} }}$
          \EndIf
        \EndFor
      \EndFor
      \State \Return $\{ (\widehat{\bm{f}}^{(b)}, \widehat{\bm{\sigma}}^{(b)}) \mid b = 1, \dots, B \}$
    \end{algorithmic}
  \end{algorithm}
\end{figure}

\begin{table}[!htb]
  \begin{subtable}{0.5\linewidth}
    \input{../results/example/mack_param_cond_normal.tex}
    \subcaption{Conditional}
  \end{subtable}
  \hfill
  \begin{subtable}{0.5\linewidth}
    \input{../results/example/mack_param_uncond_normal.tex}
    \subcaption{Unconditional}
  \end{subtable}
  \caption{Results from the parametric bootstrap with normal distribution for the data in \cref{tab:uk-motor}}
  \label{tab:mack-param-normal}
\end{table}

\begin{table}[!htb]
  \begin{subtable}{0.5\linewidth}
    \input{../results/example/mack_param_cond_gamma.tex}
    \subcaption{Conditional}
  \end{subtable}
  \hfill
  \begin{subtable}{0.5\linewidth}
    \input{../results/example/mack_param_uncond_gamma.tex}
    \subcaption{Unconditional}
  \end{subtable}
  \caption{Results from the parametric bootstrap with gamma distribution for the data in \cref{tab:uk-motor}}
  \label{tab:mack-param-gamma}
\end{table}

\section{Incorporating the process error} \label{sec:mack-proc}

We end this chapter by discussing how the process error\index{process error} can be incorporated into the bootstrap procedures from the previous section. Recall that our aim is ultimately to obtain a predictive distribution\index{predictive distribution} for the reserve incorporating both parameter and process error, which we can then use to study the impact of violations of the Mack chain ladder assumptions. Following the procedure outlined in \cref{sec:boot-proc}, this means that we have to generate pseudo-realisations of the reserve which fluctuate around point estimates obtained using bootstrapped development factors and variance parameters. In light of \cref{eq:reserve-predictor}, the latter are given by
\begin{equation}
  R^{(b)} \coloneqq \sum_{i = 2}^I (C^{(b)}_{iI} - C_{i, I + 1 - i})
\end{equation}
with $\widehat{C}^{(b)}_{iI} = C_{i, I + 1 - i} \prod_{j = I + 1 - i}^{I - 1} \widehat{f}^{(b)}_j$. Bootstrap methods are always derived from the underlying data-generating process\index{data-generating process}; it therefore follows that we must generate pseudo-realisations of the future claim amounts in the lower triangle $\mathcal{D}^{\mathsf{c}}_I = \{ C_{ij} \mid i + j > I + 1 \}$ in order to simulate the process error of the reserve.

The procedures from the previous section can be easily adapted to achieve this. For the semiparametric bootstrap, we resample the residuals a second time and again use \cref{eq:time-series-model}, substitute the bootstrap replicates of the parameter estimators this time, i.e.\
\begin{equation}
  C^{(s)}_{i, j + 1} = \widehat{f}^{(b)}_j C_{ij} + \widehat{\sigma}^{(b)}_j \sqrt{C_{ij}} r^{(s)}_{i, j + 1} \quad \text{or} \quad C^{(s)}_{i, j + 1} = \widehat{f}^{(b)}_j C^{(s)}_{ij} + \widehat{\sigma}^{(b)}_j \sqrt{C^{(s)}_{ij}} r^{(s)}_{i, j + 1} \,,
\end{equation}
depending on whether the conditional or unconditional approach is used. A parametric version is obtained simulating from the distribution obtained by plugging in the $\widehat{f}^{(b)}_j$ and $\widehat{\sigma}^{(b)}_j$, and the pairs bootstrap must borrow from one of them, as described in \cref{sec:boot-proc}.

The same problems encountered in \cref{sec:mack-boot} regarding the possibility of negative draws crop up here as well. For the semiparametric bootstrap, they can be solved in the same way using the shifted log-normal residuals from \cref{eq:log-normal-resids}, which by construction cannot lead to negative pseudo-responses.  For the parametric bootstrap, we follow the previously mentioned suggestion in \cite{england:dist} and simulate future claim amounts from a gamma distribution. In the conditional approach, for example, if $C^{(s)}_{ij} \sim \Gamma(\alpha^{(b)}_{ij}, \beta^{(b)}_{ij})$, then we need to have
\begin{equation}
  \condexpp{C_{ij}}{C_{i, j - 1}} = \frac{\alpha^{(b)}_{ij}}{\beta^{(b)}_{ij}} = f^{(b)}_{j-1} C_{i, j-1} \quad \text{and} \quad \condvarr{C_{ij}}{C_{i, j - 1}} = \frac{\alpha^{(b)}_{ij}}{(\beta^{(b)}_{ij})^2} = (\sigma^{(b)}_{j-1})^2 C_{i, j-1} \,,
\end{equation}
and must therefore choose parameters
\begin{equation} \label{eq:gamma-sim}
  \alpha^{(b)}_{ij} = \frac{(f^{(b)}_{j-1})^2 C_{i, j-1}}{(\sigma^{(b)}_{j-1})^2} \quad \text{and} \quad \beta^{(b)}_{ij} = \frac{f^{(b)}_{j-1}}{(\sigma^{(b)}_{j-1})^2} \,.
\end{equation}
The solution adopted for the pairs bootstrap will depend on which of the two other variants is used in the simulation step. We will use the parametric approach for the simulations in \cref{chapter:sim}.

In \cref{sec:boot-proc}, we gave not much difference could be discerned between the fitted and predictive distributions. This stands in stark contrast to \cref{fig:fit-pred-pairs,fig:fit-pred-param,fig:fit-pred-semiparam-standard,fig:fit-pred-semiparam-student-log-normal}, which compare these for the reserve under different bootstrap configurations. We can clearly see that fitted distribution severely underestimates the uncertainty of the reserve in all cases.

\begin{figure}[!p]
  \begin{subfigure}{0.45 \textwidth}
    \includegraphics{mack_pairs_parametric}
    \subcaption{Parametric}
  \end{subfigure}
  \begin{subfigure}{0.45 \textwidth}
    \includegraphics{mack_pairs_semiparametric}
    \subcaption{Semiparametric}
  \end{subfigure}
  \caption{Comparison of the fitted and predictive distribution for the pairs bootstrap with different simulation methods}
  \label{fig:fit-pred-pairs}
\end{figure}

\begin{figure}[!p]
  \begin{subfigure}{0.45 \textwidth}
    \includegraphics{mack_semiparam_cond_standardised}
    \subcaption{Conditional}
  \end{subfigure}
  \begin{subfigure}{0.45 \textwidth}
    \includegraphics{mack_semiparam_uncond_standardised}
    \subcaption{Unconditional}
  \end{subfigure}
  \caption{Comparison of the fitted and predictive distribution for the semiparametric bootstrap with standardised residuals}
  \label{fig:fit-pred-semiparam-standard}
\end{figure}

\begin{landscape}
  \begin{figure}
    \centering
    \begin{subfigure}{0.45\linewidth}
      \includegraphics{mack_semiparam_cond_studentised}
      \subcaption{Conditional, studentised residuals}
    \end{subfigure}
    \begin{subfigure}{0.45\linewidth}
      \includegraphics{mack_semiparam_uncond_studentised}
      \subcaption{Unconditional, studentised residuals}
    \end{subfigure}
    \begin{subfigure}{0.45\linewidth}
      \includegraphics{mack_semiparam_cond_log-normal}
      \subcaption{Conditional, log-normal residuals}
    \end{subfigure}
    \begin{subfigure}{0.45\linewidth}
      \includegraphics{mack_semiparam_uncond_log-normal}
      \subcaption{Unconditional, log-normal residuals}
    \end{subfigure}
    \caption{Comparison of the fitted and predictive distribution for the semiparametric bootstrap}
    \label{fig:fit-pred-semiparam-student-log-normal}
  \end{figure}
\end{landscape}

\begin{landscape}
  \begin{figure}
    \centering
    \begin{subfigure}{0.45\linewidth}
      \includegraphics{mack_param_cond_normal}
      \subcaption{Conditional, normal distribution}
    \end{subfigure}
    \begin{subfigure}{0.45\linewidth}
      \includegraphics{mack_param_uncond_normal}
      \subcaption{Unconditional, normal distribution}
    \end{subfigure}
    \begin{subfigure}{0.45\linewidth}
      \includegraphics{mack_param_cond_gamma}
      \subcaption{Conditional, gamma distribution}
    \end{subfigure}
    \begin{subfigure}{0.45\linewidth}
      \includegraphics{mack_param_uncond_gamma}
      \subcaption{Unconditional, gamma distribution}
    \end{subfigure}
    \caption{Comparison of the fitted and predictive distribution for the parametric bootstrap}
    \label{fig:fit-pred-param}
  \end{figure}
\end{landscape}

\chapter{Overdispersed Poisson GLM} \label{chapter:poisson}

\section{Introduction} \label{sec:odp-intro}

The overdispersed Poisson model\index{overdispersed Poisson model} (ODP), proposed by Renshaw and Verrall in \cite{renshaw}, belongs to the family of so-called \emph{generalised linear models}\index{generalised linear model} (GLM). In contrast to \modelref{model:mack}, it describes the incremental claims\index{claims triangle!incremental} $X_{ij}$, and is non-recursive: the diffferent observations in the triangle are modelled independently from each other. To ease the exposition, we will only state the assumptions for the ordinary Poisson model (i.e.\ without overdispersion) at this point; after this concept has been introduced in \cref{subsec:quasi}, we will then adjust the model accordingly.

\begin{model}[Poisson GLM] \leavevmode \label{model:poisson}
  \begin{enumerate}[label=\bf{\textup{(}Pois\arabic*\textup{)}},ref=\textup{(}Pois\arabic*\textup{)}, wide]
    \item \label{assump:poisson1}
          The incremental claims are independent from each other.
    \item \label{assump:poisson2}
          There exist parameters $c$, $a_1, \dots, a_I$ and $b_1, \dots, b_I$ such that
          \begin{equation} \label{eq:linear-predictor-poisson}
            \log(\expect{X_{ij}}) = c + a_i + b_j \,,
          \end{equation}
          with $a_1 = b_1 = 0$.
    \item \label{assump:poisson3}
          The incremental claims follow a Poisson distribution:
          \begin{equation}
            X_{ij} \sim \mathrm{Pois}(e^{c + a_i + b_j}) \,.
          \end{equation}
  \end{enumerate}
\end{model}

The condition $a_1 = b_1 = 0$ is necessary to obtain an identifiable model; without it, we could derive, from any set of parameters $c, a_1, \dots, a_I, b_1, \dots, b_I$ satisfying the assumptions, an infinite number of alternatives $c + a' + b', a_1 - a', \dots, a_I - a', b_1 - b', \dots, b_I - b'$ for $a', b' \in \mathbb{R}$. We therefore have two superfluous degrees of freedom, which can be eliminated by imposing the same number of conditions on the parameters.

By defining $\xi_i \coloneqq e^{c + a_i}$ and $\gamma_j \coloneqq e^{b_j}$, we get a different parametrisation of the model which has a multiplicative structure for the mean, i.e.\
\begin{equation}
  \expect{X_{ij}} = \xi_i \gamma_j \,.
\end{equation}
This is sometimes preferred to the additive one for reasons of interpretability. Indeed, it is clear that the multiplicative form has one fewer degree of freedom than the linear one, and if we remove this by imposing the constraint
\begin{equation}
  \sum_{j = 1}^I \gamma_j = 1
\end{equation}
then we can view the $\xi_i$ as expected ultimate claim amounts, and the $\gamma_j$ as the expected development pattern.

As mentioned in the introduction, stochastic claims reserving models have to reproduce the chain ladder point predictions in order to be acceptable to practitioners. While less obvious than for \modelref{model:mack}, it can be shown that \modelref{model:poisson} also satisfies this requirement (see \cite[Lemma 2.16]{wuthrich:stochastic-reserving}).

\section{Generalised linear models} \label{sec:glm}

GLMs\index{generalised linear model} were first conceived by Nelder and Wedderburn in \cite{nelder} as a way of unifying the many disparate generalisations of linear regression with Gaussian errors which were then in existence. These sought to extend the classical model by allowing the use of different functional forms for the conditional mean and different distributions for the response, thus making it suited to modelling counts data (Poisson regression) or the probability of binary events (logistic regression), among others. For a set of covariates $X_1, \dots, X_p$ and a response variable $Y$, a GLM consists of three parts:
\begin{enumerate}
  \item The \emph{random component}, a distribution for response $Y$ belonging to the so-called \emph{exponential dispersion model} family\index{exponential dispersion model family} (EDM), which consists of all probability distributions whose density (with respect either to the Lebesgue or counting measure) has the form
        \begin{equation} \label{eq:exp-disp-fam}
          p(y \mid \theta, \phi) = \exp \left \{ \frac{y \theta - b(\theta)}{a(\phi)} + c(y, \phi) \right \} \,,
        \end{equation}
        where $a$, $b$ and $c$ are known functions, and $b$ is at least twice differentiable. We call $\theta$ the \emph{canonical parameter} of the distribution and $\phi$ the \emph{dispersion parameter}\index{dispersion parameter}.
  \item The \emph{systematic component}, a predictor $\eta \coloneqq \bm{\mathrm{x}}^T \bm{\beta}$ which is a linear function of the covariates.
  \item A monotonic differentiable link function $g: \mathbb{R} \rightarrow \mathbb{R}$ giving the relation between the conditional expectation and the linear predictor,
        \begin{equation}
          \mu \coloneqq \condexpp{Y}{X_1, \dots, X_p} = g^{-1}(\eta) \,.
        \end{equation}
\end{enumerate}
The Gaussian distribution $\mathcal{N}(\mu, \sigma^2)$ can be seen to belong to the EDM family by rewriting its density as
\begin{align}
  \frac{1}{\sqrt{2 \pi} \sigma} \exp \left \{ -\frac{1}{2} \left( \frac{y - \mu}{\sigma} \right)^2\right \} & = \exp \left \{ -\frac{y^2}{2 \sigma^2} + \frac{y \mu}{\sigma^2} - \frac{\mu^2}{2 \sigma^2} - \log(\sqrt{2 \pi} \sigma) \right \} \\
                                                                                                            & = \exp \left \{ \frac{y \mu - \mu^2 / 2}{\sigma^2} - \frac{y^2}{2 \sigma^2} - \log(\sqrt{2 \pi} \sigma) \right \} \,,
\end{align}
which is of the form \cref{eq:exp-disp-fam} with $\theta = \mu$, $b(\theta) = \frac{\theta^2}{2}$, $\phi = \sigma^2$, $a(\phi) = \phi$ and $c(y, \sigma) = -\frac{y^2}{2 \sigma^2} - \log(\sqrt{2 \pi} \sigma)$. Thus, the familiar normal linear model can be obtained from the GLM framework by choosing the response distribution $\mathcal{N}(\mu, \sigma^2)$ and link function $g(\mu) = \mu$.

\subsection{Estimation}

The EDM family has a number of properties which greatly facilitate the computations involved in estimation. Recall from likelihood theory that $l(\theta \mid y, \phi) \coloneqq \log p(y \mid \theta, \phi)$ satisfies
\begin{equation} \label{eq:exp-var-likelihood}
  \expect{\frac{\partial l(\theta \mid Y)}{\partial \theta}} = 0 \,, \qquad \var{\frac{\partial l(\theta \mid Y)}{\partial \theta}} = -\expect{\frac{\partial^2 l(\theta \mid Y)}{\theta^2}} \,,
\end{equation}
where $\frac{\partial l(\theta \mid Y)}{\partial \theta}$ is known as the \emph{score function}. Using \cref{eq:exp-disp-fam}, we then find that
\begin{equation}
  \expect{\frac{Y - b'(\theta)}{a(\phi)}} = 0 \,, \qquad \var{\frac{Y - b'(\theta)}{a(\phi)}} = -\expect{\frac{-b''(\theta)}{a(\phi)}} \,,
\end{equation}
from which we obtain the elegant relations
\begin{equation}
  \mu = b'(\theta) \,, \qquad \var{Y} = a(\phi) b''(\theta) \,.
\end{equation}
Observe that this implies that $\frac{d \mu}{d \theta} = b''(\theta) > 0$ (because the variance is always positive), which means that $\theta \mapsto \mu(\theta)$ is one-to-one and therefore invertible. In particular, we can always write the likelihood as function of the mean. The function $V(\mu) \coloneqq b''((b')^{-1}(\mu))$ is called the \emph{variance function}\index{variance function} and determines how the scale of the response varies as a function of its mean.

Special care has to be taken with the parameter $\phi$, as it occupies a rather awkward position in GLM theory.

The problem arises from the desire of the progenitors of the GLM framework to incorporate two-parameter distributions such as the normal and gamma distribution into a paradigm which can fundamentally only handle a single parameter gracefully (the more flexible approach of \emph{vector GLMs} is an attempt to remedy this; see \cite[Chapter 2]{yee} for a general discussion). The dispersion is therefore regelated to the role of nuisance parameter and subjected to severe (and often unrealistic) constraints. In essence, one would like $\phi$ to be constant as a function of the covariates, but this precludes certain special cases such as binomial regression with a different number of trials for each observation in the sample which practitioners wanted to accommodate. In the end, the compromise which seems to have gained the widest acceptance in the literature is to allow the function $a$ in the denominator of \cref{eq:exp-disp-fam} to vary across different sample responses as $a_i(\phi) = \phi / w_i$, where $w_i$ is a known weight. The parameter $\phi$ itself is then considered as known, and estimated outside of the GLM framework, most commonly using the Pearson statistic
\begin{equation}
  \widehat{\phi} \coloneqq \frac{1}{n - p} \sum_{i = 1}^N \frac{(Y_i - \widehat{\mu_i})^2}{V(\widehat{\mu_i})} \,.
\end{equation}

Given a sample $(\bm{\mathrm{x_1}}, Y_1), \dots, (\bm{\mathrm{x_N}}, Y_N)$, the standard way to fit a GLM is by means of maximum likelihood estimation\index{maximum likelihood estimation} (MLE). The joint log-likelihood of the sample is given by
\begin{equation} \label{eq:joint-log-likelihood}
  l(\bm{\beta} \mid \mathbf{y}, \phi) = \sum_{i = 1}^N \frac{y_i \theta_i - b(\theta_i)}{a_i(\phi)} + c(y_i, \phi) \,,
\end{equation}
which we must differentiate with respect to $\beta_j$ to obtain the likelihood equations. An application of the chain rule gives us
\begin{align}
  \frac{\partial l(\bm{\beta} \mid \mathbf{y}, \phi)}{\partial \beta_j} & = \sum_{i = 1}^N \frac{\partial l(\bm{\beta} \mid y_i, \phi)}{\partial \theta_i} \frac{\partial \theta_i}{\partial \mu_i} \pdv{\mu_i}{\eta_i} \frac{\partial \eta_i}{\partial \beta_j} \\
                                                                        & = \sum_{i = 1}^N \frac{y_i - b'_i(\theta)}{a_i(\phi)} \frac{1}{b_i''(\theta_i)} \pdv{\mu_i}{\eta_i} x_{ij}                                                                             \\
                                                                        & = \sum_{i = 1}^N \frac{y_i - \mu_i}{\var{Y_i}} \pdv{\mu_i}{\eta_i} x_{ij} \,, \label{eq:grad-likelihood}
\end{align}
and setting this equal to $0$ yields a system of $p$ (usually nonlinear) equations. It is generally impossible to solve these analytically, and so we must resort to numerical methods. In particular, we use a modified version of the Newton-Raphson algorithm known as \emph{Fisher scoring}\index{Fisher scoring}, which replaces the negative Hessian of the log-likelihood, called the \emph{observed information}, by its expectation
\begin{equation}
  \mathcal{I}_{jk} \coloneqq \expect{-\frac{\partial^2 l(\bm{\beta} \mid \mathbf{y}, \phi)}{\partial \beta_j \partial \beta_k}} \,,
\end{equation}
which is known as the \emph{Fisher information matrix}\index{Fisher information}. Thus, starting from an initial guess $\bm{\widehat{\beta}}^{(0)}$ for the parameters, we compute a successive approximations via
\begin{equation} \label{eq:fisher-scoring}
  \bm{\widehat{\beta}}^{(k + 1)} = \bm{\widehat{\beta}}^{(k)} + \mathcal{I}(\bm{\widehat{\beta}}^{(k)})^{-1} \nabla l(\bm{\widehat{\beta}}^{(k)} \mid \mathbf{y}, \phi) \,.
\end{equation}
Similarly to \cref{eq:exp-var-likelihood}, it can be shown that
\begin{equation}
  \expect{\pdv{l(\bm{\beta} \mid \mathbf{y}, \phi)}{\bm{\beta}, \bm{\beta}^T}} = -\var{\nabla l(\bm{\beta} \mid \mathbf{y}, \phi) \nabla l(\bm{\beta} \mid \mathbf{y}, \phi)^T} < 0 \,,
\end{equation}
from which we also see that the log-likelihood is concave, and will therefore have a global maximum. Using the fact that the $Y_i$ are independent, so that $\expect{(Y_i - \mu_i)(Y_l - \mu_l)} = 0$ for $i \neq l$, we then obtain
\begin{align}
  I_{jk} & = \expect{\left( \sum_{i = 1}^N \frac{Y_i - \mu_i}{\var{Y_i}} \pdv{\mu_i}{\eta_i} x_{ij}\right) \left( \sum_{l = 1}^N \frac{Y_l - \mu_l}{\var{Y_l}} \frac{\partial \mu_l}{\partial \eta_l} x_{lk} \right)} \\
         & = \sum_{i = 1}^N \left( \pdv{\mu_i}{\eta_i} \right)^2 \frac{\expect{(Y_i - \mu_i)^2}}{\var{Y_i}^2} x_{ij} x_{ik}                                                                                           \\
         & = \sum_{i = 1}^N \left( \pdv{\mu_i}{\eta_i} \right)^2 \frac{x_{ij} x_{ik}}{\var{Y_i}}                                                                                                                      \\
         & = \bm{\mathrm{x}_j}^T \mathbf{W} \bm{\mathrm{x}_k} \label{eq:information-comps}
\end{align}
where $\mathbf{W}^{(k)}$ is a diagonal matrix with
\begin{equation} \label{eq:irwls-weight-mat}
  \mathbf{W}^{(k)}_{ii} = \frac{1}{\var{Y_i}} \pdv{\mu_i}{\eta_i}_{\bm{\widehat{\beta}}^{(k)}}^2 \,.
\end{equation}
Hence we have $\mathcal{I}(\bm{\widehat{\beta}}^{(k)}) = \mathbf{X}^T \mathbf{W}^{(k)} \mathbf{X}$ and we see from \cref{eq:grad-likelihood} that
\begin{equation} \label{eq:irwls-grad}
  \nabla l(\bm{\beta} \mid \mathbf{y}, \phi) = \mathbf{X}^T \mathbf{W} \mathbf{\tilde{z}}
\end{equation}
with $\mathbf{\tilde{z}}_i = (y_i - \mu_i) \left( \pdv{\eta_i}{\mu_i} \right)$. Multiplying both sides of \ref{eq:fisher-scoring} by $\mathcal{I}(\bm{\beta}^{(k)})$ and using \cref{eq:irwls-weight-mat,eq:irwls-grad,eq:information-comps}, we finally obtain
\begin{align} \label{eq:irwls-iter}
  \mathbf{X}^T \mathbf{W}^{(k)} \mathbf{X} \bm{\widehat{\beta}}^{(k + 1)} =  \mathbf{X}^T \mathbf{W}^{(k)} \mathbf{z}
\end{align}
with $\mathbf{z} = \mathbf{X} \bm{\widehat{\beta}}^{(k)} + \mathbf{\tilde{z}}$ and all quantities evaluated at the current estimate $\bm{\beta}^{(k)}$ of the parameter vector. In other words, the Fisher scoring is equivalent to a series of weighted least squares\index{weighted least squares} problems, where the new parameter estimates are obtained by regressing the vector $\mathbf{z}$ on the original covariates $\bm{\mathrm{x}_1}, \dots, \bm{\mathrm{x}_N}$ using weight matrix $\mathbf{W}$, and $\mathbf{z}$ and $\mathbf{W}$ are determined by the current estimate $\bm{\beta}^{(k)}$---hence why the algorithm is called \emph{iteratively reweighted least squares}\index{iteratively reweighted least squares} (IRWLS).

This procedure can be specialised to the particular case of \modelref{model:poisson} in the following way. First, in order to obtain the matrix-vector form used above, we must flatten the tabular response (using, for example, the colexicographical ordering $(i, j) \mapsto j I + i$, i.e.\ column-major order). This gives us a triangle in long format, as illustrated in \cref{tab:uk-motor-long} for the incremental form of the UK Motor dataset from \textcite{christofides}.
\begin{table}[!htb]
  \centering
  \begin{tabularx}{0.3 \linewidth}{CCC}\toprule%
    \bf{Origin} & \bf{Dev} & \bf{Value} \\ \midrule
    \csvreader[
    late after line =                   \\
    ]{%
      ../results/example/UKMotor_long.csv
    }{origin=\Origin, dev=\Dev, value=\Value}{%
    \Origin     & \Dev     & \Value
    }\bottomrule%
  \end{tabularx}
  \caption{UK Motor incremental claims triangle in long format}
  \label{tab:uk-motor-long}
\end{table}
If we define the parameter vector
\begin{equation}
  \bm{\beta} \coloneqq \begin{bmatrix} c & a_2 & \cdots & a_I & b_2 & \cdots & b_I \end{bmatrix}^T \,,
\end{equation}
then \cref{eq:linear-predictor-poisson} can be rewritten as
\begin{equation} \label{eq:poisson-covariates}
  \log(\mu_{ij}) = c + a_i + b_j = (\bm{\mathrm{e}_1} + \bm{\mathrm{e}_i} + \bm{\mathrm{e}_{I + j - 1}})^T \bm{\beta}
\end{equation}
where $\bm{\mathrm{e}_k}$ denotes the $k$th standard basis vector in $\mathbb{R}^{(2I - 1)}$. Hence we see that the covariates are binary vectors of length $2I - 1$, with the position of the nonzero entries determined by the indices of the observation in the triangle, forming the rows of a very sparse design matrix. As \modelref{model:poisson} uses the log link, we have $\mu_{ij} = e^{\eta_{ij}}$ and
\begin{equation}
  \frac{\partial \mu_{ij}}{\partial \eta_{ij}} = e^{\eta_{ij}} \,,
\end{equation}
from which, using \cref{eq:irwls-weight-mat}, we finally obtain
\begin{equation}
  \mathbf{W}_{ii} = \frac{1}{e^{\eta_{ij}}} (e^{\eta_{ij}})^2 = e^{\eta_{ij}} \,,
\end{equation}
giving us all the components of the IRWLS algorithm. \cref{fig:odp-mat-eq} shows in matrix form.

\begin{figure}
  \centering
  \large
  \begin{displaymath}
    \begin{split}
      &\underbrace{
        \begin{bmatrix}
          1      & 0      & 0      & \dots & 0      \\
          1      & 1      & 0      & \dots & 0      \\
          \vdots & \vdots & \vdots &       & \vdots \\
          1      & 0      & 0      & \dots & 1      \\
        \end{bmatrix}
      }_{\mathbf{X}^T}
      \underbrace{
        \begin{bmatrix}
          e^{\widehat{c}^{(k)}} &                                             &        &                                                                     \\
                                & e^{\widehat{c}^{(k)} + \widehat{a}^{(k)}_1} &        &                                                                   & \\
                                &                                             & \ddots &                                                                     \\
                                &                                             &        & e^{\widehat{c}^{(k)} + \widehat{a}^{(k)}_I + \widehat{b}^{(k)}_I}   \\
        \end{bmatrix}
      }_{\mathbf{W}^{(k)}}
      \underbrace{
        \begin{bmatrix}
          1      & 1      & 1      & \dots & 1      \\
          0      & 1      & 0      & \dots & 0      \\
          \vdots & \vdots & \vdots &       & \vdots \\
          0      & 0      & 0      & \dots & 1      \\
        \end{bmatrix}
      }_{\mathbf{X}}
      \underbrace{
        \begin{bmatrix}
          \widehat{c}^{(k)} \\ \widehat{a}^{(k)}_2 \\ \vdots \\ \widehat{a}^{(k)}_I \\ \widehat{b}^{(k)}_2 \\ \vdots \\ \widehat{b}^{(k)}_I
        \end{bmatrix}
      }_{\bm{\widehat{\beta}}^{(k)}} \\
      &=%
      \underbrace{
        \begin{bmatrix}
          1      & 0      & 0      & \dots & 0      \\
          1      & 1      & 0      & \dots & 0      \\
          \vdots & \vdots & \vdots &       & \vdots \\
          1      & 0      & 0      & \dots & 1      \\
        \end{bmatrix}
      }_{\mathbf{X}^T}
      \underbrace{
        \begin{bmatrix}
          e^{\widehat{c}^{(k)}} &                                             &        &                                                                     \\
                                & e^{\widehat{c}^{(k)} + \widehat{a}^{(k)}_1} &        &                                                                   & \\
                                &                                             & \ddots &                                                                     \\
                                &                                             &        & e^{\widehat{c}^{(k)} + \widehat{a}^{(k)}_I + \widehat{b}^{(k)}_I}   \\
        \end{bmatrix}
      }_{\mathbf{W}^{(k)}}
      \mathbf{z}
    \end{split}
  \end{displaymath}
  \caption{IRWLS equation for Poisson GLM in matrix form}
  \label{fig:odp-mat-eq}
\end{figure}

\subsection{Quasi-likelihood methods} \label{subsec:quasi}

We have assumed up to this point that a GLM requires us to specify an exact distribution for the response variable. In many practical situations, however, this is either infeasible or leads to unrealistic models. An example which is particularly common with count data is a phenomenon known as \emph{overdispersion}\index{overdispersion}, where the variability of the data is greater than would be suggested by e.g.\ the Poisson or binomial distribution. Recall that the variance of a $\mathrm{Pois}(\lambda)$ distribution is $\lambda$, and that of a $B(n, p)$ distribution is $np(1 - p)$; in both cases, it is fully determined by the mean, and we have no degree of freedom with which to adjust it in order to obtain a better fit to the data, as would be the case with the normal distribution, for example.

To remedy this, an extension can be made to the GLM framework, which only relies on the specification of a relation between mean and variance. Recall from above that the MLE works by setting the score equal to $0$. If we write the likelihood in terms of $\bm{\mu}$, this will have components
\begin{equation} \label{eq:lhood-mean-param}
  \frac{\partial l(\bm{\mu} \mid \mathbf{y}, \phi)}{\partial \mu_j} = \frac{y_j - \mu_j}{\phi V(\mu_j)} \,,
\end{equation}
and is therefore completely determined by $V(\cdot)$. Suppose now, conversely, that we start from $V(\cdot)$. We could then define functions
\begin{equation}
  Q_i(\mu \mid y_i, \phi) \coloneqq \int_{y_i}^{\mu} \frac{y_i - u}{\phi V(u)} \, du \,,
\end{equation}
and estimate $\bm{\mu}$ (and therefore $\bm{\beta}$) by minimising
\begin{equation}
  Q(\bm{\mu} \mid \mathbf{y}, \phi) \coloneqq \sum_{i = 1}^N Q_i(\mu_i \mid y_i, \phi) \,.
\end{equation}
It must be stressed that $Q$ has no probabilistic significance: it does not, in general, correspond to the log-likelihood of any distribution. Rather, it functions a device to obtain estimates of the desired parameters, fulfilling in this a similar role to that of the log-likelihood, which is why we refer to it as a \emph{quasi-likelihood function}\sepfootnote{fn:quasi-likelihood}\index{quasi-likelihood}. This extension of the classical GLM framework allows for more flexibility in our modelling, since we only have to assume a particular mean-variance relation for the response, not a distribution. This finally allows us to state the main model of this chapter.

\begin{model}[overdispersed Poisson GLM] \leavevmode \label{model:odp}
  \begin{enumerate}[label=\bf{\textup{(}ODP\arabic*\textup{)}},ref=\textup{(}ODP\arabic*\textup{)}, wide]
    \item The incremental claims are independent from each other.
    \item There exist parameters $c$, $a_1, \dots, a_I$ and $b_1, \dots, b_I$ such that
          \begin{equation}
            \log(\mu_{ij}) = c + a_i + b_j \,,
          \end{equation}
          with $\mu_{ij} \coloneqq \expect{X_{ij}}$ and $a_1 = b_1 = 0$.
    \item There exists a parameter $\phi$ such that
          \begin{equation}
            \var{X_{ij}} = \phi \mu_{ij} \,.
          \end{equation}
  \end{enumerate}
\end{model}

It is usual to identify quasi-likelihood models derived from specific distributions (i.e.\ using the same variance function) by prefixing 'quasi' to their name, e.g.\ quasi-Poisson or quasi-binomial. The quasi-model will yield the same point estimates for the parameters as the ordinary variant, provided the observations belong to the support of the latter. For data consisting entirely of positive integers (like the triangle from \cref{tab:uk-motor}), it thus follows from the remark at the end of \cref{sec:odp-intro} that \modelref{model:odp} yields the same predictions as the chain ladder, although it differs significantly when it comes to the error/variance, as can be seen from \cref{tab:odp-bench}

More generally, it can be shown (see \cite[Section 2]{renshaw}) that the chain ladder results will be reproduced as long as the additional condition
\begin{equation}
  \sum_{i = 1}^I X_{ij} \geq 0
\end{equation}
is satisfied for $j \in \{ 1, \dots, I \}$. \Modelref{model:odp} is therefore robust to the presence of a limited number of negative claim amounts, which is sometimes observed in practice. Moreover, it lifts the unrealistic restriction that the response values must be integers, and gives us a way of accounting for overdispersion, which is a feature of many run-off triangles.

\begin{table}[!htb]
  \centering
  \begin{tabularx}{0.7\linewidth}{CCC \mcol{4em} CC}\toprule
    \multirow{2}{*}{$i/j$} & \multirow{2}{*}{$\widehat{a}_i$} & \multirow{2}{*}{$\widehat{b}_j$} & \multirow{2}{*}{$\widehat{R}_j$} & \multicolumn{2}{c}{\resizebox{4em}{!}{$\widehat{\mathrm{MSEP}}(\widehat{R}_j)$}}                   \\ \cmidrule{5-6}
                           &                                  &                                  &                                  & Poisson                                                                          & ODP             \\ \midrule
    \csvreader[
      head to column names,
    late after line =                                                                                                                                                                                                                    \\
    ]{%
      ../results/example/pois_bench_table.csv
    }{}{%
    \idx                   & \a                               & \b                               & \reserve                         & \prederrorpois                                                                   & \prederrorquasi
    } \midrule
  \end{tabularx} \\
  \begin{tabularx}{0.7\linewidth}{LR}
    \csvreader[
      head to column names
    ]{../results/example/pois_bench_point.csv}{}{$\widehat{c}$ & \intercept \\ $\widehat{\phi}$ &\disp} \\ \bottomrule
  \end{tabularx}
  \caption{ODP and Poisson GLM results for the UK Motor triangle from \cref{tab:uk-motor}}
  \label{tab:odp-bench}
\end{table}

\section{Bootstrap methodology}

Developing bootstrap procedures for \modelref{model:odp} is in some respects easier than for \modelref{model:mack}. The absence of a recursive structure makes it more straightforward to reason about resampling, and there is a reasonable amount of literature on the subject to draw upon (see for instance \cite{pinheiro} and \cite{england:dist}). As in \cref{sec:mack-boot}, we shall take \cref{sec:boot-reg} as our starting point, adjusting the methods described there to obtain nonparametric, semiparametric and parametric bootstraps for \modelref{model:odp}. 

The nonparametric variant\index{bootstrap!nonparametric}, which involves resampling predictor-response pairs, is the most difficult to adapt to the present model. To understand why, recall that the typical size of a claims triangle is quite limited; consequently, such resampling can easily lead to a situation in which there are no observations for a given row or column, in which case no estimates can be obtained for the corresponding parameters. This is especially likely for the lower left and upper right corner points, which are the only observations in their row and column, respectively.

In order to avoid this difficulty, we therefore need to find a way of guaranteeing that every row and column contain at least one observation. The crudest option would be to discard any resampled triangle which is defective, but this method quite inefficient; the same result can be achieved at far lower computational cost by constructing resampled triangles to satisfy the desired property in the first place. We experimented with three ways of doing this: preserving the corner points, preserving the first row and column, and randomly selecting an observation in every row and column to preserve; the estimates for the MSEP obtained from all three are shown in \cref{tab:semiparam-pois-dev-res}. None of these turn out to produce satisfactory results, however, as seen from comparison with \cref{tab:odp-bench}. We will therefore exclude the pairs bootstrap from consideration for \modelref{model:odp}.

\begin{table}
  \centering
  \input{../results/example/glm_pairs.tex}
  \caption{MSEP estimates from the pairs bootstrap with different implementation strategies for the data in \cref{tab:uk-motor}}
  \label{tab:semiparam-pois-dev-res}
\end{table}

For the semiparameteric bootstrap\index{semiparametric bootstrap}, the essential step, as in \cref{sec:mack-boot}, is to find a satisfactory definition for the residuals such that they are i.i.d. We have to deal with an additional complication here, however, as it generally not possible to write the response as the sum of mean and error terms for distributions other than the Gaussian, meaning that there are no `natural residuals' for most GLMs\sepfootnote{fn:glm}. Consequently, a multitude of different types are available, reflecting different strategies for producing suitable diagnostic quantities; we will consider two of them in particular. 

The \emph{Pearson residuals}\index{residuals!pearson}
\begin{equation} \label{eq:pearson-resid}
  r_{ij} \coloneqq \frac{X_{ij} - \widehat{\mu}_{ij}}{\sqrt{V(\widehat{\mu}_{ij})}} \,,
\end{equation}
attempt to deal with the inherent heteroscedasticity\index{heteroscedasticity} of the response by dividing out the component of the variance which is specific to each observation, similarly to the standardised residuals in the context of weighted linear regression. Extending this analogy further, we can adjust \cref{eq:pearson-resid} to account for the leverage\index{leverage} of the observation, i.e.\
\begin{equation}
  \tilde{r}_{ij} \coloneqq \frac{X_{ij} - \widehat{\mu}_{ij}}{\sqrt{V(\widehat{\mu}_{ij}) (1 - h_{ij})}} \,,
\end{equation}
where $h_{ij}$ is the appropriate diagonal element in the hat matrix
\begin{equation} \label{eq:odp-H-mat}
  \mathbf{H} = \mathbf{X} (\mathbf{X} \mathbf{W}^{(k_f)} \mathbf{X})^{-1} \mathbf{X}^T \mathbf{W}^{(k_f)} \mathbf{z}^{(k_f)}
\end{equation}
corresponding to the final iteration $k_f$ of the IRWLS algorithm.

A second type of residuals is based on a goodness-of-fit measure known as the \emph{deviance}\index{deviance}, which can be derived from \cref{eq:lhood-mean-param} by observing that the mean parametrisation of the log-likelihood is maximised at $\bm{\mu} = \mathbf{y}$, so that the quantity
\begin{equation}
  D(\mathbf{y}, \bm{\mu}) \coloneqq \sum_{i = 1}^N d(y_i, \mu_i) \coloneqq 2 \sum_{i = 1}^N (l(y_i \mid y_i) - l(\widehat{\mu}_i \mid y_i))
\end{equation}
expresses the departure of the model from a perfect fit. The functions $D(\mathbf{y}, \bm{\mu})$ and $d(y_i, \mu_i)$ are called the \emph{total}\index{deviance!total} and \emph{unit deviance}\index{deviance!unit}, respectively, and the \emph{deviance residuals}\index{residuals!deviance} are then defined as
\begin{equation}
  r_{ij} \coloneqq \mathrm{sign}(x_{ij} - \widehat{\mu}_{ij}) \sqrt{d(x_{ij}, \widehat{\mu}_{ij})} \,,
\end{equation}
which can also be leverage-adjusted to obtain
\begin{equation}
  \tilde{r}_{ij} \coloneqq \frac{\mathrm{sign}(x_{ij} - \widehat{\mu}_{ij}) \sqrt{d(x_{ij}, \widehat{\mu}_{ij})}}{\sqrt{1 - h_{ij}}} \,,
\end{equation}
where $H$ is defined as in \cref{eq:odp-H-mat}. Under certain assumptions (see \cite[Section 7.5]{dunn:glm-in-r}), it can be shown that both the Pearson and deviance residuals converge asymptotically towards a normal distribution, which forms the basis of their use as diagnostic quantities.

\Cref{alg:semiparam-pois} then outlines the semiparametric bootstrap procedure for \modelref{model:odp}, which follows the familiar pattern of fit-resample-refit. The notation $\delta(x_{ij}, \mu_{ij})$ refers to one of the residuals which we just defined; inverting them to obtain pseudo-responses requires solving the equations
\begin{equation}
  \delta(x, \widehat{\mu}_{ij}) = r^{(b)}_{ij} \,,
\end{equation}
which are nontrivial for the deviance residuals. It should also be noted that the Pearson and deviance residuals corresponding to the lower left and upper right corner will always have a value of zero; we follow the general trend of the literature (see \cite[706]{pinheiro} or \cite[6]{verdonck}) in excluding them from the resampling process.

\begin{algorithm}
  \begin{algorithmic}
    \Require{Incremental claims triangle $\mathcal{D}_I$, number of bootstrap samples $B$}
    \State $\{(r_{ij}, \widehat{\mu}_{ij}) \mid i + j \leq I + 1 \} \gets$ \Call{fit}{$\mathcal{D}_I$}
    \For{$b \gets 1, B$}
      \State $\{r^{(b)}_{ij} \mid i + j \leq I + 1 \} \gets$ \Call{resample}{$\{r_{ij} \mid i + j \leq I + 1 \}$}
      \For{$j \gets 1, I$}
        \For{$i \gets 1, I + 1 - j$}
          \State $X_{ij}^{(b)} \gets$ \Call{solve}{$\delta(x, \widehat{\mu}_{ij}) = r_{ij}^{(b)}$}
        \EndFor
      \EndFor
      \State $\widehat{\bm{\beta}}^{(b)} \coloneqq (\widehat{c}^{(b)}, \widehat{a}_2^{(b)}, \dots, \widehat{a}_I^{(b)}, \widehat{b}_2^{(b)}, \dots, \widehat{b}_I^{(b)}) \gets$ \Call{fit}{$\mathcal{D}^{(b)}_I$}
    \EndFor
    \State \Return{$\{ \widehat{\bm{\beta}}^{(b)} \mid b = 1, \dots, B \}$}
  \end{algorithmic}
  \caption{Semiparametric bootstrap for \modelref{model:odp}}
  \label{alg:semiparam-pois}
\end{algorithm}

The results of applying the semiparameteric bootstrap to the data in \cref{tab:uk-motor} are shown in \cref{tab:semiparam-pois-res}. As in \cref{sec:mack-boot}, we note that the MSEP is obtained using \cref{eq:pred-error-boot}, which we discuss below in \cref{sec:glm-proc}; it is included here in the interest of continuity. All bootstrap results in chapter were computed with $B = 1000$ and $S = 100$. 

\begin{table}
  \begin{subtable}{0.5\linewidth}
    \input{../results/example/glm_semiparam_pearson.tex}
    \subcaption{Pearson}
  \end{subtable}
  \hfill
  \begin{subtable}{0.5\linewidth}
    \input{../results/example/glm_semiparam_deviance.tex}
    \subcaption{Deviance}
  \end{subtable}
  \caption{Results from the semiparametric bootstrap with different residual types for the data in \cref{tab:uk-motor}}
  \label{tab:semiparam-pois-res}
\end{table}

Comparing \cref{tab:semiparam-pois-res} to \cref{tab:odp-bench}, we clearly see that the variant with deviance residuals underestimates the uncertainty of the reserve. The reason for this is a technical limitation which was pointed out in \cite[13-14]{hartl}: for positive response data, we can only resample residuals which satisfy the condition
\begin{equation}
  r_{ij} > -\sqrt{2 \min({\mathcal{D}_I})} \,.
\end{equation}
Larger residuals are more likely to violate this, but discarding them introduces bias in the bootstrap, causing the predictive distribution to become narrower than is warranted by the model. Consequently, we will not consider deviance residuals in the remainder of this study.

Finally, we consider the parametric bootstrap\index{bootstrap!parametric}, which can be adapted without difficulty from the theory in \cref{sec:boot-reg}: fit the model to the data to obtain estimates for $c$, $a_i$, $b_j$ and $\phi$, and use a random number generator to simulate pseudo-responses from the fitted distribution. Because \modelref{model:odp} is a quasi-model, we can choose any distribution as long as it satisfies
\begin{equation}
  \var{X_{ij}} = \widehat{\phi} \, V(\widehat{\mu}_{ij}) = \widehat{\phi} \, \widehat{\mu}_{ij} \,.
\end{equation}
An obvious candidate would be the normal distribution $\mathcal{N}(\widehat{\mu}_{ij}, \phi \widehat{\mu}_{ij})$; alternatives proposed in the literature (see \cite{england:add-anal-boot-est}) include the gamma distribution $\Gamma(\alpha_{ij}, \beta_{ij})$ with
\begin{equation}
  \alpha_{ij} = \frac{\widehat{\mu}_{ij}}{\widehat{\phi}} \quad \text{and} \quad \beta_{ij} = \frac{1}{\widehat{\phi}} \,,
\end{equation}
and the scaled Poisson distribution
\begin{equation}
  Y = \widehat{\phi} X \,, \quad X \sim \mathrm{Pois}(\widehat{\mu}_{ij} / \widehat{\phi}) \,,
\end{equation}
which satisfies $\expect{Y} = \widehat{\mu}_{ij}$ and $\var{Y} = \phi \widehat{\mu}_{ij}$.
The model is then refitted to the simulated triangle to obtain $\widehat{c}^{(b)}$, $\widehat{a}^{(b)}_i$, $\widehat{b}^{(b)}_j$ and $\widehat{\phi}^{(b)}$. \Cref{alg:param-pois} outlines the procedure for the normal distribution, and the results for the three choices of distribution are shown in \cref{tab:odp-param-norm-gamma,tab:odp-param-pois}

\begin{table}[!htb]
  \centering
  \begin{subtable}{0.45\linewidth}
    \input{../results/example/glm_param_normal.tex}
    \subcaption{Normal}
  \end{subtable}
  \hfill
  \begin{subtable}{0.45\linewidth}
    \input{../results/example/glm_param_gamma.tex}
    \subcaption{Gamma}
  \end{subtable}
  \caption{Results from the parametric bootstrap with different distributions for the data in \cref{tab:uk-motor}}
  \label{tab:odp-param-norm-gamma}
\end{table}

\begin{table}[!htb]
  \centering
  \input{../results/example/glm_param_poisson.tex}
  \caption{Results from the parametric bootstrap with scaled Poisson distribution for the data in \cref{tab:uk-motor}}
  \label{tab:odp-param-pois}
\end{table}

\begin{algorithm}[!htb]
  \begin{algorithmic}
    \Require{Incremental claims triangle $\mathcal{D}_I$, number of bootstrap samples $B$}
    \State $\{\widehat{\mu}_{ij} \mid i + j \leq I + 1 \} \gets$ \Call{fit}{$\mathcal{D}_I$}
    \For{$b \gets 1, B$}
      \For{$j \gets 1, I$}
        \For{$i \gets 1, I + 1 - j$}
          \State $X_{ij}^{(b)} \gets$ \Call{sample}{$\mathcal{N}(\widehat{\mu}_{ij}, \phi \widehat{\mu}_{ij})$}
        \EndFor
      \EndFor
      \State $\widehat{\bm{\beta}}^{(b)} \coloneqq (\widehat{c}^{(b)}, \widehat{a}_2^{(b)}, \dots, \widehat{a}_I^{(b)}, \widehat{b}_2^{(b)}, \dots, \widehat{b}_I^{(b)}) \gets$ \Call{fit}{$\mathcal{D}^{(b)}_I$}
    \EndFor
    \State \Return{$\{ \widehat{\bm{\beta}}^{(b)} \mid b = 1, \dots, B \}$}
  \end{algorithmic}
  \caption{Parametric bootstrap for \modelref{model:odp}}
  \label{alg:param-pois}
\end{algorithm}

\section{Incorporating the process error} \label{sec:glm-proc}

The only remaining question now is how to incorporate the intrinsic variability of the response in the bootstrap methods from the previous section. In light of \cref{sec:boot-proc}, we want to achieve this using the predictive distribution, just as in \cref{sec:mack-proc}. This involves simulating the stochastic fluctuation of the reserve
\begin{equation}
  R = \sum_{\mathclap{\substack{1 \leq i, j \leq I \mathstrut \\ i + j > I + 1}}} X_{ij}
\end{equation}
around the point estimates obtained using the bootstrapped parameters $\widehat{c}^{(b)}$, $\widehat{a}^{(b)}_i$, $\widehat{b}^{(b)}_j$ and $\widehat{\phi}^{(b)}$. Consequently, we need to simulate pseudo-realisations of the lower triangle $\mathcal{D}^c_I$, which can be done by modifying \cref{alg:semiparam-pois,alg:param-pois} in a straightforward way. For the semiparametric bootstrap, we resample the residuals a second time and invert the appropriate formula to obtain pseudo-responses; for the parametric bootstrap, we simulate them from the fitted model obtained by plugging in the bootstrap replicates of the parameters. \Cref{fig:odp-pred-dist-semiparam,fig:odp-pred-dist-param} show the resulting predictive distributions for the data in \cref{tab:uk-motor}.

\begin{figure}[!p]
  \includegraphics{glm_semiparam}
  \caption{Predictive distribution for the semiparametric bootstrap}
  \label{fig:odp-pred-dist-semiparam}
\end{figure}

\begin{figure}[!p]
  \includegraphics{glm_param}
  \caption{Predictive distribution for the parametric bootstrap with different distributions}
  \label{fig:odp-pred-dist-param}
\end{figure}

\chapter{Simulation study} \label{chapter:sim}

As explained in the \cref{chapter:intro} and \cref{sec:boot-proc}, we want to study the impact of deviations from the model assumptions of \modelref{model:mack} and \modelref{model:odp} on the predictive distribution of the total outstanding claims. To this end, we will simulate a large number of run-off triangles, following a similar approach to the one described in \cite{schiegl}, which satisfy these assumptions by construction, except for a number of deviating observations (we will call such data \emph{contaminated}). We will then use the bootstrap methods described in \cref{chapter:mack,chapter:poisson} to obtain the predictive distribution while excluding a given subset of points from the resampling process. Our aim is to find out whether predictive distributions computed from data in which the correct points have been discarded (which we will refer to as the \emph{cleaned} data) differ significantly from those obtained after removing arbitrary observations. This would then suggest a way of detecting irregularities in new triangles: compare for every point (resp.\ calendar, origin period) the predictive bootstrap distribution obtained with and without this point; if a significant difference can be detected, then this subset constitutes a deviation from the model.


\section{Mack's model} \label{sec:mack-sim}

Recall from \cref{sec:mack-intro} that \modelref{model:mack} is recursive in nature, modelling the cumulative claim amounts in the next development period as a function of those in the previous one. Hence, it requires that observations for the first development period be given initially. This could be done using a separate model, but we choose here the simpler approach of borrowing the first column from the UK Motor example data in \cref{tab:uk-motor}. Similarly, we choose as the development factors $f_1, \dots, f_{I - 1}$ and variance parameters $\sigma_1, \dots, \sigma_{I - 1}$ of the simulated triangles the fitted ones from this dataset.

In order to obtain a synthetic triangle which satisfies \cref{assump:mack-expectation,assump:mack-variance,assump:mack-independence} except at a single point $(i^*, j^*)$ (where $i^* + j^* \leq I + 1$ and $j^* > 1$), we can then start from the initial column and use any suitable data-generating process (such as \cref{eq:time-series-model}) to recursively generate the next antidiagonal of the triangle. For the non-contaminated rows, we continue this process until reaching the current calendar year. In the contaminated row, we stop at column $j^* - 1$ and use $c_\mu$ and $c_\sigma$ to draw an observation from the perturbed distribution $\mathcal{N}(c_\mu f_{j^* - 1} C^*_{i^*, j^* - 1}, (c_\sigma \sigma_{j^* - 1})^2 C^*_{i^*, j^* - 1})$, after which we resume the previous process. The whole procedure is outlined in \cref{alg:pert-mack-triangle}. Except for some additional bookkeeping, simulating triangles where an entire origin or calendar period has been perturbed can be done in exactly the same way.

\begin{algorithm}[!htb]
  \begin{algorithmic}
    \Require{Development factors $f_1, \dots, f_{I - 1}$, variance parameters $\sigma_1, \dots, \sigma_{I - 1}$, perturbed point $(i^*, j^*)$ with $i^* + j^* \leq I + 1$ and $j^* > 1$, perturbation factors $c_\mu$, $c_\sigma$}
    \ForAll{$1 \leq i, j \leq I$}
      \State $C_{ij} \gets$ \textsc{NA}
    \EndFor
    \For{$i \gets 1, I$}
      \State $C^*_{i1} \gets C_{i1}$
      \If{$i \neq i^*$}
        \For{$j \gets 2, I + 1 - i$}
          \State $C^*_{ij} \gets$ \Call{sample}{$\mathcal{N}(f_j C^*_{i, j - 1}, \sigma_j^2 C^*_{i, j - 1})$}
        \EndFor
      \Else
        \For{$j \gets 2, j^* - 1$}
          \State $C^*_{i^*j} \gets$ \Call{sample}{$\mathcal{N}(f_j C^*_{i^*, j - 1}, \sigma_j^2 C^*_{i^*, j - 1})$}
        \EndFor
        \State $C^*_{i^*j^*} \gets$ \Call{sample}{$\mathcal{N}(c_\mu \, f_j C^*_{i^*, j^* - 1}, (c_\sigma \, \sigma_j)^2 C^*_{i^*, j^* - 1})$}
        \For{$j \gets j^* + 1, I + 1 - i$}
          \State $C^*_{i^*j} \gets$ \Call{sample}{$\mathcal{N}(f_j C^*_{i^*, j - 1}, \sigma_j^2 C^*_{i^*, j - 1})$}
        \EndFor
      \EndIf
    \EndFor
    \State \Return{$(C^*_{ij})_{1 \leq i,j \leq I}$}
  \end{algorithmic}
  \caption{Simulating a cumulative claims triangle with a single perturbed point according to \modelref{model:mack}}
  \label{alg:pert-mack-triangle}
\end{algorithm}

\section{Overdispersed Poisson GLM} \label{sec:odp-sim}

Simulating triangles conforming to \modelref{model:odp} is significantly easier than for \modelref{model:mack}, as we don't have to deal with recursion. Using the fitted parameters from \cref{tab:uk-motor-long} for $\phi$, $c$, $a_2, \dots, a_I$ and $b_2, \dots, b_I$, we generate an observation for every cell $(i, j)$ in the upper triangle by drawing from an appropriate distribution. Triangles with a single contaminated point can then be obtained by applying the perturbation before calling the random number generator. Again, similar algorithms can be used for perturbed origin or calendar periods.

\begin{algorithm}[!htb]
  \begin{algorithmic}
    \Require{Intercept $c$, development period parameters $a_2, \dots, a_I$, origin period parameters $b_2, \dots, b_I$, dispersion parameter $\phi$, perturbed point $(i^*, j^*)$ with $i^* + j^* \leq I + 1$ and $j^* > 1$, perturbation factor $c_\lambda$}
    \State $a_1 \gets 0$, $b_1 \gets 0$
    \ForAll{$1 \leq i, j \leq I$}
      \State $\mu_{ij} \gets \exp(c + a_i + b_j)$
      \State $X_{ij} \gets \textsc{NA}$
    \EndFor
    \For{$i \gets 1, I$}
      \For{$j \gets 1, I + 1 - i$}
        \If{$i = i^*$ and $j = j^*$}
          \State $X^*_{ij} \gets$ \Call{sample}{$\mathcal{N}(c_\lambda \mu_{ij}, \phi c_\lambda \mu_{ij})$}
        \Else
          \State $X^*_{ij} \gets$ \Call{sample}{$\mathcal{N}(\mu_{ij}, \phi \mu_{ij})$}
        \EndIf
      \EndFor
    \EndFor
    \State \Return{$(X^*_{ij})_{1 \leq i,j \leq I}$}
  \end{algorithmic}
  \caption{Simulating an incremental claims triangle with single perturbed point according to \modelref{model:odp}}
  \label{alg:pert-odp-triangle}
\end{algorithm}

\section{Setup}

We are now ready to discuss the general setup of the simulation study. Using \cref{alg:pert-mack-triangle,alg:pert-odp-triangle}, we generate claims triangles which perfectly satisfy the assumptions of \modelref{model:mack} or \modelref{model:odp}, except for a selected number of points. In particular, we consider the three cases of
\begin{inparaenum}[(i)]
  \item a single perturbed observation
  \item a perturbed calendar period and
  \item a perturbed origin period.
\end{inparaenum}
In each instance, we carry out one of the bootstrap procedures outlined in \cref{chapter:mack,chapter:poisson} while excluding a subset of observations of the same type as the contaminated one, and then compare the resulting predictive distributions. Our hope is that we can detect a significant difference between the predictive distribution obtained when excluding the right subset and the ones where a different subset has been removed.

The way in which observations can be excluded when carrying out the bootstrap is specific to each method, and raises a number of practical issues. For the semiparametric bootstraps, the most obvious way is to simply remove the corresponding residuals from the resampling pool. In the case of \modelref{model:mack}, this implies that the first column cannot contain contaminated points; this makes sense, as this model does not make any statements regarding the behaviour of the initial observations. While odd points in the triangle may be considered \emph{outliers} in a general sense, they do not necessarily constitute deviations from the model assumptions. To minimise the impact of this extraneous factor on the results, we have taken the approach of borrowing this initial column from the benchmark dataset in \cref{tab:uk-motor}, as explained in \cref{sec:mack-sim}.

For the parametric bootstrap, it is not as clear how we should go about excluding certain points. Obviously, our goal is to nullify the impact of these observations on the bootstrap result, which suggests removing them from the initial fitting process when obtaining the parametric model used to simulate new pseudo-data. We have to be careful here, however, as this can sometimes lead to situations where the model cannot be fit at all. In particular, for \modelref{model:mack}, we cannot exclude the upper right corner point, because we cannot estimate the final development factor otherwise. The approach outlined here is therefore not suitable to detecting deviations for this point. This is no great loss, however: the final development year requires special attention in any case, as it is very tied up with issues of tail factors and extrapolation. For \modelref{model:poisson}, the restriction is even more stringent; here, we cannot exclude the lower left corner either, because the resulting model would have no parameter corresponding to the final origin period, and consequently cannot be used for projection.

Finally, for the pairs bootstrap, we simply exclude the affected points from the resampling process.

\section{Implementation and results}

The code for carrying out the simulations and generating the examples from previous chapters has been mostly implemented in the \texttt{R} language \cite{R}. The computational power required to execute the large number of possible configurations exceeds the capacity of this high-level scripting language, however: the results dataframe for \modelref{model:mack} contains \SI{37602000} rows, for example. It was therefore decided to write the most numerically intensive parts of the simulation in \texttt{Fortran} and link it to \texttt{R} through a \texttt{C++} wrapper using the \texttt{Rcpp} package \cite{eddelbuettel}. For reproducibility and convenience, all software has been bundled together into a package, which is available at \url{https://github.com/OthmanElHammouchi/claimsBoot}. Instructions for installing are provided there as well.

The results are mostly presented as faceted density plots comparing, for different configurations, the predictive density of the reserve when the contaminated observations have been excluded (shown in red) with the predictive densities obtained after excluding model-conforming points. All simulations were done with $B = 1000$ and $S = 1$. As it would be impractical to show all of them, we have endeavoured to select a representative sample for inclusion. The reader is encouraged to download the code and experiment with it for him- or herself.

\subsection{Mack's model} \label{subsec:mack-sim-res}

In experiments where a single point has been perturbed, the semiparametric bootstrap performs poorly when used with standardised and log-normal residuals, as shown by \cref{fig:mack-single-cond-semiparam-standardised,fig:mack-single-cond-semiparam-log-normal,fig:mack-single-uncond-semiparam-standardised,fig:mack-single-uncond-semiparam-log-normal}. The predictive distribution does not differ significantly between the cleaned and contaminated data; this holds across different perturbation factors, and regardless of whether the conditional or unconditional approach is used.

Closer inspection of the simulation reveals the reason for this: standardised and log-normal residuals are not very sensitive to anomalies in a single observation. Consider for instance the triangle in \cref{tab:sim-triangle-example}, which is perturbed in point $(1, 2)$ with $c_\mu = 100$ and $c_\sigma = 1$, leading to hugely inflated values in the first row. The corresponding residual in \cref{tab:resids-log-normal-example,tab:resids-standard-example} does not exhibit a deviation of commensurate severity, however, and so excluding it does.

This stands in stark contrast with the studentised residuals, which are strongly affected by single point deviations in the simulated triangle, as can be seen in \cref{tab:resids-studentised-example}. Consequently, the cleaned triangle stands out from the contaminated ones. This is illustrated in \cref{tab:resids-student-res-mean}, which contains the means of the predictive distribution based on different outlier and excluded points with $c_\mu = 2$ and $c_\sigma = 1$. The minimum in each row is highlighted in red. This lines up almost perfectly with diagonal, which means that the predictive reserve mean of the cleaned triangle tends to stand out when using studentised residuals.

The parametric bootstrap shows good performance across the board. The choice of distribution does not appear to have a noticeable impact, although we do observe a substantial difference between the conditional and unconditional approaches, with the latter generally outperforming the former, often drastically, as shown in \cref{fig:mack-single-param}. Lastly, the pairs bootstrap is better at distinguishing contaminated from cleaned data than the semiparametric bootstrap with log-normal or standardised residuals, but does not outperform the one using studentised residuals or the parametric bootstrap. As \cref{tab:resids-pairs-res-mean} shows, the mean of the predictive distribution simulated on the basis of the cleaned data stands out for every outlier point, but often only barely. In all cases, differences tend to be noticeable for later development periods.

The same general trends are observed in a more pronounced form when a calendar or origin period is perturbed, owing to the larger number of deviating points. Consequently, even the semiparametric bootstrap with log-normal or standardised residuals shows serviceable results, as can be seen in \cref{fig:mack-calendar-cond-semiparam-log-normal,fig:mack-calendar-uncond-semiparam-log-normal,fig:mack-calendar-cond-semiparam-standard,fig:mack-calendar-uncond-semiparam-standard}. Later periods universally show stronger effects, because they contain more observations; for the first period, no bootstrap method can distinguish between clean and contaminated data.

\enlargethispage{10\baselineskip}

\begin{table}[!htb]
  \input{../results/sim_triangle_example.tex}
  \caption{Simulated triangle where observation $C_{12}$ has been perturbed with $c_\mu = 100$ and $c_\sigma = 1$}
  \label{tab:sim-triangle-example}
\end{table}

\begin{table}[!htb]
  \setlength\tabcolsep{3pt}
  \begin{subtable}{0.45\linewidth}
    \centering
    \input{../results/resids_standardised_example.tex}
    \subcaption{Standardised}
    \label{tab:resids-standard-example}
  \end{subtable}
  \hfill
  \begin{subtable}{0.45\linewidth}
    \centering
    \input{../results/resids_log_normal_example.tex}
    \subcaption{Log-normal}
    \label{tab:resids-log-normal-example}
  \end{subtable}
  \caption{Residuals corresponding to the triangle in \cref{tab:sim-triangle-example}}
\end{table}

\begin{table}[!htb]
  \input{../results/resids_studentised_example.tex}
  \caption{Studentised residuals corresponding to the triangle in \cref{tab:sim-triangle-example}}
  \label{tab:resids-studentised-example}
\end{table}

\newgeometry{bottom=2cm}

%% single
% residuals cond standardised
\begin{landscape}
  \begin{figure}
    \centering
    \includegraphics{mack_single_densities_cond_residuals_standardised}
    \caption[Single outlier density plots for Mack's model, conditional semiparametric bootstrap with standardised residuals]{Density plots of predictive distributions obtained using a conditional semiparametric bootstrap of \modelref{model:mack} with standardised residuals for different deviating points. The curves in each facet correspond to densities obtained while excluding a different point from the resampling process, with the result for the cleaned dataset highlighted in red. For all subfigures $c_\mu = c_\sigma = 0.5$.}
    \label{fig:mack-single-cond-semiparam-standardised}
  \end{figure}
\end{landscape}

% residuals cond log-normal
\begin{landscape}
  \begin{figure}
    \centering
    \includegraphics{mack_single_densities_cond_residuals_log-normal}
    \caption[Single outlier density plots for Mack's model, conditional semiparametric bootstrap with log-normal residuals]{Density plots of predictive distributions obtained using a conditional semiparametric bootstrap of \modelref{model:mack} with log-normal residuals for different deviating points. The curves in each facet correspond to densities obtained while excluding a different point from the resampling process, with the result for the cleaned dataset highlighted in red. For all subfigures $c_\mu = 0.5$ and $c_\sigma = 2$.}
    \label{fig:mack-single-cond-semiparam-log-normal}
  \end{figure}
\end{landscape}

% residuals uncond standardised
\begin{landscape}
  \begin{figure}
    \centering
    \includegraphics{mack_single_densities_uncond_residuals_standardised}
    \caption[Single outlier density plots for Mack's model, unconditional semiparametric bootstrap with standardised residuals]{Density plots of predictive distributions obtained using an unconditional semiparametric bootstrap of \modelref{model:mack} with standardised residuals for different deviating points. The curves in each facet correspond to densities obtained while excluding a different point from the resampling process, with the result for the cleaned dataset highlighted in red. For all subfigures $c_\mu = 2$ and $c_\sigma = 0.5$.}
    \label{fig:mack-single-uncond-semiparam-standardised}
  \end{figure}
\end{landscape}

% residuals uncond log-normal
\begin{landscape}
  \begin{figure}
    \centering
    \includegraphics{mack_single_densities_uncond_residuals_log-normal}
    \caption[Single outlier density plots for Mack's model, unconditional semiparametric bootstrap with log-normal residuals]{Density plots of predictive distributions obtained using an unconditional semiparametric bootstrap of \modelref{model:mack} with log-normal residuals for different deviating points. The curves in each facet correspond to densities obtained while excluding a different point from the resampling process, with the result for the cleaned dataset highlighted in red. For all subfigures $c_\mu = c_\sigma = 2$.}
    \label{fig:mack-single-uncond-semiparam-log-normal}
  \end{figure}
\end{landscape}

% parametric
\begin{landscape}
  \begin{figure}
    % cond normal
    \begin{subfigure}{0.5\linewidth}
      \centering
      \includegraphics{mack_single_densities_cond_parametric_normal}
      \subcaption{Conditional, normal distribution, $c_\mu = 2$}
    \end{subfigure}
    % uncond normal
    \begin{subfigure}{0.5\linewidth}
      \centering
      \includegraphics{mack_single_densities_uncond_parametric_normal}
      \subcaption{Unconditional, normal distribution, $c_\mu = 0.5$}
    \end{subfigure}
    % cond gamma
    \begin{subfigure}{0.5\linewidth}
      \centering
      \includegraphics{mack_single_densities_cond_parametric_gamma}
      \subcaption{Conditional, gamma distribution, $c_\mu = 0.5$}
    \end{subfigure}
    % uncond gamma
    \begin{subfigure}{0.5\linewidth}
      \centering
      \includegraphics{mack_single_densities_uncond_parametric_gamma}
      \subcaption{Unconditional, gamma distribution, $c_\mu = 2$}
    \end{subfigure}
    \caption[Single outlier density plots for Mack's model, parametric bootstrap]{Density plots of predictive distributions obtained using a parametric bootstrap of \modelref{model:mack} with deviating point $(i^*, j^*) = (4, 2)$. The curves in each facet correspond to densities obtained while excluding a different point from the resampling process, with the result for the cleaned dataset highlighted in red. For all subfigures, $c_\sigma = 1$.}
    \label{fig:mack-single-param}
  \end{figure}
\end{landscape}

% residuals studentised table
\begin{landscape}
  \setlength\tabcolsep{4pt}
  \renewcommand{\arraystretch}{1.2}
  \input{../results/mean_table_studentised.tex}
\end{landscape}

% pairs
\begin{landscape}
  \begin{figure}
    \centering
    \includegraphics{mack_single_densities_pairs}
    \caption[Single outlier density plots for Mack's model, pairs bootstrap]{Density plots of predictive distributions obtained using a pairs bootstrap of \modelref{model:mack} for different deviating points. The curves in each facet correspond to densities obtained while excluding a different point from the resampling process, with the result for the cleaned dataset highlighted in red. }
    \label{fig:mack-single-pairs}
  \end{figure}
\end{landscape}

% pairs table
\begin{landscape}
  \setlength\tabcolsep{2pt}
  \renewcommand{\arraystretch}{1.2}
  \input{../results/mean_table_pairs.tex}
\end{landscape}

%% calendar
% residuals cond standardised
\begin{landscape}
  \begin{figure}
    \begin{subfigure}{0.5\linewidth}
      \centering
      \includegraphics{mack_calendar_densities_cond_residuals_standardised}
      \subcaption{Conditional, $c_\mu = c_\sigma = 0.5$}
      \label{fig:mack-calendar-cond-semiparam-standard}
    \end{subfigure}
    % residuals uncond standardised
    \begin{subfigure}{0.5\linewidth}
      \centering
      \includegraphics{mack_calendar_densities_uncond_residuals_standardised}
      \subcaption{Unconditional, $c_\mu = 2$ and $c_\sigma = 0.5$}
      \label{fig:mack-calendar-uncond-semiparam-standard}
    \end{subfigure}
    \caption[Calendar outlier density plots for Mack's model, semiparametric bootstrap with standardised residuals]{Density plots of predictive distributions obtained using a semiparametric bootstrap of \modelref{model:mack} with standardised residuals for different deviating calendar periods. The curves in each facet correspond to densities obtained while excluding a different calendar period from the resampling process, with the result for the cleaned dataset highlighted in red.}
  \end{figure}
\end{landscape}

% residuals cond log-normal
\begin{landscape}
  \begin{figure}
    \begin{subfigure}{0.5\linewidth}
      \centering
      \includegraphics{mack_calendar_densities_cond_residuals_log-normal}
      \subcaption{Conditional, $c_\mu = 0.5$ and $c_\sigma = 2$}
      \label{fig:mack-calendar-cond-semiparam-log-normal}
    \end{subfigure}
    % residuals uncond log-normal
    \begin{subfigure}{0.5\linewidth}
      \centering
      \includegraphics{mack_calendar_densities_uncond_residuals_log-normal}
      \subcaption{Unconditional, $c_\mu = c_\sigma = 2$}
      \label{fig:mack-calendar-uncond-semiparam-log-normal}
    \end{subfigure}
    \caption[Calendar outlier density plots for Mack's model, semiparametric bootstrap with log-normal residuals]{Density plots of predictive distributions obtained using a semiparametric bootstrap of \modelref{model:mack} with log-normal residuals for different deviating calendar periods. The curves in each facet correspond to densities obtained while excluding a different calendar period from the resampling process, with the result for the cleaned dataset highlighted in red.}
  \end{figure}
\end{landscape}

% parametric cond normal
\begin{landscape}
  \begin{figure}
    \begin{subfigure}{0.5\linewidth}
      \centering
      \includegraphics{mack_calendar_densities_cond_parametric_normal}
      \subcaption{Conditional, normal distribution, $c_\mu = 0.5$ and $c_\sigma = 1$}
      \label{fig:mack-calendar-cond-param-normal}
    \end{subfigure}
    % parametric uncond gamma
    \begin{subfigure}{0.5\linewidth}
      \centering
      \includegraphics{mack_calendar_densities_uncond_parametric_gamma}
      \subcaption{Unconditional, gamma distribution, $c_\mu =  2$ and $c_\sigma = 1$}
      \label{fig:mack-calendar-uncond-param-gamma}
    \end{subfigure}
    \caption[Calendar outlier density plots for Mack's model, parametric bootstrap]{Density plots of predictive distributions obtained using a parametric bootstrap of \modelref{model:mack} for different deviating calendar periods. The curves in each facet correspond to densities obtained while excluding a different calendar period from the resampling process, with the result for the cleaned dataset highlighted in red.}
  \end{figure}
\end{landscape}

% pairs
\begin{landscape}
  \begin{figure}
    \centering
    \includegraphics{mack_calendar_densities_pairs}
    \caption[Calendar outlier density plots for Mack's model, pairs bootstrap]{Density plots of predictive distributions obtained using a pairs bootstrap of \modelref{model:mack} for different deviating calendar periods. The curves in each facet correspond to densities obtained while excluding a different calendar period from the resampling process, with the result for the cleaned dataset highlighted in red.}
    \label{fig:mack-calendar-pairs}
  \end{figure}
\end{landscape}

\subsection{Overdispersed Poisson GLM}

In the single point simulation, the semiparametric bootstrap performs rather poorly compared to the parametric one, regardless of which distribution is used in the latter. As in \cref{subsec:mack-sim-res}, we can trace the cause of this by analysing how the residuals are affected by perturbations. \Cref{tab:glm-triangle-example} shows a simulated triangle containing a perturbation at the point $(1, 1)$. The Pearson residuals for the original and perturbed triangles are shown in \cref{fig:glm-pears-sim-example}. As we can see, the contamination does not remain localised, but spreads across the whole triangle of residuals, leading to a situation in which the affected point no longer stands out.

\input{../results/glm_triangle_example.tex}

\begin{table}[!htb]
  \begin{subtable}{\linewidth}
    \input{../results/glm_resids_original_example.tex}
    \subcaption{Original}
  \end{subtable}
  \newline
  \vspace*{0.25cm}
  \newline
  \begin{subtable}{\linewidth}
    \input{../results/glm_resids_perturbed_example.tex}
    \subcaption{Perturbed}
  \end{subtable}
  \caption{Example of Pearson residuals obtained in the simulation}
  \label{fig:glm-pears-sim-example}
\end{table}

%% single
% parametric gamma
\begin{landscape}
  \begin{figure}
    \centering
    \includegraphics{glm_single_densities_parametric_gamma}
    \caption[Single outlier density plots for ODP model, parametric bootstrap with gamma distribution]{Density plots of predictive distributions obtained using a parametric bootstrap of \modelref{model:odp} with gamma distribution for different deviating points. The curves in each facet correspond to densities obtained while excluding a different point from the resampling process, with the result for the cleaned dataset highlighted in red. For all subfigures, $c_\lambda = 0.5$.}
    \label{fig:odp-single-param-gamma}
  \end{figure}
\end{landscape}

% parametric normal
\begin{landscape}
  \begin{figure}
    \centering
    \includegraphics{glm_single_densities_parametric_normal}
    \caption[Single outlier density plots for ODP model, parametric bootstrap with normal distribution]{Density plots of predictive distributions obtained using a parametric bootstrap of \modelref{model:odp} with normal distribution for different deviating points. The curves in each facet correspond to densities obtained while excluding a different point from the resampling process, with the result for the cleaned dataset highlighted in red. For all subfigures, $c_\lambda = 2$.}
    \label{fig:odp-single-param-norm}
  \end{figure}
\end{landscape}

% parametric poisson
\begin{landscape}
  \begin{figure}
    \centering
    \includegraphics{glm_single_densities_parametric_poisson}
    \caption[Single outlier density plots for ODP model, parametric bootstrap with scaled Poisson distribution]{Density plots of predictive distributions obtained using a parametric bootstrap of \modelref{model:odp} with scaled Poisson distribution for different deviating points. The curves in each facet correspond to densities obtained while excluding a different point from the resampling process, with the result for the cleaned dataset highlighted in red. For all subfigures, $c_\lambda = 0.5$.}
    \label{fig:odp-single-param-pois}
  \end{figure}
\end{landscape}

% residuals
\begin{landscape}
  \begin{figure}
    \centering
    \includegraphics{glm_single_densities_residuals}
    \caption[Single outlier density plots for ODP model, semiparametric bootstrap with Pearson residuals]{Density plots of predictive distributions obtained using a semiparametric bootstrap of \modelref{model:odp} with Pearson residuals for different deviating points. The curves in each facet correspond to densities obtained while excluding a different point from the resampling process, with the result for the cleaned dataset highlighted in red. For all subfigures, $c_\lambda = 2$.}
    \label{fig:odp-single-semiparam}
  \end{figure}
\end{landscape}

%% calendar
% parametric gamma
\begin{landscape}
  \begin{figure}
    \begin{subfigure}{0.5\linewidth}
      \centering
      \includegraphics{glm_calendar_densities_parametric_gamma}
      \subcaption{Gamma distribution, $c_\lambda = 0.5$}
      \label{fig:odp-calendar-param-gamma}
    \end{subfigure}
    % parametric normal
    \begin{subfigure}{0.5\linewidth}
      \centering
      \includegraphics{glm_calendar_densities_parametric_normal}
      \subcaption{Normal distribution, $c_\lambda = 2$}
      \label{fig:odp-calendar-param-norm}
    \end{subfigure}
    \caption[Calendar outlier density plots for ODP model, parametric bootstrap with normal distribution]{Density plots of predictive distributions obtained using a parametric bootstrap of \modelref{model:odp} for different deviating calendar periods. The curves in each facet correspond to densities obtained while excluding a different calendar period from the resampling process, with the result for the cleaned dataset highlighted in red.}
  \end{figure}
\end{landscape}

% parametric poisson
\begin{landscape}
  \begin{figure}
    \begin{subfigure}{0.5\linewidth}
      \centering
      \includegraphics{glm_calendar_densities_parametric_poisson}
      \subcaption{Parametric with scaled Poisson distribution, $c_\lambda = 0.5$}
      \label{fig:odp-calendar-param-pois}
    \end{subfigure}
    % residuals
    \begin{subfigure}{0.5\linewidth}
      \centering
      \includegraphics{glm_calendar_densities_residuals}
      \subcaption{Semiparametric, $c_\lambda = 2$}
      \label{fig:odp-calendar-semiparam}
    \end{subfigure}
    \caption[Calendar outlier density plots for ODP model, semiparametric and parametric bootstrap]{Density plots of predictive distributions obtained using parametric and semiparametric bootstraps of \modelref{model:odp} for different deviating calendar periods. The curves in each facet correspond to densities obtained while excluding a different calendar period from the resampling process, with the result for the cleaned dataset highlighted in red.}
  \end{figure}
\end{landscape}

\restoregeometry

\backmatter%

\chapter{Conclusion} \label{conclusion}

We have investigated the application of bootstrap methods to the problem of detecting deviations from the model assumptions for two major actuarial reserving models, \modelref{model:mack} and \modelref{model:odp}. By simulating datasets which exhibit a myriad of different irregularities and studying their effect on the predictive distribution, we managed to identify a number of bootstrap configurations which appear promising for this purpose. The strength of the observed effect is tied to the sensitivity of a bootstrap procedure to the presence of contamination in the data. Parametric bootstraps were found to perform very well: even deviations in a single point caused a significant change in the predictive distribution. For semiparametric bootstraps, the results strongly depend on the type of residuals employed. Finally, the fully nonparametric pairs bootstrap, which can only be used with Mack's model, showed a reasonable performance, though not as good as the parametric variant. These results suggest a way of flagging suspicious datapoints by reverse-engineering the simulation process: instead of starting from synthetic triangles, we use real data and generate a predictive distribution while excluding all observations by turn; the ones for which effect is observed can then be identified as requiring special attention. Although this falls outside the purview of our investigation, it suggests an interesting avenue for future research.

\printbibliography%
\printindex

\end{document}
